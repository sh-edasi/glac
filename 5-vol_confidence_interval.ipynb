{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glacierml as gl\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "pd.set_option('display.max_column',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c28d4-2830-451a-bbc3-ca4a4766b99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home_path = '/path/to/project/directory'\n",
    "[\n",
    "        data_path, RGI_path, glathida_path, ref_path,\n",
    "        coregistration_testing_path, \n",
    "        arch_test_path, LOO_path\n",
    "] = gl.set_paths(home_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e5ba7-ecee-407e-8204-6013710d1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f2192",
   "metadata": {},
   "source": [
    "#### Let's do a calculation.\n",
    "#### ${S} \\approx \\sum_{k=1}^{N_k} \\hat{V}_  k \\pm Z^*_{\\alpha / 2} \\sqrt{\\sum_k^{N_k} \\sigma^2_k}$\n",
    "\n",
    "First, let's load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, df = gl.load_LOO_data(home_path, include_train = False, include_refs = False)\n",
    "cols, train = gl.load_LOO_data(home_path, include_train = True, include_refs = False)\n",
    "\n",
    "# E_delta_a, areas, cols, train = gl.load_LOO_data(include_train = True)\n",
    "# E_delta_a, areas, cols, df = gl.load_LOO_data(include_train = False)\n",
    "g = (\n",
    "    pd.read_csv(glathida_path + '/T.csv')[\n",
    "        ['AREA','MEAN_THICKNESS_UNCERTAINTY','MEAN_THICKNESS']\n",
    "    ]\n",
    ")\n",
    "g = g.rename(columns = {\n",
    "    'MEAN_THICKNESS_UNCERTAINTY':'u',\n",
    "    'AREA':'a',\n",
    "    'MEAN_THICKNESS':'t'\n",
    "})\n",
    "g.u = g.u/1e3\n",
    "g.t = g.t/1e3\n",
    "g = g.dropna(subset = ['u'])\n",
    "df[cols] = df[cols]/1e3\n",
    "train[cols] = train[cols]/1e3\n",
    "train['Thickness'] = train['Thickness'] / 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd605c",
   "metadata": {},
   "source": [
    "#### For the case of independent variables, $\\sigma^2_k =  \\left(\\sigma^H_k\\right)^2 \\left(\\sigma^A_k\\right)^2 + {A}_k^2 \\left(\\sigma^H_k\\right)^2 \n",
    "     +  H_k^2 \\left(\\sigma^A_k\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436efe87",
   "metadata": {},
   "source": [
    "#### Let $\\left(\\sigma^H_k\\right)^2 = \\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) + \\text{Var}\\left(\\epsilon^{\\mathcal{R}}_k\\right) +\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_k\\right)$,\n",
    "\n",
    "<!-- % #### and $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) \\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$, -->\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) \\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$,\n",
    "\n",
    "<!-- #### and $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\frac{1}{n_B n_j}\\sum_B^{n_B}\\sum_{j}^{n_j} \\left(r_{jB} - \\mu_{r_B}\\right)^2$, -->\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)$,\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_i\\right) \\approx \\text{Var} \\left(\\mathcal{M}_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10de73a",
   "metadata": {},
   "source": [
    "#### Start with $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right)\\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "### epsilonH is the uncertainty due to limited training data ###\n",
    "### var(epsilonH) represents variance of  ###\n",
    "# var_eps_H = (1/(len(cols) - 1)*np.sum(\n",
    "#     df[cols].sub(np.mean(df[cols],axis = 1),axis = 0)**2,axis = 1\n",
    "# )\n",
    "var_eps_H = np.var(df[cols],axis = 1)\n",
    "df = pd.concat([df,pd.Series(var_eps_H,name = 'var_eps_H')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73661e",
   "metadata": {},
   "source": [
    "#### That was easy. Now for  $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Var(epsR_B) ###\n",
    "### First bin residuals ###\n",
    "bins = []\n",
    "[bins.append(x/1e3) for x in range(1, 300, 1)]\n",
    "bins.append(0)\n",
    "bins = np.sort(bins).tolist()\n",
    "names = [str(x) for x in bins]\n",
    "bins.append(np.inf)\n",
    "train = pd.concat(\n",
    "    [\n",
    "        train,\n",
    "         pd.cut(train['Thickness'], bins, labels=names).rename('bin')\n",
    "    ],axis = 1\n",
    ")\n",
    "train['bin'] = train['bin'].astype(float)\n",
    "bins = train['bin'].sort_values().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe20e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterate through residuals and calculate a standard deviation ###\n",
    "c = dict.fromkeys(bins)\n",
    "re = dict.fromkeys(bins)\n",
    "\n",
    "for n,i in tqdm(enumerate(c.keys())):    \n",
    "    if i <= 170/1e3:\n",
    "        j = i + 1/1e3\n",
    "    if i > 170/1e3:\n",
    "        j = i - 10/1e3\n",
    "    dft = train[\n",
    "        (train['bin'] == i)       \n",
    "               ]\n",
    "    while len(dft) < 2:\n",
    "        if i <= 95/1e3:\n",
    "            j +=1/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] >= i)&\n",
    "            (train['bin'] <= j)       \n",
    "                   ]\n",
    "        if i >= 95/1e3 and i < 170/1e3:\n",
    "            j +=10/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] >= i)&\n",
    "            (train['bin'] <= j)       \n",
    "                   ]\n",
    "        if i > 170/1e3:\n",
    "            j -=10/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] <= i)&\n",
    "            (train['bin'] >= j)       \n",
    "                   ]\n",
    "        \n",
    "    h = dft[cols]\n",
    "    r = h.subtract(dft['Thickness'], axis=0)\n",
    "    c[i] = np.std(r.to_numpy().flatten())\n",
    "    re[i] = np.mean(r.to_numpy(), axis=0) \n",
    "stds = np.fromiter(c.values(), dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96706e9",
   "metadata": {},
   "source": [
    "#### Now we can fit $\\sqrt{\\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)}$ to $h_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit a statistical model to the range \n",
    "z = np.linspace(0.01,0.3,99) \n",
    "# z = train['Thickness']\n",
    "model = np.poly1d(np.polyfit(np.log(z),np.log(stds), 1))\n",
    "\n",
    "c1 = np.round(np.exp(model[0]),2)\n",
    "c2 = np.round((model[1]),1)\n",
    "print(c1)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "plt.scatter((z),(stds),alpha = 0.25,label = r'$\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.plot((z),np.exp(model(np.log(z))),\n",
    "                    label = rf'$\\sigma^{{\\mathcal{{R}}}}_B \\approx {c1} h_B^{{{c2}}} $',\n",
    "        c = 'orange'\n",
    "        )\n",
    "plt.legend()\n",
    "plt.title(r'Standard Deviation of Binned Residuals $\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.xlabel('GlaThida Thickness Bin (km)')\n",
    "plt.ylabel(r'$\\sigma^{\\mathcal{R}}_B$ (km)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d0f85",
   "metadata": {},
   "source": [
    "#### Apply to $H_k$ and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply statistical model of residuals to mean estimated thickness ###\n",
    "var_eps_R = (np.mean((c1*df[cols]**c2),axis = 1))**2\n",
    "df = pd.concat([df,pd.Series(var_eps_R,name = 'var_eps_R')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cee71",
   "metadata": {},
   "source": [
    "#### now for $\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_m\\right) \\approx \\text{Var}\\left(\\mathcal{M}_{m}\\right)$. \n",
    "\n",
    "#### GlaThiDa reports a thickness uncertainty. Consider this a std dev $\\sigma^{\\mathcal{M}}_m$ where $n_m < n_i$. We can do as we did with variance of residuals and fit another statistical model.\n",
    "\n",
    "#### Fit a a statistical model with $\\sqrt{\\text{Var}\\left(\\mathcal{M}_{m}\\right)}$ as the dependent variable to corresponding measured thickness $h_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = []\n",
    "for i in g.u:\n",
    "    g_u = gl.findlog(i)\n",
    "    u.append(g_u)\n",
    "u = np.array(u)\n",
    "### Fit a statistical model to the range \n",
    "# z = np.linspace(g.a.min(),g.a.max(),len(g)) \n",
    "# z = train['Thickness']\n",
    "model = np.poly1d(np.polyfit(np.log(g.t),(u), 1))\n",
    "\n",
    "c3 = np.round(np.exp(model[0]),2)\n",
    "c4 = np.round((model[1]),1)\n",
    "print(c3)\n",
    "print(c4)\n",
    "\n",
    "\n",
    "plt.scatter((g.t),(g.u),alpha = 0.25,label = r'$\\sigma^{\\mathcal{M}}_m$')\n",
    "plt.plot(np.sort(g.t),np.exp(model(np.log(np.sort(g.t)))),\n",
    "                    label = rf'$\\sigma^{{\\mathcal{{M}}}}_m \\approx {c3} h_m^{{{c4}}} $',\n",
    "         c = 'orange'\n",
    "        )\n",
    "plt.legend()\n",
    "# plt.title(r'Standard Deviation of Binned Residuals $\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.xlabel('GlaThida Measured Thickness for which we have uncertainty $h_m$ (km)')\n",
    "plt.ylabel(r'GlaThiDa Reported Thickness Uncertainty $\\sigma^{\\mathcal{M}}_m$ (km)')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.grid(which = 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35c342",
   "metadata": {},
   "source": [
    "#### Apply to $H_k$ and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eda4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_eps_M = (np.mean((c3*df[cols]**c4),axis = 1))**2\n",
    "df = pd.concat([df,pd.Series(var_eps_M,name = 'var_eps_M')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b243ed",
   "metadata": {},
   "source": [
    "#### Okay, thinking about residuals. We have captured the variance of residuals, but we haven't really considered the accuracy of our residuals Var($\\epsilon^{\\mathcal{RA}}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put them together ###\n",
    "var_H = var_eps_H + var_eps_R + var_eps_M\n",
    "df = pd.concat([df,pd.Series(var_H,name = 'var_H')],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be17ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.mean(df[cols],axis = 1)\n",
    "# xy = np.vstack([np.log10(h),np.log10(var_eps_H)])\n",
    "# print('calculating density')\n",
    "# z = gaussian_kde(xy)(xy)\n",
    "# np.save('epsh_z.npy',z)\n",
    "# z = np.load('epsh_z.npy')\n",
    "# plt.scatter(h,var_eps_H,label = 'h')\n",
    "plt.scatter(h,(var_eps_H),label = r'Var$\\left(\\epsilon^H_k\\right)$',marker = '.')\n",
    "\n",
    "plt.scatter(h,(var_eps_R),label = r'Var$\\left(\\epsilon^R_k\\right)$',marker = '.')\n",
    "plt.scatter(h,var_eps_M,label = r'Var$\\left(\\epsilon^M_k\\right)$',marker = '.')\n",
    "plt.xlabel('Estimated Mean Thickness (km)')\n",
    "plt.ylabel('(km)$^2$')\n",
    "plt.title('Global Glacier Thickness Uncertainty Components')\n",
    "plt.legend()\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),var_eps_H)\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),var_eps_H + var_eps_R + var_eps_M)\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),np.mean(df[cols],axis = 1))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503affe0",
   "metadata": {},
   "source": [
    "#### Okay, we have $(\\sigma^H_k)^2$ calculated and appended.\n",
    "#### Let's calculate $(\\sigma^A_k)^2$. We follow the procedure set out by Pfeffer et al. 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "p = 0.7\n",
    "e1 = 0.039\n",
    "var_A = (k*e1*(df['Area']**p))**2\n",
    "df = pd.concat([df,pd.Series(var_A,name = 'var_A')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e242002",
   "metadata": {},
   "source": [
    "#### That was easy. Now what is left?\n",
    "#### Let $H_k = \\frac{1}{N_j}\\sum_{j=1}^{N_j} \\mathcal{H}_{kj}$, and $A_k$ be the reported RGI Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculation for variance of independent variables. ###\n",
    "E_A = df['Area']\n",
    "E_H = np.mean(df[cols],axis = 1)\n",
    "\n",
    "v1 = (var_H * var_A)\n",
    "v2 = (var_H * E_A**2) \n",
    "v3 = (E_H**2 * var_A)\n",
    "sigma_k_ind = v1 + v2 + v3 \n",
    "df = pd.concat([df,pd.Series(sigma_k_ind,name = 'sig_k_ind')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba1afff",
   "metadata": {},
   "source": [
    "#### ${S} \\approx \\sum_{k=1}^{N_k} \\hat{V}_k \\pm Z^*_{\\alpha / 2} \\sqrt{\\sum_k^{N_k} \\sigma^2_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = df['sig_k_ind'] # = (Var(H)*Var(A) + A^2Var(H) + H^2Var(A))\n",
    "\n",
    "sp = np.sum(si)\n",
    "\n",
    "se = np.sqrt(sp)\n",
    "\n",
    "Z = (1.96)\n",
    "Vlb = ( (np.sum(E_H * E_A) - (Z*se)) ) / 1e3\n",
    "Vub = ( (np.sum(E_H * E_A) + (Z*se)) ) / 1e3\n",
    "\n",
    "print(f'[{Vlb},{Vub}]  * 10^3 km^3')\n",
    "\n",
    "print(f'Mid CI: {((Vub + Vlb) / 2)} * 10^3 km^3')\n",
    "\n",
    "print(f'CI Half Width: {np.round((Vub - Vlb) / 2,3)} * 10^3 km^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "x = E_H\n",
    "y = (np.sqrt(var_H) / (E_H)) * 100\n",
    "# if not os.path.exists('thickness_z.npy'):\n",
    "#     print('poop')\n",
    "#     xy = np.vstack([np.log10(x),np.log10(y)])\n",
    "#     print('calculating density')\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "#     np.save(z,'thickness_z.npy')\n",
    "# else:\n",
    "#     z = np.load('thickness_z.npy')\n",
    "ax[0].scatter(E_H, (np.sqrt(var_H) / (E_H)) * 100, marker='.',\n",
    "              label=r'$\\sigma_k / H_k \\times 100$')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Percent Uncertainty')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "\n",
    "x = E_H * E_A\n",
    "y = (np.sqrt(df['sig_k_ind']) / (E_H * E_A)) * 100\n",
    "# if not os.path.exists('thickness_z.npy'):\n",
    "#     xy = np.vstack([np.log10(x),np.log10(y)])\n",
    "#     print('calculating density')\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "#     np.save(z,'vol_z.npy')\n",
    "# else:\n",
    "#     z = np.load('vol_z.npy')\n",
    "ax[1].scatter(x, y, marker='.')\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_ylabel('Glacier Volume Percent Uncertainty')\n",
    "ax[1].set_xlabel('Glacier Volume (km$^3$)')\n",
    "\n",
    "fig.suptitle('Global Thickness and Volume Percent Uncertainties')\n",
    "\n",
    "# Add labels 'A' and 'B' to upper-right corners of the axes\n",
    "ax[0].text(0.95, 0.95, 'A', transform=ax[0].transAxes, fontsize=12, ha='right', va='top', fontweight='bold')\n",
    "ax[1].text(0.95, 0.95, 'B', transform=ax[1].transAxes, fontsize=12, ha='right', va='top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "# ax[0].legend()\n",
    "\n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = np.sqrt(var_H)\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${\\sigma_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2616b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "ax[0].scatter(E_H,((var_H)/(E_H)),marker = '.',\n",
    "              label = r'$\\sigma_k^2 / H_k $')\n",
    "ax[0].set_xscale('log')\n",
    "# plt.yscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Coefficient of Variation')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "ax[0].legend()\n",
    "              \n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = (var_H) / E_H\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${{\\sigma_k}^2}/{\\mu_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km$^3$)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "ax[0].scatter(E_A,((var_H)/(E_H)),marker = '.',\n",
    "              label = r'$\\sigma_k^2 / H_k $')\n",
    "ax[0].set_xscale('log')\n",
    "# plt.yscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Coefficient of Variation')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "ax[0].legend()\n",
    "              \n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = (var_H) / E_H\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${{\\sigma_k}^2}/{\\mu_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km$^3$)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.mean(df[cols],axis = 1) * df['Area']\n",
    "data_2 = np.sqrt(df['sig_k_ind'])\n",
    "\n",
    "ks_stat, p_value = stats.ks_2samp(data_1, data_2)\n",
    "print(ks_stat)\n",
    "# data_2 = data['FMT'] * data['Area']\n",
    "#sort data\n",
    "\n",
    "fig = plt.subplots(1,1,figsize = (10,8))\n",
    "cdf_data_1 = np.sort(data_1)\n",
    "cdf_data_2 = np.sort(data_2)\n",
    "\n",
    "\n",
    "cdf1 = np.arange(len(data_1)) / float(len(data_1))\n",
    "cdf2 = np.arange(len(data_2)) / float(len(data_2))\n",
    "\n",
    "\n",
    "plt.plot(cdf_data_1, cdf1, label='RGI Volume')\n",
    "plt.plot(cdf_data_2, cdf2, label='RGI Volume Uncertainty')\n",
    "\n",
    "plt.title('CDF of Glacier Volume and Volume Uncertainty Ignoring Covariance')\n",
    "plt.xlabel('Value (km$^3$)')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc = 'center right')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('final.pkl')\n",
    "# df.to_pickle('/data/fast1/glacierml/data/final_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d3ab7",
   "metadata": {},
   "source": [
    "#### And that's it for this notebook. For a further breakdown of variance components see variance_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacierml (Python3.8.10)",
   "language": "python",
   "name": "prethicktor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
