{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03c5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glacierml as gl\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, ref = gl.notebook_data_loader()\n",
    "\n",
    "test = pd.merge(df, ref, on = 'RGIId', how = 'outer')\n",
    "\n",
    "# x is originally df, so we sum those values where y is blank\n",
    "\n",
    "test['Area_x'] = test['Area_x'].astype(float)\n",
    "\n",
    "thick_loss = sum(test['Mean Thickness'][test['Area_y'].isnull()]) / 1e3\n",
    "area_loss = sum(test['Area_x'][test['Area_y'].isnull()]) / 1e3\n",
    "\n",
    "print(f'Thickness lost from reference merge (km): {thick_loss}')\n",
    "\n",
    "print(f'Area lost from reference merge (km^2): {area_loss}')\n",
    "\n",
    "lost = test[test['Area_y'].isnull()]\n",
    "\n",
    "lost_data = pd.DataFrame({\n",
    "    'Fields':[\n",
    "                'E&L Thickness', 'Farinotti Thickness',\n",
    "                'E&L&F Combined Thickness','E&L Lost Thickness',\n",
    "                'E&L Area','Farinotti Area',\n",
    "                'E&L&F Combined Area','E&L Lost Area'],\n",
    "    'Mean':[\n",
    "                df['Mean Thickness'].mean(),ref['Farinotti Mean Thickness'].mean(), \n",
    "                ref['Edasi Mean Thickness'].mean(), lost['Mean Thickness'].mean(),\n",
    "                df['Area'].mean(), ref['Area'].mean(), \n",
    "                ref['Area'].mean(), lost['Area_x'].mean()\n",
    "        ],\n",
    "    \n",
    "    'Median':[\n",
    "                df['Mean Thickness'].median(),ref['Farinotti Mean Thickness'].median(), \n",
    "                ref['Edasi Mean Thickness'].median(), lost['Mean Thickness'].median(),\n",
    "                df['Area'].median(), ref['Area'].median(), \n",
    "                ref['Area'].median(), lost['Area_x'].median()\n",
    "         ],\n",
    "    \n",
    "    'Max':[\n",
    "                df['Mean Thickness'].max(),ref['Farinotti Mean Thickness'].max(), \n",
    "                ref['Edasi Mean Thickness'].max(),lost['Mean Thickness'].max(),\n",
    "                df['Area'].max(), ref['Area'].max(), \n",
    "                ref['Area'].max(), lost['Area_x'].max()\n",
    "        ],\n",
    "    \n",
    "    'Min':[\n",
    "                df['Mean Thickness'].min(),ref['Farinotti Mean Thickness'].min(), \n",
    "                ref['Edasi Mean Thickness'].min(), lost['Mean Thickness'].min(),\n",
    "                df['Area'].min(), ref['Area'].min(), \n",
    "                ref['Area'].min(), lost['Area_x'].min()\n",
    "        ]\n",
    "})\n",
    "lost_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93183c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1,1,1, projection=crs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "plt.scatter(\n",
    "    x = lost['CenLon_x'], \n",
    "    y = lost['CenLat_x'],\n",
    "#     c = dft['VE / VF'],\n",
    "#     cmap = 'viridis',\n",
    "    marker = '.', \n",
    "#     alpha = 1\n",
    "\n",
    ")\n",
    "ax.set_title('Farinotti excluded glaciers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2cab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbd5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89389756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6128/6128 [02:23<00:00, 42.85it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glacierml as gl\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# print('Please select co-registration method: df1, df2, df3, df4, df5, df6, df7, df8')\n",
    "\n",
    "coregistration = 'df8'\n",
    "\n",
    "print('Loading predictions...')\n",
    "predictions = gl.predictions_finder(coregistration = coregistration)\n",
    "predictions = predictions.reset_index()\n",
    "predictions = predictions.drop('index', axis = 1)\n",
    "\n",
    "df = pd.DataFrame(columns = {\n",
    "        'RGIId','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "        '11','12','13','14','15','16','17','18','19','20','21',\n",
    "        '22','23','24',\n",
    "})\n",
    "\n",
    "print('Predictions loaded')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74333e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Compiling predictions...')\n",
    "for index in tqdm(predictions.index):\n",
    "    idx = index\n",
    "#     print(idx)\n",
    "\n",
    "    coregistration =  predictions['coregistration'].iloc[idx]\n",
    "    architecture = '_' + predictions['architecture'].iloc[idx]\n",
    "    learning_rate = predictions['learning rate'].iloc[idx]\n",
    "    epochs = '2000'\n",
    "    df_glob = gl.global_predictions_loader(\n",
    "        coregistration = coregistration,\n",
    "        architecture = architecture,\n",
    "        learning_rate = learning_rate,\n",
    "        epochs = epochs\n",
    "\n",
    "    )\n",
    "    \n",
    "\n",
    "    df = pd.concat([df,df_glob])\n",
    "# df = df[[\n",
    "#         'RGIId','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "#         '11','12','13','14','15','16','17','18','19','20','21',\n",
    "#         '22','23','24',\n",
    "# ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc352412",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns = {\n",
    "    'architecture':'layer architecture'\n",
    "})\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.merge(predictions, deviations, on = 'layer architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2655f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = df_weights[[\n",
    "    'layer architecture',\n",
    "    'coregistration',\n",
    "    'predicted volume',\n",
    "    'std dev',\n",
    "    'architecture weight'\n",
    "]]\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b284628",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mean = sum(\n",
    "    df_weights['predicted volume'] * (1/(df_weights['std dev'])**2) * (df_weights['architecture weight'])\n",
    ") / sum((1/(df_weights['std dev'])**2) * (df_weights['architecture weight']))\n",
    "        \n",
    "new_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd26304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load deviations table \n",
    "rootdir = 'zults/'\n",
    "predictions = pd.DataFrame()\n",
    "deviations = pd.DataFrame()\n",
    "\n",
    "for file in tqdm(os.listdir(rootdir)):\n",
    "    if 'predictions' in file:\n",
    "        file_reader = pd.read_csv(rootdir + file)\n",
    "        predictions = predictions.append(file_reader, ignore_index = True)\n",
    "    if 'deviations' in file:\n",
    "        file_reader = pd.read_csv(rootdir + file)\n",
    "        deviations = pd.concat([deviations, file_reader], ignore_index = True)\n",
    "\n",
    "deviations = deviations.drop('Unnamed: 0', axis = 1)\n",
    "predictions = predictions.drop('Unnamed: 0', axis = 1)\n",
    "# deviations['total parameters'] = deviations['total parameters'].astype(int)\n",
    "# deviations['trained parameters'] = deviations['trained parameters'].astype(int)\n",
    "# deviations['total inputs'] = deviations['total inputs'].astype(int)\n",
    "deviations = deviations[\n",
    "    (deviations['df'].str.contains(coregistration)) \n",
    "#     &\n",
    "#     (deviations['layer architecture'] == '10-5')\n",
    "    &\n",
    "    (deviations['learning rate'] == 0.01)\n",
    "#     &\n",
    "#     (deviations['epochs'] == 999)\n",
    "    &\n",
    "    (deviations['dropout'] == 1)\n",
    "]\n",
    "deviations['test - train'] = (\n",
    "    abs(deviations['test mae avg'] - deviations['train mae avg'])\n",
    ")\n",
    "deviations = deviations.sort_values(\n",
    "    [\n",
    "        'layer architecture',\n",
    "        'test - train',\n",
    "        'epochs',\n",
    "        'test mae avg', \n",
    "        'train mae avg',\n",
    "        'test predicted thickness std dev',\n",
    "        'layer architecture',\n",
    "        'learning rate',\n",
    "        'df',\n",
    "        'layer architecture'\n",
    "    ]\n",
    ")\n",
    "deviations['paramater ratio'] = deviations['trained parameters'] / deviations['total inputs']\n",
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ecc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations['architecture weight'].dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df, deviations, on = 'layer architecture')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[\n",
    "        'RGIId','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "        '11','12','13','14','15','16','17','18','19','20','21',\n",
    "        '22','23','24','architecture weight'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82681ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ba4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_raw = test.groupby('RGIId')[\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "        '11','12','13','14','15','16','17','18','19','20','21',\n",
    "        '22','23','24', 'architecture weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['RGIId'] == 'RGI60-11.00001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = df_glob[[\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "        '11','12','13','14','15','16','17','18','19','20','21',\n",
    "        '22','23','24',\n",
    "]].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ed217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glob['test_mean'] = test_mean\n",
    "df_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_rgi_id, obj in tqdm(compiled_raw):\n",
    "    print(this_rgi_id)\n",
    "    print(obj)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame()\n",
    "\n",
    "for this_rgi_id, obj in tqdm(compiled_raw):\n",
    "    print(this_rgi_id)\n",
    "    print(obj)\n",
    "    \n",
    "    obj['weight'] = obj['architecture weight'] + obj[['0', '1', '2', '3', '4',\n",
    "                                                     '5', '6', '7', '8', '9',\n",
    "                                                     '10','11','12','13','14',\n",
    "                                                     '15','16','17','18','19',\n",
    "                                                     '20','21','22','23','24']].std(axis = 1)\n",
    "    \n",
    "    \n",
    "    obj['weighted mean'] = obj['weight'] * obj[['0', '1', '2', '3', '4',\n",
    "                                               '5', '6', '7', '8', '9',\n",
    "                                               '10','11','12','13','14',\n",
    "                                               '15','16','17','18','19',\n",
    "                                               '20','21','22','23','24']].mean(axis = 1)\n",
    "    \n",
    "    \n",
    "    weighted_glacier_mean = sum(obj['weighted mean']) / sum(obj['weight'])\n",
    "    break\n",
    "#     rgi_id = pd.Series(this_rgi_id, name = 'RGIId')\n",
    "#     dft = pd.concat([dft, rgi_id])\n",
    "#     dft = dft.reset_index()\n",
    "#     dft = dft.drop('index', axis = 1)\n",
    "    \n",
    "#     weighted_glacier_mean_entry = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a05059",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_glacier_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8567d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Predictions compiled')\n",
    "print('Aggregating statistics...')\n",
    "dft = pd.DataFrame()\n",
    "for this_rgi_id, obj in tqdm(compiled_raw):\n",
    "    rgi_id = pd.Series(this_rgi_id, name = 'RGIId')\n",
    "#     print(f\"Data associated with RGI_ID = {this_rgi_id}:\")\n",
    "    dft = pd.concat([dft, rgi_id])\n",
    "    dft = dft.reset_index()\n",
    "    dft = dft.drop('index', axis = 1)\n",
    "    obj['weight'] = obj['architecture weight'] + obj[['0', '1', '2', '3', '4',\n",
    "                                                     '5', '6', '7', '8', '9',\n",
    "                                                     '10','11','12','13','14',\n",
    "                                                     '15','16','17','18','19',\n",
    "                                                     '20','21','22','23','24']].std(axis = 1)\n",
    "    \n",
    "    \n",
    "    obj['weighted mean'] = obj['weight'] * obj[['0', '1', '2', '3', '4',\n",
    "                                               '5', '6', '7', '8', '9',\n",
    "                                               '10','11','12','13','14',\n",
    "                                               '15','16','17','18','19',\n",
    "                                               '20','21','22','23','24']].mean(axis = 1)\n",
    "    \n",
    "    \n",
    "    weighted_glacier_mean = sum(obj['weighted mean']) / sum(obj['weight'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    stacked_object = obj[[\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9','10',\n",
    "        '11','12','13','14','15','16','17','18','19','20','21',\n",
    "        '22','23','24',\n",
    "    ]].stack()\n",
    "    \n",
    "    glacier_count = len(stacked_object)\n",
    "    dft.loc[dft.index[-1], 'Weighted Mean Thickness'] = weighted_glacier_mean\n",
    "    dft.loc[dft.index[-1], 'Mean Thickness'] = stacked_object.mean()\n",
    "    dft.loc[dft.index[-1], 'Median Thickness'] = stacked_object.median()\n",
    "    dft.loc[dft.index[-1],'Thickness Std Dev'] = stacked_object.std()\n",
    "    \n",
    "    statistic, p_value = shapiro(stacked_object)    \n",
    "    dft.loc[dft.index[-1],'Shapiro-Wilk statistic'] = statistic\n",
    "    dft.loc[dft.index[-1],'Shapiro-Wilk p_value'] = p_value\n",
    "\n",
    "    \n",
    "    q75, q25 = np.percentile(stacked_object, [75, 25])    \n",
    "    dft.loc[dft.index[-1],'IQR'] = q75 - q25 \n",
    "    \n",
    "    lower_bound = np.percentile(stacked_object, 50 - 34.1)\n",
    "    median = np.percentile(stacked_object, 50)\n",
    "    upper_bound = np.percentile(stacked_object, 50 + 34.1)\n",
    "    \n",
    "    dft.loc[dft.index[-1],'Lower Bound'] = lower_bound\n",
    "    dft.loc[dft.index[-1],'Upper Bound'] = upper_bound\n",
    "    dft.loc[dft.index[-1],'Median Value'] = median\n",
    "    dft.loc[dft.index[-1],'Total estimates'] = glacier_count\n",
    "    \n",
    "    break\n",
    "    \n",
    "dft = dft.rename(columns = {\n",
    "    0:'RGIId'\n",
    "})\n",
    "dft = dft.drop_duplicates()\n",
    "# dft.to_csv(\n",
    "#     'predicted_thicknesses/sermeq_aggregated_bootstrap_predictions_coregistration_' + \n",
    "#     coregistration + '.csv'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
