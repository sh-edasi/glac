{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3331c805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 20:35:04.230467: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade tensorflow \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glacierml as gl\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tensorflow.python.util import deprecation\n",
    "import os\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import janitor\n",
    "import tabulate\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ec87af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a3c61",
   "metadata": {},
   "source": [
    "# Step 1.\n",
    "### Load and define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b11ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glam = pd.read_csv('Glam.csv')\n",
    "Glam = Glam[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'thickness',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(Glam)\n",
    "\n",
    "# define model hyperparameters\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "Glam.name = 'Glam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9921e",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "### Load and models, then make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4553b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:15<00:00, 39.02s/it]\n"
     ]
    }
   ],
   "source": [
    "rootdir = 'sm2/'\n",
    "# print(rootdir)\n",
    "dnn_model={}\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            \n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c75c7bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer architecture</th>\n",
       "      <th>model parameters</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>validation split</th>\n",
       "      <th>test mae avg</th>\n",
       "      <th>train mae avg</th>\n",
       "      <th>test mae std dev</th>\n",
       "      <th>train mae std dev</th>\n",
       "      <th>test predicted thickness std dev</th>\n",
       "      <th>train predicted thickness std dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.207832</td>\n",
       "      <td>13.195737</td>\n",
       "      <td>0.325927</td>\n",
       "      <td>0.427030</td>\n",
       "      <td>0.591357</td>\n",
       "      <td>0.486747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.882613</td>\n",
       "      <td>14.671638</td>\n",
       "      <td>0.255216</td>\n",
       "      <td>0.218825</td>\n",
       "      <td>0.364422</td>\n",
       "      <td>0.352042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.949542</td>\n",
       "      <td>15.135346</td>\n",
       "      <td>0.284604</td>\n",
       "      <td>0.285442</td>\n",
       "      <td>0.520103</td>\n",
       "      <td>0.430229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.091524</td>\n",
       "      <td>15.287114</td>\n",
       "      <td>0.382745</td>\n",
       "      <td>0.327079</td>\n",
       "      <td>0.488009</td>\n",
       "      <td>0.434572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.258951</td>\n",
       "      <td>12.697351</td>\n",
       "      <td>0.994132</td>\n",
       "      <td>0.708047</td>\n",
       "      <td>1.165205</td>\n",
       "      <td>0.954331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.308620</td>\n",
       "      <td>12.831752</td>\n",
       "      <td>0.602335</td>\n",
       "      <td>0.627691</td>\n",
       "      <td>0.797038</td>\n",
       "      <td>0.746737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.468252</td>\n",
       "      <td>11.856179</td>\n",
       "      <td>0.910006</td>\n",
       "      <td>0.837275</td>\n",
       "      <td>0.966888</td>\n",
       "      <td>0.882962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.658732</td>\n",
       "      <td>13.962586</td>\n",
       "      <td>3.111561</td>\n",
       "      <td>3.332467</td>\n",
       "      <td>4.761145</td>\n",
       "      <td>4.271081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10-5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.284519</td>\n",
       "      <td>11.670497</td>\n",
       "      <td>1.606948</td>\n",
       "      <td>1.948284</td>\n",
       "      <td>1.970232</td>\n",
       "      <td>1.917905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.459327</td>\n",
       "      <td>10.380450</td>\n",
       "      <td>1.225341</td>\n",
       "      <td>1.412582</td>\n",
       "      <td>4.272036</td>\n",
       "      <td>3.714475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.536946</td>\n",
       "      <td>12.728193</td>\n",
       "      <td>2.003515</td>\n",
       "      <td>2.384888</td>\n",
       "      <td>2.387979</td>\n",
       "      <td>2.266509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8-4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.579442</td>\n",
       "      <td>16.821427</td>\n",
       "      <td>6.111449</td>\n",
       "      <td>6.122259</td>\n",
       "      <td>7.824829</td>\n",
       "      <td>7.255960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-6</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.925964</td>\n",
       "      <td>10.796466</td>\n",
       "      <td>1.243910</td>\n",
       "      <td>1.232902</td>\n",
       "      <td>2.634163</td>\n",
       "      <td>2.056257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-8</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.166462</td>\n",
       "      <td>9.875847</td>\n",
       "      <td>1.506537</td>\n",
       "      <td>0.981434</td>\n",
       "      <td>2.400590</td>\n",
       "      <td>1.982836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.320735</td>\n",
       "      <td>8.739548</td>\n",
       "      <td>1.129237</td>\n",
       "      <td>0.922487</td>\n",
       "      <td>1.347646</td>\n",
       "      <td>0.952754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer architecture  model parameters learning rate  validation split  \\\n",
       "8             32-16-8            1012.0         0.001               0.2   \n",
       "2                16-8             324.0         0.001               0.2   \n",
       "5                12-6             224.0         0.001               0.2   \n",
       "11               10-5             180.0         0.001               0.2   \n",
       "4                12-6             224.0          0.01               0.2   \n",
       "10               10-5             180.0          0.01               0.2   \n",
       "1                16-8             324.0          0.01               0.2   \n",
       "13                8-4             140.0          0.01               0.2   \n",
       "9                10-5             180.0           0.1               0.2   \n",
       "6             32-16-8            1012.0           0.1               0.2   \n",
       "12                8-4             140.0           0.1               0.2   \n",
       "14                8-4             140.0         0.001               0.2   \n",
       "3                12-6             224.0           0.1               0.2   \n",
       "0                16-8             324.0           0.1               0.2   \n",
       "7             32-16-8            1012.0          0.01               0.2   \n",
       "\n",
       "    test mae avg  train mae avg  test mae std dev  train mae std dev  \\\n",
       "8      12.207832      13.195737          0.325927           0.427030   \n",
       "2      12.882613      14.671638          0.255216           0.218825   \n",
       "5      12.949542      15.135346          0.284604           0.285442   \n",
       "11     13.091524      15.287114          0.382745           0.327079   \n",
       "4      13.258951      12.697351          0.994132           0.708047   \n",
       "10     13.308620      12.831752          0.602335           0.627691   \n",
       "1      13.468252      11.856179          0.910006           0.837275   \n",
       "13     13.658732      13.962586          3.111561           3.332467   \n",
       "9      14.284519      11.670497          1.606948           1.948284   \n",
       "6      14.459327      10.380450          1.225341           1.412582   \n",
       "12     14.536946      12.728193          2.003515           2.384888   \n",
       "14     14.579442      16.821427          6.111449           6.122259   \n",
       "3      14.925964      10.796466          1.243910           1.232902   \n",
       "0      15.166462       9.875847          1.506537           0.981434   \n",
       "7      15.320735       8.739548          1.129237           0.922487   \n",
       "\n",
       "    test predicted thickness std dev  train predicted thickness std dev  \n",
       "8                           0.591357                           0.486747  \n",
       "2                           0.364422                           0.352042  \n",
       "5                           0.520103                           0.430229  \n",
       "11                          0.488009                           0.434572  \n",
       "4                           1.165205                           0.954331  \n",
       "10                          0.797038                           0.746737  \n",
       "1                           0.966888                           0.882962  \n",
       "13                          4.761145                           4.271081  \n",
       "9                           1.970232                           1.917905  \n",
       "6                           4.272036                           3.714475  \n",
       "12                          2.387979                           2.266509  \n",
       "14                          7.824829                           7.255960  \n",
       "3                           2.634163                           2.056257  \n",
       "0                           2.400590                           1.982836  \n",
       "7                           1.347646                           0.952754  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_Glam_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "# deviations['training split'] = deviations['test mae avg'] - deviations['train mae avg']\n",
    "# too_low = deviations.index[deviations['training split'] < 0]\n",
    "# deviations = deviations.drop(too_low)\n",
    "deviations = deviations.sort_values('test mae avg')\n",
    "deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbae31",
   "metadata": {},
   "source": [
    "# Build RGI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b39c9ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_rgi60_Alaska.csv\n",
      "02_rgi60_WesternCanadaUS.csv\n",
      "03_rgi60_ArcticCanadaNorth.csv\n",
      "04_rgi60_ArcticCanadaSouth.csv\n",
      "05_rgi60_GreenlandPeriphery.csv\n",
      "06_rgi60_Iceland.csv\n",
      "07_rgi60_Svalbard.csv\n",
      "08_rgi60_Scandinavia.csv\n",
      "09_rgi60_RussianArctic.csv\n",
      "10_rgi60_NorthAsia.csv\n",
      "11_rgi60_CentralEurope.csv\n",
      "12_rgi60_CaucasusMiddleEast.csv\n",
      "13_rgi60_CentralAsia.csv\n",
      "14_rgi60_SouthAsiaWest.csv\n",
      "15_rgi60_SouthAsiaEast.csv\n",
      "16_rgi60_LowLatitudes.csv\n",
      "17_rgi60_SouthernAndes.csv\n",
      "18_rgi60_NewZealand.csv\n",
      "19_rgi60_AntarcticSubantarctic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:48<00:00,  4.33s/it]\n",
      "  0%|          | 0/208315 [00:00<?, ?it/s]/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "100%|██████████| 208315/208315 [03:21<00:00, 1031.87it/s]\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/data/fast0/datasets/rgi60-attribs/'\n",
    "RGI_extra = pd.DataFrame()\n",
    "for file in os.listdir(rootdir):\n",
    "    print(file)\n",
    "    f = pd.read_csv(rootdir+file, encoding_errors = 'replace', on_bad_lines = 'skip')\n",
    "    RGI_extra = RGI_extra.append(f, ignore_index = True)\n",
    "\n",
    "RGI = RGI_extra[[\n",
    "    'CenLat',\n",
    "    'CenLon',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Area',\n",
    "    'Aspect',\n",
    "    'Lmax'\n",
    "]]\n",
    "bad_zmed = RGI.loc[RGI['Zmed']<0].index\n",
    "RGI = RGI.drop(bad_zmed)\n",
    "bad_lmax = RGI.loc[RGI['Lmax']<0].index\n",
    "RGI = RGI.drop(bad_lmax)\n",
    "bad_slope = RGI.loc[RGI['Slope']<0].index\n",
    "RGI = RGI.drop(bad_slope)\n",
    "bad_aspect = RGI.loc[RGI['Aspect']<0].index\n",
    "RGI = RGI.drop(bad_aspect)\n",
    "RGI = RGI.reset_index()\n",
    "RGI = RGI.drop('index', axis=1)\n",
    "\n",
    "\n",
    "arch = deviations['layer architecture'].iloc[0]\n",
    "lr = deviations['learning rate'].iloc[0]\n",
    "vs = deviations['validation split'].iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we predict thicknesses across all 25 of the top performing ensemble model\n",
    "\"\"\"\n",
    "# dfs = pd.DataFrame()\n",
    "# for rs in tqdm(RS):\n",
    "#     df = deviations.iloc[:1]\n",
    "#     s = pd.Series(dnn_model[str(arch) + '_Glam_dnn_MULTI_' + str(lr) + '_' + str(vs) + '_' +str(ep) + '_' + str(rs)].predict(RGI, verbose=0).flatten(), name = rs)\n",
    "#     dfs[rs] = s\n",
    "    \n",
    "dfs = pd.DataFrame()\n",
    "for rs in tqdm(RS):\n",
    "    df = deviations.iloc[:1]\n",
    "    s = pd.Series(dnn_model[str(arch) +'_Glam_dnn_MULTI_'+str(lr)+'_'+str(vs)+'_300_'+ str(rs)].predict(RGI, verbose=0).flatten(), name = rs)\n",
    "    dfs[rs] = s\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Here we compute the average thickness for each RGI glacier\n",
    "\"\"\"\n",
    "RGI['avg predicted thickness'] = 'NaN'\n",
    "for i in tqdm(dfs.index):\n",
    "    avg_predicted_thickness = np.sum(dfs.loc[i])/len(dfs.loc[i])\n",
    "#     print(i)\n",
    "#     print(np.sum(dfs.loc[i])/len(dfs.loc[i]))\n",
    "#     print('')\n",
    "#     break\n",
    "    RGI['avg predicted thickness'].loc[i] = avg_predicted_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "620f8824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208315/208315 [19:16<00:00, 180.19it/s]\n"
     ]
    }
   ],
   "source": [
    "RGI['predicted thickness std dev'] = 'NaN'\n",
    "for i in tqdm(dfs.index):\n",
    "    # step 1: calculate mean of numbers\n",
    "    avg_predicted_thickness = np.sum(dfs.loc[i])/len(dfs.loc[i])\n",
    "    \n",
    "    # step 2: subtract the mean from each, then square the result\n",
    "    \n",
    "    diff_sq = pd.Series()\n",
    "\n",
    "    for q in dfs:\n",
    "        \n",
    "        avg_predicted_thickness - dfs[q].loc[i]\n",
    "        step_2 = pd.Series((avg_predicted_thickness - dfs[q].loc[i])**2)\n",
    "        diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "    \n",
    "    # step 3: work out the mean of the squared differences    \n",
    "    mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "    # step 4: take the square root\n",
    "    prethick_std_dev = np.sqrt(mean_diff_sq)\n",
    "    RGI['predicted thickness std dev'].loc[i] = prethick_std_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a0966fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RGI['variance'] = (RGI['predicted thickness std dev'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc696aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLat</th>\n",
       "      <th>CenLon</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Area</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Lmax</th>\n",
       "      <th>avg predicted thickness</th>\n",
       "      <th>predicted thickness std dev</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.6890</td>\n",
       "      <td>-146.823</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>2385</td>\n",
       "      <td>2725</td>\n",
       "      <td>0.360</td>\n",
       "      <td>346</td>\n",
       "      <td>839</td>\n",
       "      <td>1289.063906</td>\n",
       "      <td>278.267929</td>\n",
       "      <td>77433.040075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.4040</td>\n",
       "      <td>-146.668</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1713</td>\n",
       "      <td>2005</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.558</td>\n",
       "      <td>162</td>\n",
       "      <td>1197</td>\n",
       "      <td>1158.236172</td>\n",
       "      <td>241.924754</td>\n",
       "      <td>58527.58655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.3760</td>\n",
       "      <td>-146.080</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1609</td>\n",
       "      <td>1868</td>\n",
       "      <td>2182</td>\n",
       "      <td>1.685</td>\n",
       "      <td>175</td>\n",
       "      <td>2106</td>\n",
       "      <td>1083.203984</td>\n",
       "      <td>227.856378</td>\n",
       "      <td>51918.529149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.3810</td>\n",
       "      <td>-146.120</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1273</td>\n",
       "      <td>1944</td>\n",
       "      <td>2317</td>\n",
       "      <td>3.681</td>\n",
       "      <td>195</td>\n",
       "      <td>4175</td>\n",
       "      <td>845.136172</td>\n",
       "      <td>181.51366</td>\n",
       "      <td>32947.20882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.5510</td>\n",
       "      <td>-147.057</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1494</td>\n",
       "      <td>1914</td>\n",
       "      <td>2317</td>\n",
       "      <td>2.573</td>\n",
       "      <td>181</td>\n",
       "      <td>2981</td>\n",
       "      <td>1004.057344</td>\n",
       "      <td>211.500102</td>\n",
       "      <td>44732.293064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208310</th>\n",
       "      <td>-43.4215</td>\n",
       "      <td>170.354</td>\n",
       "      <td>34.7</td>\n",
       "      <td>1231</td>\n",
       "      <td>1724</td>\n",
       "      <td>2098</td>\n",
       "      <td>0.189</td>\n",
       "      <td>116</td>\n",
       "      <td>944</td>\n",
       "      <td>858.659922</td>\n",
       "      <td>181.499106</td>\n",
       "      <td>32941.925534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208311</th>\n",
       "      <td>-43.4550</td>\n",
       "      <td>170.349</td>\n",
       "      <td>36.1</td>\n",
       "      <td>1881</td>\n",
       "      <td>2106</td>\n",
       "      <td>2208</td>\n",
       "      <td>0.040</td>\n",
       "      <td>108</td>\n",
       "      <td>331</td>\n",
       "      <td>1315.604375</td>\n",
       "      <td>272.310819</td>\n",
       "      <td>74153.182011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208312</th>\n",
       "      <td>-43.4400</td>\n",
       "      <td>170.351</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1677</td>\n",
       "      <td>1974</td>\n",
       "      <td>2253</td>\n",
       "      <td>0.184</td>\n",
       "      <td>104</td>\n",
       "      <td>740</td>\n",
       "      <td>1168.286641</td>\n",
       "      <td>244.716841</td>\n",
       "      <td>59886.332135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208313</th>\n",
       "      <td>-43.4106</td>\n",
       "      <td>170.364</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1627</td>\n",
       "      <td>1839</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.111</td>\n",
       "      <td>135</td>\n",
       "      <td>406</td>\n",
       "      <td>1139.257656</td>\n",
       "      <td>236.082618</td>\n",
       "      <td>55735.002431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208314</th>\n",
       "      <td>-43.3829</td>\n",
       "      <td>170.323</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1837</td>\n",
       "      <td>1927</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.085</td>\n",
       "      <td>207</td>\n",
       "      <td>292</td>\n",
       "      <td>1292.534766</td>\n",
       "      <td>263.953222</td>\n",
       "      <td>69671.303402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208315 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CenLat   CenLon  Slope  Zmin  Zmed  Zmax   Area  Aspect  Lmax  \\\n",
       "0       63.6890 -146.823   42.0  1936  2385  2725  0.360     346   839   \n",
       "1       63.4040 -146.668   16.0  1713  2005  2144  0.558     162  1197   \n",
       "2       63.3760 -146.080   18.0  1609  1868  2182  1.685     175  2106   \n",
       "3       63.3810 -146.120   19.0  1273  1944  2317  3.681     195  4175   \n",
       "4       63.5510 -147.057   16.0  1494  1914  2317  2.573     181  2981   \n",
       "...         ...      ...    ...   ...   ...   ...    ...     ...   ...   \n",
       "208310 -43.4215  170.354   34.7  1231  1724  2098  0.189     116   944   \n",
       "208311 -43.4550  170.349   36.1  1881  2106  2208  0.040     108   331   \n",
       "208312 -43.4400  170.351   39.2  1677  1974  2253  0.184     104   740   \n",
       "208313 -43.4106  170.364   34.0  1627  1839  1928  0.111     135   406   \n",
       "208314 -43.3829  170.323   30.1  1837  1927  1992  0.085     207   292   \n",
       "\n",
       "       avg predicted thickness predicted thickness std dev      variance  \n",
       "0                  1289.063906                  278.267929  77433.040075  \n",
       "1                  1158.236172                  241.924754   58527.58655  \n",
       "2                  1083.203984                  227.856378  51918.529149  \n",
       "3                   845.136172                   181.51366   32947.20882  \n",
       "4                  1004.057344                  211.500102  44732.293064  \n",
       "...                        ...                         ...           ...  \n",
       "208310              858.659922                  181.499106  32941.925534  \n",
       "208311             1315.604375                  272.310819  74153.182011  \n",
       "208312             1168.286641                  244.716841  59886.332135  \n",
       "208313             1139.257656                  236.082618  55735.002431  \n",
       "208314             1292.534766                  263.953222  69671.303402  \n",
       "\n",
       "[208315 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cc0e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted RGI sum thickness\n",
      "442906059.7874193\n",
      "\n",
      "predicted RGI thickness variance\n",
      "53972755496.75347\n"
     ]
    }
   ],
   "source": [
    "print('predicted RGI sum thickness')\n",
    "print(sum(RGI['avg predicted thickness']))\n",
    "print('')\n",
    "print('predicted RGI thickness variance')\n",
    "print(sum(RGI['variance']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ae8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code plots predictions against actual thickness. Not currently modified and will load\n",
    "EVERY SINGLE MODEL. DO NOT USE JUST YET\n",
    "\"\"\"\n",
    "for rs in RS:    \n",
    "    y = dnn_model['10-5_Glam_dnn_MULTI_0.01_0.2_300_'+ str(rs)].predict(test_features, verbose=0)\n",
    "    fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.plot(test_labels,y,'.')\n",
    "    plt.plot((0,300),(0,300),'-')\n",
    "    plt.xlabel('True Thickness (m)')\n",
    "    plt.ylabel('Model Thickness (m)')\n",
    "    ax.set_title('Random State ' +str(rs))\n",
    "    plt.xlim((0,300))\n",
    "    plt.ylim((0,300))\n",
    "    # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820c44a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ccccombo_breaker()\n",
    "# past this point is under construction. \n",
    "# here be monsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99106e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load all histories\n",
    "print('Loading histories....')\n",
    "rootdir = 'sr2/'\n",
    "dnn_history = {}\n",
    "arch = deviations['layer architecture'].iloc[[0]]\n",
    "lr = deviations['learning rate'].iloc[[0]]\n",
    "vs = deviations['validation split'].iloc[[0]]\n",
    "print(arch)\n",
    "print(lr)\n",
    "print(vs)\n",
    "# for arch in tqdm(os.listdir(rootdir)):\n",
    "#     for folder in os.listdir(rootdir+arch):\n",
    "#         if 'MULTI' in folder:\n",
    "#             if 'dnn' in folder:\n",
    "\n",
    "#                 dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This cell plots each random state loss curve for the 25 random states for each run\n",
    "Also loads EVERY SINGLE MODEL currently and blows up the memory. Working on it.\n",
    "\"\"\"\n",
    "# for rs in RS:\n",
    "# for hist in dnn_history:    \n",
    "#     print(hist)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     ax.set_title(hist)\n",
    "#     gl.plot_loss(dnn_history[hist])\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314c7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots the different variable loss curves to show individual variable models\n",
    "Not currently working because we have only loaded dnn_MULTI models\n",
    "\"\"\"\n",
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "fig.patch.set_facecolor('w')\n",
    "# ax.set_ylim([5,30])\n",
    "\n",
    "# gl.plot_loss(dnn_history['T_MULTI'])\n",
    "for i, variable_name in enumerate(list(train_features)):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    gl.plot_loss(dnn_history['glacier_'+ variable_name+ '_0.1_0.2_300_6'])\n",
    "#     ax.set_ylim([35,140])\n",
    "    ax.set_title(variable_name)\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d14cb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns = {'architecture':'layer architecture'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c051640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for rs in RS:\n",
    "    print(rs)\n",
    "    df = deviations.iloc[:1]\n",
    "    s = pd.Series(dnn_model['10-5_Glam_dnn_MULTI_0.01_0.2_300_'+str(rs)].predict(RGI).flatten(), name = rs)\n",
    "    dfs[rs] = s\n",
    "#     break\n",
    "#     dfs = dfs.assign(str(s))\n",
    "print(s)\n",
    "dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5031647",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in predictions[(predictions['learning rate'] == df['learning rate']) and (predictions['layer architecture'] == df['layer architecture'])]:\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in predictions['model']:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9284f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['layer architecture']\n",
    "# df['layer architecture']\n",
    "# predictions['learning rate']\n",
    "# df['learning rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions['layer architecture'].values() == [str(0.1)]:\n",
    "    print(ding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['layer architecture']\n",
    "# predictions[(predictions['learning rate'] == df['learning rate']) and (predictions['layer architecture'] == df['layer architecture'])]:\n",
    "for architecture in predictions['layer architecture']:\n",
    "#     if predictions['layer architecture'] == df['layer architecture'] and predictions['learning rate'] == df['learning rate']:\n",
    "    print('')\n",
    "    print(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = deviations.sort_values('test mae avg')\n",
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2ae64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains code to produce histograms of all the architectures different histories\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = deviations.iloc[:1]\n",
    "\n",
    "\n",
    "for rs in RS:\n",
    "    df = predictions[predictions['architecture'] == architecture]\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3251ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to produce histograms of all the architectures different histories\n",
    "# \"\"\"\n",
    "\n",
    "# for architecture in list(predictions['architecture'].unique()):\n",
    "#     df = predictions[predictions['architecture'] == architecture]\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.set_title('Layer architecture: ' + architecture)\n",
    "#     ax.set_ylabel('prediction count')\n",
    "#     ax.set_xlabel('thickness (m)')\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.hist(df['avg test thickness'])\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6af61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots each random state loss curve for the 25 random states for each run\n",
    "Also loads EVERY SINGLE MODEL currently and blows up the memory. Working on it.\n",
    "\"\"\"\n",
    "# for rs in RS:\n",
    "for hist in dnn_history:    \n",
    "    fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "    fig.patch.set_facecolor('w')\n",
    "    ax.set_title(hist)\n",
    "    gl.plot_loss(dnn_history[hist])\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modl in dnn_modl:    \n",
    "#     y = dnn_modl[modl].predict(test_features)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.plot(test_labels,y,'.')\n",
    "#     plt.plot((0,300),(0,300),'-')\n",
    "#     plt.xlabel('True Thickness (m)')\n",
    "#     plt.ylabel('Model Thickness (m)')\n",
    "#     plt.xlim((0,300))\n",
    "#     plt.ylim((0,300))\n",
    "#     # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all histories\n",
    "print('Loading histories....')\n",
    "rootdir = 'sr2/'\n",
    "dnn_history = {}\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder:\n",
    "            if 'dnn' in folder:\n",
    "\n",
    "                dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc93aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95568cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a density plot of the most recent predictions made. Can easily be modified in a loop\n",
    "to show multiple random states and whatnot\n",
    "\"\"\"\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "sns.kdeplot(x = test_labels, y = y.flatten(),fill = True)\n",
    "plt.plot((0,300),(0,300),'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e735a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deviations.sort_values('test mae avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a741cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a7383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # load all histories\n",
    "# qqq = deviations.sort_values('test mae avg')\n",
    "\n",
    "\n",
    "for rs in RS:\n",
    "#     dnn_histor+y[qqq['layer architecture']+\n",
    "#                 'Glam_dnn_history_MULTI_'+\n",
    "#                 qqq['learning rate']+\n",
    "#                 '_0.2_300_'+\n",
    "#                 rs]\n",
    "    gl.plot_loss(dnn_history[qqq['layer architecture'].iloc[0]+\n",
    "                '_Glam_dnn_history_MULTI_'+\n",
    "                qqq['learning rate'].iloc[0]+\n",
    "                '_0.2_300_'+\n",
    "                str(rs)])\n",
    "# print('Loading histories....')\n",
    "# rootdir = 'sr2/'\n",
    "# dnn_history = {}\n",
    "# for arch in tqdm(os.listdir(rootdir)):\n",
    "#     for folder in os.listdir(rootdir+arch):\n",
    "#         if 'MULTI' in folder and 'dnn' in folder:\n",
    "\n",
    "#             dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa78f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fee1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[qqq['layer architecture'].iloc[0]+\n",
    "                    '_Glam_dnn_history_MULTI_'+\n",
    "                    qqq['learning rate'].iloc[0]+\n",
    "                    '_0.2_300_'+\n",
    "                    str(rs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_single_variable(x,y,model_type,model_name,feature_name):\n",
    "#     y = model_type[model_name].predict(test_labels)\n",
    "#     plt.scatter(train_features[feature_name], train_labels, label='Data')\n",
    "#     plt.plot(x, y,'.', color='k', label='Predictions')\n",
    "#     plt.xlabel(feature_name)\n",
    "#     plt.ylabel('THICKNESS')\n",
    "#     plt.legend()\n",
    "#     plt.plot()\n",
    "\n",
    "# x = test_labels\n",
    "# for i, variable_name in enumerate(list(train_features)):\n",
    "#     ax = plt.subplot(2,2,i+1)\n",
    "#     model_name = (dataset.name \n",
    "#     + '_' \n",
    "#     + variable_name \n",
    "#     + '_' \n",
    "#     + str(lr) \n",
    "#     + '_' \n",
    "#     + str(vs) \n",
    "#     + '_' \n",
    "#     + str(ep))\n",
    "#     plot_single_variable(x,y,dnn_model, model_name,variable_name)\n",
    "# #     ax.set_ylim([35,140])\n",
    "# #     ax.set_title(variable_name)\n",
    "# #     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea1269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to load models and histories.\n",
    "# \"\"\"\n",
    "\n",
    "# # fix hyperparameters \n",
    "# lr = 0.1\n",
    "# vs = 0.2\n",
    "# # load models\n",
    "# print('Loading models....')\n",
    "\n",
    "# linear_model = {}\n",
    "# dnn_model = {}\n",
    "# # data is already split, however if other databases are used, then this line is needed\n",
    "# # (train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_linear_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_model[\n",
    "#             'glacier' \n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "        \n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_dnn_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_model[\n",
    "#             'glacier'\n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "# print('Models loaded')\n",
    "\n",
    "# # load all histories\n",
    "# print('Loading histories....')\n",
    "# linear_history = {}\n",
    "# dnn_history = {}\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier' \n",
    "#             +'_linear_history_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ]= pd.read_csv(file_name)\n",
    "\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier_dnn_history_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = pd.read_csv(file_name)\n",
    "# print('Histories loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03759a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This cell loads the loss histories from the original model with one dense layer of 64 nodes.\n",
    "From these histories I extracted the lowest and final loss value and plotted them.\n",
    "First is plotted the loss from using different learning rates with a fixed validation split. \n",
    "Then follows a plot of different validation splits using a fixed learning rate.\n",
    "\"\"\"\n",
    "\n",
    "# set up dictionaries\n",
    "loss = {}\n",
    "dnn_lr_history = {}\n",
    "loss['glacier_min_learn'] = pd.DataFrame()\n",
    "loss['glacier_fin_learn'] = pd.DataFrame()\n",
    "\n",
    "# loop to define and then load histories\n",
    "for lr in LR:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    # find minimum and insert other model hyperparameters into table\n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_learn'] = loss['glacier_min_learn'].append(m_loss,ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # find final and insert other model hyperparameters into table\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_learn'] = loss['glacier_fin_learn'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_learn'] = loss['glacier_fin_learn'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_learn'] = loss['glacier_min_learn'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Learning rate at fixed validation split = 0.2')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# repeat above loop with fixed lr and varied vs\n",
    "lr = 0.1\n",
    "loss = {}\n",
    "dnn_vs_history = {}\n",
    "loss['glacier_min_valsplit'] = pd.DataFrame()\n",
    "loss['glacier_fin_valsplit'] = pd.DataFrame()\n",
    "for vs in VS:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    \n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].append(m_loss,ignore_index=True)\n",
    "\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Validation splits with learning rate = 0.1')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell computes the true average thickness of the glaciers in use\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pth = '/data/fast1/glacierml/T_models/'\n",
    "T_lab = pd.read_csv(pth + 'T.csv', low_memory = False)\n",
    "T_lab = T_lab[[\n",
    "    'GlaThiDa_ID',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'AREA',\n",
    "    'MEAN_SLOPE',\n",
    "    'MEAN_THICKNESS'\n",
    "]]\n",
    "T_lab = T_lab.dropna()\n",
    "\n",
    "tru_thickness = np.sum(T_lab['MEAN_THICKNESS']) / len(T_lab['MEAN_THICKNESS'])\n",
    "tru_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1983d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set definitions\n",
    "glacier = gl.data_loader(pth = '/data/fast1/glacierml/T_models/')\n",
    "# ,TT,TTT,TTTx,TTT_full\n",
    "# T_t = T.head()\n",
    "\n",
    "# rename thickness column in database\n",
    "gl.thickness_renamer(glacier)\n",
    "\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "\n",
    "# define model hyperparameters\n",
    "LR = np.logspace(-3,2,6)\n",
    "vs = 0.2\n",
    "VS = 0.1,0.15,0.2,0.25,0.3,0.35,0.4\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "glacier.name = 'glacier'\n",
    "# T_t.name = 'T_t'\n",
    "# TT.name = 'band'\n",
    "# TTT.name = 'point'\n",
    "# TTTx.name = 'TTTx'\n",
    "# TTT_full.name = 'TTT_full'\n",
    "\n",
    "# old definitions, legacy code.\n",
    "\n",
    "# arch = '16-8'\n",
    "# pth_mod = 'sm/sm_' + arch + '/'\n",
    "# pth_res = 'sr/sr_' + arch + '/'\n",
    "\n",
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "rootdir = 'sm/'\n",
    "# print(rootdir)\n",
    "dnn_model = {}\n",
    "predictions = pd.DataFrame()\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            \n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_glacier_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "deviations.sort_values('test mae avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc509540",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
