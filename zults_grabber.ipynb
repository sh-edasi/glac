{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade tensorflow \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glacierml as gl\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tensorflow.python.util import deprecation\n",
    "import os\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import janitor\n",
    "import tabulate\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ec87af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953217f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]2022-06-01 11:57:07.303646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-01 11:57:07.303666: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-01 11:57:07.303953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████| 5/5 [02:34<00:00, 30.99s/it]\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:141: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:151: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:160: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:176: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:180: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:185: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:189: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer architecture</th>\n",
       "      <th>model parameters</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>validation split</th>\n",
       "      <th>test mae avg</th>\n",
       "      <th>train mae avg</th>\n",
       "      <th>test mae std dev</th>\n",
       "      <th>train mae std dev</th>\n",
       "      <th>test predicted thickness std dev</th>\n",
       "      <th>train predicted thickness std dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10-5</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17.670567</td>\n",
       "      <td>12.744849</td>\n",
       "      <td>0.679880</td>\n",
       "      <td>0.843327</td>\n",
       "      <td>1.090071</td>\n",
       "      <td>0.905652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-8</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.308925</td>\n",
       "      <td>14.548954</td>\n",
       "      <td>0.218444</td>\n",
       "      <td>0.280178</td>\n",
       "      <td>0.384305</td>\n",
       "      <td>0.353112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-6</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.319958</td>\n",
       "      <td>12.050961</td>\n",
       "      <td>0.851421</td>\n",
       "      <td>1.096475</td>\n",
       "      <td>1.206123</td>\n",
       "      <td>1.028272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-8</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.322421</td>\n",
       "      <td>11.700051</td>\n",
       "      <td>1.103772</td>\n",
       "      <td>0.861498</td>\n",
       "      <td>1.318386</td>\n",
       "      <td>0.962624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12-6</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.609337</td>\n",
       "      <td>14.747237</td>\n",
       "      <td>0.317896</td>\n",
       "      <td>0.245189</td>\n",
       "      <td>0.451783</td>\n",
       "      <td>0.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8-4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.018690</td>\n",
       "      <td>15.172641</td>\n",
       "      <td>0.454905</td>\n",
       "      <td>0.330150</td>\n",
       "      <td>0.543278</td>\n",
       "      <td>0.553481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8-4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.246012</td>\n",
       "      <td>14.295843</td>\n",
       "      <td>3.809324</td>\n",
       "      <td>4.364966</td>\n",
       "      <td>6.337331</td>\n",
       "      <td>5.800722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10-5</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.761632</td>\n",
       "      <td>12.292652</td>\n",
       "      <td>1.246131</td>\n",
       "      <td>1.633289</td>\n",
       "      <td>2.115127</td>\n",
       "      <td>1.789434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10-5</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.901357</td>\n",
       "      <td>16.242461</td>\n",
       "      <td>5.787727</td>\n",
       "      <td>6.078379</td>\n",
       "      <td>7.712083</td>\n",
       "      <td>7.255176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8-4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.047016</td>\n",
       "      <td>13.341808</td>\n",
       "      <td>2.385893</td>\n",
       "      <td>2.433213</td>\n",
       "      <td>3.162398</td>\n",
       "      <td>2.551610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-6</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.372857</td>\n",
       "      <td>11.427789</td>\n",
       "      <td>1.398278</td>\n",
       "      <td>0.833182</td>\n",
       "      <td>2.373020</td>\n",
       "      <td>1.766813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-8</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.837230</td>\n",
       "      <td>10.959361</td>\n",
       "      <td>1.503475</td>\n",
       "      <td>1.185662</td>\n",
       "      <td>3.028146</td>\n",
       "      <td>2.317351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32-16-8</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>39.235787</td>\n",
       "      <td>33.692915</td>\n",
       "      <td>12.530244</td>\n",
       "      <td>12.729263</td>\n",
       "      <td>13.934124</td>\n",
       "      <td>13.490813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer architecture  model parameters learning rate  validation split  \\\n",
       "10               10-5             168.0          0.01               0.2   \n",
       "2                16-8             306.0         0.001               0.2   \n",
       "4                12-6             210.0          0.01               0.2   \n",
       "1                16-8             306.0          0.01               0.2   \n",
       "5                12-6             210.0         0.001               0.2   \n",
       "14                8-4             130.0         0.001               0.2   \n",
       "13                8-4             130.0          0.01               0.2   \n",
       "9                10-5             168.0           0.1               0.2   \n",
       "11               10-5             168.0         0.001               0.2   \n",
       "12                8-4             130.0           0.1               0.2   \n",
       "3                12-6             210.0           0.1               0.2   \n",
       "0                16-8             306.0           0.1               0.2   \n",
       "6             32-16-8             978.0           0.1               0.2   \n",
       "\n",
       "    test mae avg  train mae avg  test mae std dev  train mae std dev  \\\n",
       "10     17.670567      12.744849          0.679880           0.843327   \n",
       "2      18.308925      14.548954          0.218444           0.280178   \n",
       "4      18.319958      12.050961          0.851421           1.096475   \n",
       "1      18.322421      11.700051          1.103772           0.861498   \n",
       "5      18.609337      14.747237          0.317896           0.245189   \n",
       "14     19.018690      15.172641          0.454905           0.330150   \n",
       "13     19.246012      14.295843          3.809324           4.364966   \n",
       "9      19.761632      12.292652          1.246131           1.633289   \n",
       "11     19.901357      16.242461          5.787727           6.078379   \n",
       "12     20.047016      13.341808          2.385893           2.433213   \n",
       "3      20.372857      11.427789          1.398278           0.833182   \n",
       "0      20.837230      10.959361          1.503475           1.185662   \n",
       "6      39.235787      33.692915         12.530244          12.729263   \n",
       "\n",
       "    test predicted thickness std dev  train predicted thickness std dev  \n",
       "10                          1.090071                           0.905652  \n",
       "2                           0.384305                           0.353112  \n",
       "4                           1.206123                           1.028272  \n",
       "1                           1.318386                           0.962624  \n",
       "5                           0.451783                           0.419643  \n",
       "14                          0.543278                           0.553481  \n",
       "13                          6.337331                           5.800722  \n",
       "9                           2.115127                           1.789434  \n",
       "11                          7.712083                           7.255176  \n",
       "12                          3.162398                           2.551610  \n",
       "3                           2.373020                           1.766813  \n",
       "0                           3.028146                           2.317351  \n",
       "6                          13.934124                          13.490813  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Glam = pd.read_csv('Glam.csv')\n",
    "Glam = Glam[[\n",
    "#         'LAT',\n",
    "#         'LON',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'thickness',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Aspect'\n",
    "]]\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(Glam)\n",
    "\n",
    "# define model hyperparameters\n",
    "LR = np.logspace(-3,2,6)\n",
    "vs = 0.2\n",
    "VS = 0.1,0.15,0.2,0.25,0.3,0.35,0.4\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "Glam.name = 'Glam'\n",
    "\n",
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "rootdir = 'sm2/'\n",
    "# print(rootdir)\n",
    "dnn_model={}\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            \n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_Glam_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "deviations.sort_values('test mae avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b39c9ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_rgi60_Alaska.csv\n",
      "02_rgi60_WesternCanadaUS.csv\n",
      "03_rgi60_ArcticCanadaNorth.csv\n",
      "04_rgi60_ArcticCanadaSouth.csv\n",
      "05_rgi60_GreenlandPeriphery.csv\n",
      "06_rgi60_Iceland.csv\n",
      "07_rgi60_Svalbard.csv\n",
      "08_rgi60_Scandinavia.csv\n",
      "09_rgi60_RussianArctic.csv\n",
      "10_rgi60_NorthAsia.csv\n",
      "11_rgi60_CentralEurope.csv\n",
      "12_rgi60_CaucasusMiddleEast.csv\n",
      "13_rgi60_CentralAsia.csv\n",
      "14_rgi60_SouthAsiaWest.csv\n",
      "15_rgi60_SouthAsiaEast.csv\n",
      "16_rgi60_LowLatitudes.csv\n",
      "17_rgi60_SouthernAndes.csv\n",
      "18_rgi60_NewZealand.csv\n",
      "19_rgi60_AntarcticSubantarctic.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CenLat</th>\n",
       "      <th>CenLon</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Area</th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.6890</td>\n",
       "      <td>-146.8230</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>2385</td>\n",
       "      <td>2725</td>\n",
       "      <td>0.360</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.4040</td>\n",
       "      <td>-146.6680</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1713</td>\n",
       "      <td>2005</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.558</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.3760</td>\n",
       "      <td>-146.0800</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1609</td>\n",
       "      <td>1868</td>\n",
       "      <td>2182</td>\n",
       "      <td>1.685</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.3810</td>\n",
       "      <td>-146.1200</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1273</td>\n",
       "      <td>1944</td>\n",
       "      <td>2317</td>\n",
       "      <td>3.681</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.5510</td>\n",
       "      <td>-147.0570</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1494</td>\n",
       "      <td>1914</td>\n",
       "      <td>2317</td>\n",
       "      <td>2.573</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216496</th>\n",
       "      <td>-53.9860</td>\n",
       "      <td>-37.7325</td>\n",
       "      <td>29.9</td>\n",
       "      <td>310</td>\n",
       "      <td>-999</td>\n",
       "      <td>510</td>\n",
       "      <td>0.042</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216497</th>\n",
       "      <td>-54.8310</td>\n",
       "      <td>-36.1361</td>\n",
       "      <td>23.6</td>\n",
       "      <td>330</td>\n",
       "      <td>-999</td>\n",
       "      <td>830</td>\n",
       "      <td>0.567</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216498</th>\n",
       "      <td>-54.1884</td>\n",
       "      <td>-37.3018</td>\n",
       "      <td>16.8</td>\n",
       "      <td>10</td>\n",
       "      <td>-999</td>\n",
       "      <td>1110</td>\n",
       "      <td>4.118</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216499</th>\n",
       "      <td>-68.8656</td>\n",
       "      <td>-90.4266</td>\n",
       "      <td>0.4</td>\n",
       "      <td>170</td>\n",
       "      <td>-999</td>\n",
       "      <td>270</td>\n",
       "      <td>0.011</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216500</th>\n",
       "      <td>-46.8972</td>\n",
       "      <td>37.7140</td>\n",
       "      <td>9.6</td>\n",
       "      <td>970</td>\n",
       "      <td>-999</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.528</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216501 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CenLat    CenLon  Slope  Zmin  Zmed  Zmax   Area  Aspect\n",
       "0       63.6890 -146.8230   42.0  1936  2385  2725  0.360     346\n",
       "1       63.4040 -146.6680   16.0  1713  2005  2144  0.558     162\n",
       "2       63.3760 -146.0800   18.0  1609  1868  2182  1.685     175\n",
       "3       63.3810 -146.1200   19.0  1273  1944  2317  3.681     195\n",
       "4       63.5510 -147.0570   16.0  1494  1914  2317  2.573     181\n",
       "...         ...       ...    ...   ...   ...   ...    ...     ...\n",
       "216496 -53.9860  -37.7325   29.9   310  -999   510  0.042     315\n",
       "216497 -54.8310  -36.1361   23.6   330  -999   830  0.567     200\n",
       "216498 -54.1884  -37.3018   16.8    10  -999  1110  4.118     308\n",
       "216499 -68.8656  -90.4266    0.4   170  -999   270  0.011     122\n",
       "216500 -46.8972   37.7140    9.6   970  -999  1170  0.528      35\n",
       "\n",
       "[216501 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootdir = '/data/fast0/datasets/rgi60-attribs/'\n",
    "RGI_extra = pd.DataFrame()\n",
    "for file in os.listdir(rootdir):\n",
    "    print(file)\n",
    "    f = pd.read_csv(rootdir+file, encoding_errors = 'replace', on_bad_lines = 'skip')\n",
    "    RGI_extra = RGI_extra.append(f, ignore_index = True)\n",
    "\n",
    "RGI = RGI_extra[[\n",
    "    'CenLat',\n",
    "    'CenLon',\n",
    "    'Slope',\n",
    "    'Zmin',\n",
    "    'Zmed',\n",
    "    'Zmax',\n",
    "    'Area',\n",
    "    'Aspect'\n",
    "]]\n",
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4772b8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:51<00:00,  4.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1375.664795</td>\n",
       "      <td>2235.253418</td>\n",
       "      <td>1893.332153</td>\n",
       "      <td>1186.515503</td>\n",
       "      <td>1367.845093</td>\n",
       "      <td>1508.116211</td>\n",
       "      <td>1428.133301</td>\n",
       "      <td>2212.804688</td>\n",
       "      <td>-6.886380</td>\n",
       "      <td>2257.913574</td>\n",
       "      <td>...</td>\n",
       "      <td>1519.571777</td>\n",
       "      <td>639.886902</td>\n",
       "      <td>2050.856201</td>\n",
       "      <td>2779.219727</td>\n",
       "      <td>1991.155884</td>\n",
       "      <td>1785.150635</td>\n",
       "      <td>1700.054077</td>\n",
       "      <td>2446.060303</td>\n",
       "      <td>2107.048828</td>\n",
       "      <td>1400.085693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253.308105</td>\n",
       "      <td>2016.756104</td>\n",
       "      <td>1674.117310</td>\n",
       "      <td>1072.242920</td>\n",
       "      <td>1204.446533</td>\n",
       "      <td>1327.840698</td>\n",
       "      <td>1244.838379</td>\n",
       "      <td>1968.371338</td>\n",
       "      <td>-6.531976</td>\n",
       "      <td>1998.309937</td>\n",
       "      <td>...</td>\n",
       "      <td>1327.296265</td>\n",
       "      <td>568.654663</td>\n",
       "      <td>1816.974854</td>\n",
       "      <td>2439.182373</td>\n",
       "      <td>1770.564819</td>\n",
       "      <td>1589.518311</td>\n",
       "      <td>1501.732300</td>\n",
       "      <td>2169.942627</td>\n",
       "      <td>1879.807861</td>\n",
       "      <td>1221.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167.372070</td>\n",
       "      <td>1883.146729</td>\n",
       "      <td>1570.064575</td>\n",
       "      <td>999.327759</td>\n",
       "      <td>1127.453125</td>\n",
       "      <td>1241.112061</td>\n",
       "      <td>1160.122314</td>\n",
       "      <td>1841.486572</td>\n",
       "      <td>-6.269972</td>\n",
       "      <td>1868.291870</td>\n",
       "      <td>...</td>\n",
       "      <td>1247.981689</td>\n",
       "      <td>534.958313</td>\n",
       "      <td>1695.660889</td>\n",
       "      <td>2282.581787</td>\n",
       "      <td>1656.706787</td>\n",
       "      <td>1481.844360</td>\n",
       "      <td>1409.592285</td>\n",
       "      <td>2026.462769</td>\n",
       "      <td>1756.947876</td>\n",
       "      <td>1151.642334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895.391785</td>\n",
       "      <td>1462.211914</td>\n",
       "      <td>1234.246948</td>\n",
       "      <td>770.325012</td>\n",
       "      <td>878.097046</td>\n",
       "      <td>963.605835</td>\n",
       "      <td>884.651733</td>\n",
       "      <td>1432.737915</td>\n",
       "      <td>-5.114703</td>\n",
       "      <td>1451.066162</td>\n",
       "      <td>...</td>\n",
       "      <td>989.052490</td>\n",
       "      <td>422.613708</td>\n",
       "      <td>1305.374878</td>\n",
       "      <td>1775.377441</td>\n",
       "      <td>1291.038208</td>\n",
       "      <td>1137.656494</td>\n",
       "      <td>1107.920166</td>\n",
       "      <td>1565.182983</td>\n",
       "      <td>1363.371460</td>\n",
       "      <td>922.655334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1079.413818</td>\n",
       "      <td>1743.748901</td>\n",
       "      <td>1455.763550</td>\n",
       "      <td>923.655762</td>\n",
       "      <td>1042.468140</td>\n",
       "      <td>1147.835693</td>\n",
       "      <td>1065.972168</td>\n",
       "      <td>1703.575562</td>\n",
       "      <td>-5.690143</td>\n",
       "      <td>1726.696411</td>\n",
       "      <td>...</td>\n",
       "      <td>1158.633545</td>\n",
       "      <td>498.212402</td>\n",
       "      <td>1563.906982</td>\n",
       "      <td>2109.405273</td>\n",
       "      <td>1533.462280</td>\n",
       "      <td>1366.833740</td>\n",
       "      <td>1306.630615</td>\n",
       "      <td>1870.930298</td>\n",
       "      <td>1624.557373</td>\n",
       "      <td>1071.776123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216496</th>\n",
       "      <td>181.188004</td>\n",
       "      <td>266.132355</td>\n",
       "      <td>283.131622</td>\n",
       "      <td>149.470062</td>\n",
       "      <td>186.132248</td>\n",
       "      <td>177.994217</td>\n",
       "      <td>153.823090</td>\n",
       "      <td>288.025970</td>\n",
       "      <td>0.700325</td>\n",
       "      <td>290.021240</td>\n",
       "      <td>...</td>\n",
       "      <td>237.769485</td>\n",
       "      <td>109.081177</td>\n",
       "      <td>248.840805</td>\n",
       "      <td>392.124847</td>\n",
       "      <td>284.463654</td>\n",
       "      <td>205.183701</td>\n",
       "      <td>264.684326</td>\n",
       "      <td>308.454498</td>\n",
       "      <td>266.087463</td>\n",
       "      <td>244.269180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216497</th>\n",
       "      <td>196.741745</td>\n",
       "      <td>306.167694</td>\n",
       "      <td>299.487579</td>\n",
       "      <td>159.818497</td>\n",
       "      <td>200.116318</td>\n",
       "      <td>203.891098</td>\n",
       "      <td>174.443298</td>\n",
       "      <td>319.354645</td>\n",
       "      <td>0.700325</td>\n",
       "      <td>319.941925</td>\n",
       "      <td>...</td>\n",
       "      <td>247.552734</td>\n",
       "      <td>103.029144</td>\n",
       "      <td>277.914093</td>\n",
       "      <td>429.283386</td>\n",
       "      <td>308.443909</td>\n",
       "      <td>232.684784</td>\n",
       "      <td>282.853333</td>\n",
       "      <td>342.967163</td>\n",
       "      <td>299.025299</td>\n",
       "      <td>248.857895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216498</th>\n",
       "      <td>104.982857</td>\n",
       "      <td>147.054611</td>\n",
       "      <td>129.407639</td>\n",
       "      <td>122.970978</td>\n",
       "      <td>108.850189</td>\n",
       "      <td>94.203339</td>\n",
       "      <td>120.233681</td>\n",
       "      <td>103.921799</td>\n",
       "      <td>90.972397</td>\n",
       "      <td>106.925537</td>\n",
       "      <td>...</td>\n",
       "      <td>101.224533</td>\n",
       "      <td>107.825027</td>\n",
       "      <td>120.581245</td>\n",
       "      <td>118.899078</td>\n",
       "      <td>125.359291</td>\n",
       "      <td>114.125740</td>\n",
       "      <td>107.073761</td>\n",
       "      <td>135.663528</td>\n",
       "      <td>83.425461</td>\n",
       "      <td>61.450954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216499</th>\n",
       "      <td>78.844818</td>\n",
       "      <td>137.156982</td>\n",
       "      <td>137.053757</td>\n",
       "      <td>61.033531</td>\n",
       "      <td>68.583778</td>\n",
       "      <td>61.313423</td>\n",
       "      <td>33.621338</td>\n",
       "      <td>127.499359</td>\n",
       "      <td>0.700325</td>\n",
       "      <td>115.414291</td>\n",
       "      <td>...</td>\n",
       "      <td>112.699318</td>\n",
       "      <td>61.399841</td>\n",
       "      <td>75.839783</td>\n",
       "      <td>166.752304</td>\n",
       "      <td>166.275696</td>\n",
       "      <td>83.495934</td>\n",
       "      <td>155.501785</td>\n",
       "      <td>132.800751</td>\n",
       "      <td>123.412567</td>\n",
       "      <td>133.006470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216500</th>\n",
       "      <td>753.186035</td>\n",
       "      <td>1162.301758</td>\n",
       "      <td>951.216003</td>\n",
       "      <td>621.726624</td>\n",
       "      <td>724.058838</td>\n",
       "      <td>775.192017</td>\n",
       "      <td>760.496399</td>\n",
       "      <td>1140.304443</td>\n",
       "      <td>-5.730848</td>\n",
       "      <td>1159.716187</td>\n",
       "      <td>...</td>\n",
       "      <td>728.259460</td>\n",
       "      <td>304.121002</td>\n",
       "      <td>1082.663818</td>\n",
       "      <td>1432.627319</td>\n",
       "      <td>1039.132568</td>\n",
       "      <td>951.109741</td>\n",
       "      <td>867.322876</td>\n",
       "      <td>1286.894409</td>\n",
       "      <td>1100.070312</td>\n",
       "      <td>651.810486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216501 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4   \\\n",
       "0       1375.664795  2235.253418  1893.332153  1186.515503  1367.845093   \n",
       "1       1253.308105  2016.756104  1674.117310  1072.242920  1204.446533   \n",
       "2       1167.372070  1883.146729  1570.064575   999.327759  1127.453125   \n",
       "3        895.391785  1462.211914  1234.246948   770.325012   878.097046   \n",
       "4       1079.413818  1743.748901  1455.763550   923.655762  1042.468140   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "216496   181.188004   266.132355   283.131622   149.470062   186.132248   \n",
       "216497   196.741745   306.167694   299.487579   159.818497   200.116318   \n",
       "216498   104.982857   147.054611   129.407639   122.970978   108.850189   \n",
       "216499    78.844818   137.156982   137.053757    61.033531    68.583778   \n",
       "216500   753.186035  1162.301758   951.216003   621.726624   724.058838   \n",
       "\n",
       "                 5            6            7          8            9   ...  \\\n",
       "0       1508.116211  1428.133301  2212.804688  -6.886380  2257.913574  ...   \n",
       "1       1327.840698  1244.838379  1968.371338  -6.531976  1998.309937  ...   \n",
       "2       1241.112061  1160.122314  1841.486572  -6.269972  1868.291870  ...   \n",
       "3        963.605835   884.651733  1432.737915  -5.114703  1451.066162  ...   \n",
       "4       1147.835693  1065.972168  1703.575562  -5.690143  1726.696411  ...   \n",
       "...             ...          ...          ...        ...          ...  ...   \n",
       "216496   177.994217   153.823090   288.025970   0.700325   290.021240  ...   \n",
       "216497   203.891098   174.443298   319.354645   0.700325   319.941925  ...   \n",
       "216498    94.203339   120.233681   103.921799  90.972397   106.925537  ...   \n",
       "216499    61.313423    33.621338   127.499359   0.700325   115.414291  ...   \n",
       "216500   775.192017   760.496399  1140.304443  -5.730848  1159.716187  ...   \n",
       "\n",
       "                 15          16           17           18           19  \\\n",
       "0       1519.571777  639.886902  2050.856201  2779.219727  1991.155884   \n",
       "1       1327.296265  568.654663  1816.974854  2439.182373  1770.564819   \n",
       "2       1247.981689  534.958313  1695.660889  2282.581787  1656.706787   \n",
       "3        989.052490  422.613708  1305.374878  1775.377441  1291.038208   \n",
       "4       1158.633545  498.212402  1563.906982  2109.405273  1533.462280   \n",
       "...             ...         ...          ...          ...          ...   \n",
       "216496   237.769485  109.081177   248.840805   392.124847   284.463654   \n",
       "216497   247.552734  103.029144   277.914093   429.283386   308.443909   \n",
       "216498   101.224533  107.825027   120.581245   118.899078   125.359291   \n",
       "216499   112.699318   61.399841    75.839783   166.752304   166.275696   \n",
       "216500   728.259460  304.121002  1082.663818  1432.627319  1039.132568   \n",
       "\n",
       "                 20           21           22           23           24  \n",
       "0       1785.150635  1700.054077  2446.060303  2107.048828  1400.085693  \n",
       "1       1589.518311  1501.732300  2169.942627  1879.807861  1221.028809  \n",
       "2       1481.844360  1409.592285  2026.462769  1756.947876  1151.642334  \n",
       "3       1137.656494  1107.920166  1565.182983  1363.371460   922.655334  \n",
       "4       1366.833740  1306.630615  1870.930298  1624.557373  1071.776123  \n",
       "...             ...          ...          ...          ...          ...  \n",
       "216496   205.183701   264.684326   308.454498   266.087463   244.269180  \n",
       "216497   232.684784   282.853333   342.967163   299.025299   248.857895  \n",
       "216498   114.125740   107.073761   135.663528    83.425461    61.450954  \n",
       "216499    83.495934   155.501785   132.800751   123.412567   133.006470  \n",
       "216500   951.109741   867.322876  1286.894409  1100.070312   651.810486  \n",
       "\n",
       "[216501 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell contains code to predict thicknesses across all 25 of the top performing ensemble model\n",
    "\"\"\"\n",
    "dfs = pd.DataFrame()\n",
    "for rs in tqdm(RS):\n",
    "    df = deviations.iloc[:1]\n",
    "    s = pd.Series(dnn_model['10-5_Glam_dnn_MULTI_0.01_0.2_300_'+str(rs)].predict(RGI, verbose=0).flatten(), name = rs)\n",
    "    dfs[rs] = s\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce78212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692.795\n",
      "1502.2703125\n",
      "1405.5234375\n",
      "1094.42375\n",
      "1300.616328125\n",
      "1027.80875\n",
      "1688.20203125\n",
      "2510.533125\n",
      "1537.31671875\n",
      "1100.03875\n",
      "1171.49484375\n",
      "1138.2634375\n",
      "688.6009375\n",
      "1389.69296875\n",
      "2188.81859375\n",
      "1154.2165625\n",
      "1230.74328125\n",
      "1300.574453125\n",
      "1363.6565625\n",
      "963.27359375\n",
      "794.5059375\n",
      "741.833515625\n",
      "1272.07328125\n",
      "1767.56375\n",
      "907.65984375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we compute the average thickness for each RGI glacier\n",
    "\"\"\"\n",
    "\n",
    "for i in dfs:\n",
    "#     print(dfs.loc[i])\n",
    "#     print(len(dfs.loc[i]))\n",
    "    print(np.sum(dfs.loc[i])/len(dfs.loc[i]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9820c44a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ccccombo_breaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1017586/3289444459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mccccombo_breaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# past this point is under construction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# here be monsters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ccccombo_breaker' is not defined"
     ]
    }
   ],
   "source": [
    "ccccombo_breaker()\n",
    "# past this point is under construction. \n",
    "# here be monsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5ba6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns = {'architecture':'layer architecture'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c001d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for rs in RS:\n",
    "    print(rs)\n",
    "    df = deviations.iloc[:1]\n",
    "    s = pd.Series(dnn_model['10-5_Glam_dnn_MULTI_0.01_0.2_300_'+str(rs)].predict(RGI).flatten(), name = rs)\n",
    "    dfs[rs] = s\n",
    "#     break\n",
    "#     dfs = dfs.assign(str(s))\n",
    "print(s)\n",
    "dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in predictions[(predictions['learning rate'] == df['learning rate']) and (predictions['layer architecture'] == df['layer architecture'])]:\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8a552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in predictions['model']:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['layer architecture']\n",
    "# df['layer architecture']\n",
    "# predictions['learning rate']\n",
    "# df['learning rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33757705",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions['layer architecture'].values() == [str(0.1)]:\n",
    "    print(ding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ad79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['layer architecture']\n",
    "# predictions[(predictions['learning rate'] == df['learning rate']) and (predictions['layer architecture'] == df['layer architecture'])]:\n",
    "for architecture in predictions['layer architecture']:\n",
    "#     if predictions['layer architecture'] == df['layer architecture'] and predictions['learning rate'] == df['learning rate']:\n",
    "    print('')\n",
    "    print(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9738bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains code to produce histograms of all the architectures different histories\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = deviations.iloc[:1]\n",
    "\n",
    "\n",
    "for rs in list(predictions['architecture'].unique()):\n",
    "    df = predictions[predictions['architecture'] == architecture]\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('Layer architecture: ' + architecture)\n",
    "    ax.set_ylabel('prediction count')\n",
    "    ax.set_xlabel('thickness (m)')\n",
    "    fig.patch.set_facecolor('w')\n",
    "    plt.hist(df['avg test thickness'])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31457a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to produce histograms of all the architectures different histories\n",
    "# \"\"\"\n",
    "\n",
    "# for architecture in list(predictions['architecture'].unique()):\n",
    "#     df = predictions[predictions['architecture'] == architecture]\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.set_title('Layer architecture: ' + architecture)\n",
    "#     ax.set_ylabel('prediction count')\n",
    "#     ax.set_xlabel('thickness (m)')\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.hist(df['avg test thickness'])\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modl in dnn_modl:    \n",
    "#     y = dnn_modl[modl].predict(test_features)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.plot(test_labels,y,'.')\n",
    "#     plt.plot((0,300),(0,300),'-')\n",
    "#     plt.xlabel('True Thickness (m)')\n",
    "#     plt.ylabel('Model Thickness (m)')\n",
    "#     plt.xlim((0,300))\n",
    "#     plt.ylim((0,300))\n",
    "#     # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all histories\n",
    "print('Loading histories....')\n",
    "rootdir = 'sr2/'\n",
    "dnn_history = {}\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder:\n",
    "            if 'dnn' in folder:\n",
    "\n",
    "                dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc93aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_historyf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95568cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code plots predictions against actual thickness. Not currently modified and will load\n",
    "EVERY SINGLE MODEL. DO NOT USE JUST YET\n",
    "\"\"\"\n",
    "# for modl in dnn_modl:    \n",
    "#     y = dnn_modl[modl].predict(test_features)\n",
    "#     fig,ax=plt.subplots(1,1,figsize=(15,10))\n",
    "#     fig.patch.set_facecolor('w')\n",
    "#     plt.plot(test_labels,y,'.')\n",
    "#     plt.plot((0,300),(0,300),'-')\n",
    "#     plt.xlabel('True Thickness (m)')\n",
    "#     plt.ylabel('Model Thickness (m)')\n",
    "#     plt.xlim((0,300))\n",
    "#     plt.ylim((0,300))\n",
    "#     # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a density plot of the most recent predictions made. Can easily be modified in a loop\n",
    "to show multiple random states and whatnot\n",
    "\"\"\"\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "sns.kdeplot(x = test_labels, y = y.flatten(),fill = True)\n",
    "plt.plot((0,300),(0,300),'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209dfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots the different variable loss curves to show individual variable models\n",
    "Not currently working because we have only loaded dnn_MULTI models\n",
    "\"\"\"\n",
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "fig.patch.set_facecolor('w')\n",
    "# ax.set_ylim([5,30])\n",
    "\n",
    "# gl.plot_loss(dnn_history['T_MULTI'])\n",
    "for i, variable_name in enumerate(list(train_features)):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    gl.plot_loss(dnn_history['glacier_'+ variable_name+ '_0.1_0.2_300_6'])\n",
    "#     ax.set_ylim([35,140])\n",
    "    ax.set_title(variable_name)\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e735a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deviations.sort_values('test mae avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a741cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a7383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # load all histories\n",
    "# qqq = deviations.sort_values('test mae avg')\n",
    "\n",
    "\n",
    "for rs in RS:\n",
    "#     dnn_histor+y[qqq['layer architecture']+\n",
    "#                 'Glam_dnn_history_MULTI_'+\n",
    "#                 qqq['learning rate']+\n",
    "#                 '_0.2_300_'+\n",
    "#                 rs]\n",
    "    gl.plot_loss(dnn_history[qqq['layer architecture'].iloc[0]+\n",
    "                '_Glam_dnn_history_MULTI_'+\n",
    "                qqq['learning rate'].iloc[0]+\n",
    "                '_0.2_300_'+\n",
    "                str(rs)])\n",
    "# print('Loading histories....')\n",
    "# rootdir = 'sr2/'\n",
    "# dnn_history = {}\n",
    "# for arch in tqdm(os.listdir(rootdir)):\n",
    "#     for folder in os.listdir(rootdir+arch):\n",
    "#         if 'MULTI' in folder and 'dnn' in folder:\n",
    "\n",
    "#             dnn_history[arch[3:] + '_'+ folder] = pd.read_csv(rootdir+arch+'/'+folder)\n",
    "# dnn_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa78f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fee1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[qqq['layer architecture'].iloc[0]+\n",
    "                    '_Glam_dnn_history_MULTI_'+\n",
    "                    qqq['learning rate'].iloc[0]+\n",
    "                    '_0.2_300_'+\n",
    "                    str(rs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b56b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell plots each random state loss curve for the 25 random states for each run\n",
    "Also loads EVERY SINGLE MODEL currently and blows up the memory. Working on it.\n",
    "\"\"\"\n",
    "# for rs in RS:\n",
    "for hist in dnn_history:    \n",
    "    fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "    fig.patch.set_facecolor('w')\n",
    "    ax.set_title(hist)\n",
    "    gl.plot_loss(dnn_history[hist])\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_single_variable(x,y,model_type,model_name,feature_name):\n",
    "#     y = model_type[model_name].predict(test_labels)\n",
    "#     plt.scatter(train_features[feature_name], train_labels, label='Data')\n",
    "#     plt.plot(x, y,'.', color='k', label='Predictions')\n",
    "#     plt.xlabel(feature_name)\n",
    "#     plt.ylabel('THICKNESS')\n",
    "#     plt.legend()\n",
    "#     plt.plot()\n",
    "\n",
    "# x = test_labels\n",
    "# for i, variable_name in enumerate(list(train_features)):\n",
    "#     ax = plt.subplot(2,2,i+1)\n",
    "#     model_name = (dataset.name \n",
    "#     + '_' \n",
    "#     + variable_name \n",
    "#     + '_' \n",
    "#     + str(lr) \n",
    "#     + '_' \n",
    "#     + str(vs) \n",
    "#     + '_' \n",
    "#     + str(ep))\n",
    "#     plot_single_variable(x,y,dnn_model, model_name,variable_name)\n",
    "# #     ax.set_ylim([35,140])\n",
    "# #     ax.set_title(variable_name)\n",
    "# #     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea1269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This cell contains code to load models and histories.\n",
    "# \"\"\"\n",
    "\n",
    "# # fix hyperparameters \n",
    "# lr = 0.1\n",
    "# vs = 0.2\n",
    "# # load models\n",
    "# print('Loading models....')\n",
    "\n",
    "# linear_model = {}\n",
    "# dnn_model = {}\n",
    "# # data is already split, however if other databases are used, then this line is needed\n",
    "# # (train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_linear_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_model[\n",
    "#             'glacier' \n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "        \n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#         pth_mod \n",
    "#         + 'glacier' \n",
    "#         + '_dnn_' \n",
    "#         + variable_name \n",
    "#         + '_' \n",
    "#         + str(lr)\n",
    "#         + '_'\n",
    "#         + str(vs)\n",
    "#         + '_'\n",
    "#         + str(ep)\n",
    "#         + '_'\n",
    "#         + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_model[\n",
    "#             'glacier'\n",
    "#             + '_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = tf.keras.models.load_model(file_name)\n",
    "# print('Models loaded')\n",
    "\n",
    "# # load all histories\n",
    "# print('Loading histories....')\n",
    "# linear_history = {}\n",
    "# dnn_history = {}\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier' \n",
    "#             +'_linear_history_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         linear_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ]= pd.read_csv(file_name)\n",
    "\n",
    "# for variable_name in tqdm(list(train_features) + ['MULTI']):\n",
    "#     for rs in RS:\n",
    "#         file_name = (\n",
    "#             pth_res \n",
    "#             + 'glacier_dnn_history_' \n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         )\n",
    "\n",
    "#         dnn_history[\n",
    "#             'glacier' \n",
    "#             +'_'\n",
    "#             + variable_name \n",
    "#             + '_' \n",
    "#             + str(lr)\n",
    "#             + '_'\n",
    "#             + str(vs)\n",
    "#             + '_'\n",
    "#             + str(ep)\n",
    "#             + '_'\n",
    "#             + str(rs)\n",
    "#         ] = pd.read_csv(file_name)\n",
    "# print('Histories loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03759a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This cell loads the loss histories from the original model with one dense layer of 64 nodes.\n",
    "From these histories I extracted the lowest and final loss value and plotted them.\n",
    "First is plotted the loss from using different learning rates with a fixed validation split. \n",
    "Then follows a plot of different validation splits using a fixed learning rate.\n",
    "\"\"\"\n",
    "\n",
    "# set up dictionaries\n",
    "loss = {}\n",
    "dnn_lr_history = {}\n",
    "loss['glacier_min_learn'] = pd.DataFrame()\n",
    "loss['glacier_fin_learn'] = pd.DataFrame()\n",
    "\n",
    "# loop to define and then load histories\n",
    "for lr in LR:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    # find minimum and insert other model hyperparameters into table\n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_learn'] = loss['glacier_min_learn'].append(m_loss,ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # find final and insert other model hyperparameters into table\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_learn'] = loss['glacier_fin_learn'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_learn'] = loss['glacier_fin_learn'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_learn'] = loss['glacier_min_learn'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_learn'].plot(x='learning rate', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_learn'].plot(x='learning rate', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Learning rate at fixed validation split = 0.2')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# repeat above loop with fixed lr and varied vs\n",
    "lr = 0.1\n",
    "loss = {}\n",
    "dnn_vs_history = {}\n",
    "loss['glacier_min_valsplit'] = pd.DataFrame()\n",
    "loss['glacier_fin_valsplit'] = pd.DataFrame()\n",
    "for vs in VS:\n",
    "    file_name = (\n",
    "    'sr/sr_64/'\n",
    "    + 'glacier_dnn_history_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    file = (\n",
    "    'glacier_MULTI_'\n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dnn_lr_history[\n",
    "    'glacier_MULTI_' \n",
    "    + str(lr)\n",
    "    + '_'\n",
    "    + str(vs)\n",
    "    + '_'\n",
    "    + str(ep)\n",
    "    ] = pd.read_csv(file_name)\n",
    "    \n",
    "    \n",
    "    m_loss = dnn_lr_history[file].min()\n",
    "    m_loss['learning rate'] = str(lr)\n",
    "    m_loss['validation split'] = str(vs)\n",
    "    m_loss['epochs'] = str(ep)\n",
    "    loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].append(m_loss,ignore_index=True)\n",
    "\n",
    "    f = dnn_lr_history[file].last_valid_index()\n",
    "    f_loss = dnn_lr_history[file].iloc[[f]]\n",
    "    f_loss['learning rate'] = str(lr)\n",
    "    f_loss['validation split'] = str(vs)\n",
    "    f_loss['epochs'] = str(ep)\n",
    "\n",
    "    loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].append(f_loss,ignore_index=True)\n",
    "\n",
    "loss['glacier_fin_valsplit'] = loss['glacier_fin_valsplit'].rename(columns = {\n",
    "    'loss':'loss_final',\n",
    "    'val_loss':'val_loss_final'\n",
    "})\n",
    "\n",
    "loss['glacier_min_valsplit'] = loss['glacier_min_valsplit'].rename(columns = {\n",
    "    'loss':'loss_minimum',\n",
    "    'val_loss':'val_loss_minimum'\n",
    "})\n",
    "    \n",
    "print('Results compiled')\n",
    "sns.set(rc={\"figure.figsize\":(15,10)})\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([5,30])\n",
    "# loss['glacier_min'].plot(x='validation split', y = ['loss_minimum', 'val_loss_minimum'], kind='bar',  ax=ax)\n",
    "# loss['glacier_fin'].plot(x='validation split', y = ['loss_final', 'val_loss_final'], kind='bar',  ax=ax)\n",
    "\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'loss_final',color = 'blue',  ax=ax)\n",
    "loss['glacier_fin_valsplit'].plot(x='validation split', y = 'val_loss_final',color = 'green', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'loss_minimum', color = 'red', ax=ax)\n",
    "loss['glacier_min_valsplit'].plot(x='validation split', y = 'val_loss_minimum',color = 'orange', ax=ax)\n",
    "ax.set_xlabel('Validation splits with learning rate = 0.1')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('GlaThiDa Glacier scale dataset multivariable regression hyperparameterization')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell computes the true average thickness of the glaciers in use\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pth = '/data/fast1/glacierml/T_models/'\n",
    "T_lab = pd.read_csv(pth + 'T.csv', low_memory = False)\n",
    "T_lab = T_lab[[\n",
    "    'GlaThiDa_ID',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'AREA',\n",
    "    'MEAN_SLOPE',\n",
    "    'MEAN_THICKNESS'\n",
    "]]\n",
    "T_lab = T_lab.dropna()\n",
    "\n",
    "tru_thickness = np.sum(T_lab['MEAN_THICKNESS']) / len(T_lab['MEAN_THICKNESS'])\n",
    "tru_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c1983d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Importing T database\n",
      "Import complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [03:18<00:00, 11.70s/it]\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:145: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:155: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:180: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:184: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py:193: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer architecture</th>\n",
       "      <th>model parameters</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>validation split</th>\n",
       "      <th>test mae avg</th>\n",
       "      <th>train mae avg</th>\n",
       "      <th>test mae std dev</th>\n",
       "      <th>train mae std dev</th>\n",
       "      <th>test predicted thickness std dev</th>\n",
       "      <th>train predicted thickness std dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16-8</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32.222112</td>\n",
       "      <td>11.185435</td>\n",
       "      <td>15.672680</td>\n",
       "      <td>0.781699</td>\n",
       "      <td>11.355721</td>\n",
       "      <td>2.016813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>16-10</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>36.024810</td>\n",
       "      <td>11.227334</td>\n",
       "      <td>24.742688</td>\n",
       "      <td>1.040625</td>\n",
       "      <td>15.684682</td>\n",
       "      <td>2.046897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20-10</td>\n",
       "      <td>330.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>36.684168</td>\n",
       "      <td>11.496173</td>\n",
       "      <td>14.202592</td>\n",
       "      <td>1.278700</td>\n",
       "      <td>10.680337</td>\n",
       "      <td>3.192789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>37.283502</td>\n",
       "      <td>10.851193</td>\n",
       "      <td>11.034718</td>\n",
       "      <td>1.349086</td>\n",
       "      <td>11.279318</td>\n",
       "      <td>2.103120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12-6-3</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>43.009678</td>\n",
       "      <td>11.580005</td>\n",
       "      <td>26.626463</td>\n",
       "      <td>1.937355</td>\n",
       "      <td>18.220395</td>\n",
       "      <td>2.425202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16-4</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>43.577870</td>\n",
       "      <td>11.656853</td>\n",
       "      <td>31.413210</td>\n",
       "      <td>1.234964</td>\n",
       "      <td>19.864055</td>\n",
       "      <td>2.447538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12-12</td>\n",
       "      <td>238.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>45.322772</td>\n",
       "      <td>11.870251</td>\n",
       "      <td>22.462913</td>\n",
       "      <td>1.256479</td>\n",
       "      <td>14.430677</td>\n",
       "      <td>2.799342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16-6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>45.996561</td>\n",
       "      <td>11.689291</td>\n",
       "      <td>30.385058</td>\n",
       "      <td>1.121671</td>\n",
       "      <td>20.223611</td>\n",
       "      <td>2.061004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>12-8</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50.875385</td>\n",
       "      <td>11.734783</td>\n",
       "      <td>31.989574</td>\n",
       "      <td>1.095910</td>\n",
       "      <td>19.970083</td>\n",
       "      <td>2.007298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-6</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>52.556535</td>\n",
       "      <td>12.016238</td>\n",
       "      <td>29.897222</td>\n",
       "      <td>2.022535</td>\n",
       "      <td>18.720065</td>\n",
       "      <td>2.426674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8-4-2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>52.665127</td>\n",
       "      <td>15.672720</td>\n",
       "      <td>35.395604</td>\n",
       "      <td>3.879644</td>\n",
       "      <td>23.759894</td>\n",
       "      <td>3.316205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6-4-2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>53.366313</td>\n",
       "      <td>15.648812</td>\n",
       "      <td>36.169190</td>\n",
       "      <td>3.759207</td>\n",
       "      <td>24.534691</td>\n",
       "      <td>3.020535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10-10</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>57.100336</td>\n",
       "      <td>11.989840</td>\n",
       "      <td>32.922630</td>\n",
       "      <td>1.151477</td>\n",
       "      <td>21.598586</td>\n",
       "      <td>2.464462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4-2-2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>58.121310</td>\n",
       "      <td>17.201753</td>\n",
       "      <td>41.036515</td>\n",
       "      <td>3.176356</td>\n",
       "      <td>28.376875</td>\n",
       "      <td>2.889817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4-2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.777793</td>\n",
       "      <td>14.844569</td>\n",
       "      <td>35.808941</td>\n",
       "      <td>2.452786</td>\n",
       "      <td>24.718858</td>\n",
       "      <td>2.531801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8-4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>75.645246</td>\n",
       "      <td>12.850741</td>\n",
       "      <td>42.434359</td>\n",
       "      <td>1.526745</td>\n",
       "      <td>26.488120</td>\n",
       "      <td>2.575208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>85.592064</td>\n",
       "      <td>13.864899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>87.143379</td>\n",
       "      <td>16.183233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6-3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>99.422656</td>\n",
       "      <td>13.991101</td>\n",
       "      <td>41.306614</td>\n",
       "      <td>2.026843</td>\n",
       "      <td>25.900069</td>\n",
       "      <td>2.404127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer architecture  model parameters learning rate  validation split  \\\n",
       "24               16-8             234.0           0.1               0.2   \n",
       "30              16-10             270.0           0.1               0.2   \n",
       "27              20-10             330.0           0.1               0.2   \n",
       "6                  64            4554.0           0.1               0.2   \n",
       "0              12-6-3             172.0           0.1               0.2   \n",
       "21               16-4             162.0           0.1               0.2   \n",
       "42              12-12             238.0           0.1               0.2   \n",
       "36               16-6             198.0           0.1               0.2   \n",
       "33               12-8             182.0           0.1               0.2   \n",
       "3                12-6             154.0           0.1               0.2   \n",
       "15              8-4-2              98.0           0.1               0.2   \n",
       "9               6-4-2              80.0           0.1               0.2   \n",
       "39              10-10             180.0           0.1               0.2   \n",
       "12              4-2-2              48.0           0.1               0.2   \n",
       "45                4-2              42.0           0.1               0.2   \n",
       "18                8-4              90.0           0.1               0.2   \n",
       "8                  64            4554.0          0.01               0.2   \n",
       "7                  64            4554.0         0.001               0.2   \n",
       "48                6-3              64.0           0.1               0.2   \n",
       "\n",
       "    test mae avg  train mae avg  test mae std dev  train mae std dev  \\\n",
       "24     32.222112      11.185435         15.672680           0.781699   \n",
       "30     36.024810      11.227334         24.742688           1.040625   \n",
       "27     36.684168      11.496173         14.202592           1.278700   \n",
       "6      37.283502      10.851193         11.034718           1.349086   \n",
       "0      43.009678      11.580005         26.626463           1.937355   \n",
       "21     43.577870      11.656853         31.413210           1.234964   \n",
       "42     45.322772      11.870251         22.462913           1.256479   \n",
       "36     45.996561      11.689291         30.385058           1.121671   \n",
       "33     50.875385      11.734783         31.989574           1.095910   \n",
       "3      52.556535      12.016238         29.897222           2.022535   \n",
       "15     52.665127      15.672720         35.395604           3.879644   \n",
       "9      53.366313      15.648812         36.169190           3.759207   \n",
       "39     57.100336      11.989840         32.922630           1.151477   \n",
       "12     58.121310      17.201753         41.036515           3.176356   \n",
       "45     62.777793      14.844569         35.808941           2.452786   \n",
       "18     75.645246      12.850741         42.434359           1.526745   \n",
       "8      85.592064      13.864899          0.000000           0.000000   \n",
       "7      87.143379      16.183233          0.000000           0.000000   \n",
       "48     99.422656      13.991101         41.306614           2.026843   \n",
       "\n",
       "    test predicted thickness std dev  train predicted thickness std dev  \n",
       "24                         11.355721                           2.016813  \n",
       "30                         15.684682                           2.046897  \n",
       "27                         10.680337                           3.192789  \n",
       "6                          11.279318                           2.103120  \n",
       "0                          18.220395                           2.425202  \n",
       "21                         19.864055                           2.447538  \n",
       "42                         14.430677                           2.799342  \n",
       "36                         20.223611                           2.061004  \n",
       "33                         19.970083                           2.007298  \n",
       "3                          18.720065                           2.426674  \n",
       "15                         23.759894                           3.316205  \n",
       "9                          24.534691                           3.020535  \n",
       "39                         21.598586                           2.464462  \n",
       "12                         28.376875                           2.889817  \n",
       "45                         24.718858                           2.531801  \n",
       "18                         26.488120                           2.575208  \n",
       "8                           0.000000                           0.000000  \n",
       "7                           0.000000                           0.000000  \n",
       "48                         25.900069                           2.404127  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set definitions\n",
    "glacier = gl.data_loader(pth = '/data/fast1/glacierml/T_models/')\n",
    "# ,TT,TTT,TTTx,TTT_full\n",
    "# T_t = T.head()\n",
    "\n",
    "# rename thickness column in database\n",
    "gl.thickness_renamer(glacier)\n",
    "\n",
    "# split data for training and validation\n",
    "(train_features, test_features, train_labels, test_labels) = gl.data_splitter(glacier)\n",
    "\n",
    "# define model hyperparameters\n",
    "LR = np.logspace(-3,2,6)\n",
    "vs = 0.2\n",
    "VS = 0.1,0.15,0.2,0.25,0.3,0.35,0.4\n",
    "RS = range(0,25,1)\n",
    "ep = 300\n",
    "\n",
    "# name databases\n",
    "glacier.name = 'glacier'\n",
    "# T_t.name = 'T_t'\n",
    "# TT.name = 'band'\n",
    "# TTT.name = 'point'\n",
    "# TTTx.name = 'TTTx'\n",
    "# TTT_full.name = 'TTT_full'\n",
    "\n",
    "# old definitions, legacy code.\n",
    "\n",
    "# arch = '16-8'\n",
    "# pth_mod = 'sm/sm_' + arch + '/'\n",
    "# pth_res = 'sr/sr_' + arch + '/'\n",
    "\n",
    "\"\"\"\n",
    "Here we evaluate models and make predictions, then display the zults\n",
    "\"\"\"\n",
    "rootdir = 'sm/'\n",
    "# print(rootdir)\n",
    "dnn_model = {}\n",
    "predictions = pd.DataFrame()\n",
    "for arch in tqdm(os.listdir(rootdir)):\n",
    "    for folder in os.listdir(rootdir+arch):\n",
    "        if 'MULTI' in folder and 'dnn' in folder:\n",
    "            \n",
    "            if '0.1' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.1'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'\n",
    "                \n",
    "            if '0.01' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features, verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]\n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.01'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'          \n",
    "            \n",
    "            if '0.001' in folder:\n",
    "                dnn_model[arch[3:]+'_'+folder] = tf.keras.models.load_model(rootdir \n",
    "                    + arch \n",
    "                    + '/' \n",
    "                    + folder)\n",
    "\n",
    "                mae_test = dnn_model[arch[3:]+'_'+folder].evaluate(test_features,\n",
    "                                                             test_labels,verbose=0)\n",
    "\n",
    "                mae_train = dnn_model[arch[3:]+'_'+folder].evaluate(train_features,\n",
    "                                             train_labels,verbose=0)\n",
    "\n",
    "                pred_train = dnn_model[arch[3:]+'_'+folder].predict(train_features,verbose=0)\n",
    "\n",
    "                pred_test = dnn_model[arch[3:]+'_'+folder].predict(test_features,verbose=0)\n",
    "                avg_thickness = pd.Series((np.sum(pred_train) / len(pred_train)), name = 'avg train thickness')\n",
    "\n",
    "                avg_test_thickness = pd.Series((np.sum(pred_test) / len(pred_test)),  name = 'avg test thickness')\n",
    "                temp_df = pd.merge(avg_thickness, avg_test_thickness, right_index=True, left_index=True)\n",
    "                predictions = predictions.append(temp_df, ignore_index=True)\n",
    "                predictions.loc[predictions.index[-1], 'model']= folder\n",
    "                predictions.loc[predictions.index[-1], 'test mae']= mae_test\n",
    "                predictions.loc[predictions.index[-1], 'train mae']= mae_train\n",
    "                predictions.loc[predictions.index[-1], 'architecture']= arch[3:]            \n",
    "                predictions.loc[predictions.index[-1], 'learning rate']= '0.001'\n",
    "                predictions.loc[predictions.index[-1], 'validation split']= '0.2'                \n",
    "                \n",
    "predictions.rename(columns = {0:'avg train thickness'},inplace = True)\n",
    "\n",
    "# these models are ridiculous, so we drop them.\n",
    "# idx = predictions.index[predictions['architecture']=='64']\n",
    "# predictions = predictions.drop(predictions.loc[idx].index)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we compute for each layer architecture avg mae, mae std dev, and\n",
    "prediction std dev.\n",
    "\"\"\"\n",
    "deviations = pd.DataFrame()\n",
    "for architecture in list(predictions['architecture'].unique()):\n",
    "    for learning_rate in list(predictions['learning rate'].unique()):\n",
    "        # define temp dataframe for calculations that contains only one layer architecture\n",
    "#         df = (predictions[predictions['architecture'] == architecture]) and (predictions[predictions['learning rate'] == str(learning_rate)])\n",
    "        df = predictions[(predictions['architecture'] == architecture) & (predictions['learning rate' ]== learning_rate)]\n",
    "#         break\n",
    "#         print(df)\n",
    "        # step 1: calculate mean of numbers\n",
    "        test_mae_mean = np.sum(df['test mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for test_mae in df['test mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((test_mae - test_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        test_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "       # repeat for train mae \n",
    "\n",
    "        # step 1: calculate mean of numbers\n",
    "        train_mae_mean = np.sum(df['train mae']) / len(df) \n",
    "\n",
    "        diff_sq = pd.Series()\n",
    "\n",
    "        for train_mae in df['train mae']:\n",
    "            # step 2: subtract the mean from each, then square the result\n",
    "            step_2 = pd.Series((train_mae - train_mae_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "\n",
    "        # step 3: work out the mean of the squared differences    \n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "\n",
    "        # step 4: take the square root\n",
    "        train_mae_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # repeat process for train thicknesses\n",
    "        thickness_train_mean = np.sum(df['avg train thickness']) / len(df)   \n",
    "        for thickness in df['avg train thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_train_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        train_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "\n",
    "        # repeat process for test thicknesses\n",
    "        thickness_test_mean = np.sum(df['avg test thickness']) / len(df)   \n",
    "        for thickness in df['avg test thickness']:\n",
    "            step_2 = pd.Series((thickness - thickness_test_mean)**2)\n",
    "            diff_sq = diff_sq.append(step_2, ignore_index=True)\n",
    "        mean_diff_sq = (np.sum(diff_sq) / len(diff_sq))\n",
    "        test_thickness_std_dev = np.sqrt(mean_diff_sq)\n",
    "\n",
    "        # turn the last number computed into a series so it may be appended to build the table.\n",
    "        # it will be dropped later, no worries.\n",
    "        test_thick_std_dev = pd.Series(test_thickness_std_dev)\n",
    "\n",
    "        deviations = deviations.append(test_thick_std_dev, ignore_index=True)\n",
    "        deviations.loc[deviations.index[-1], 'layer architecture']= architecture\n",
    "        deviations.loc[deviations.index[-1], 'model parameters'] = dnn_model[architecture + '_glacier_dnn_MULTI_0.1_0.2_300_0'].count_params()\n",
    "        deviations.loc[deviations.index[-1], 'learning rate']= learning_rate\n",
    "        deviations.loc[deviations.index[-1], 'validation split']= 0.2\n",
    "        deviations.loc[deviations.index[-1], 'test mae avg'] = test_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'train mae avg']= train_mae_mean\n",
    "        deviations.loc[deviations.index[-1], 'test mae std dev']= test_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train mae std dev']= train_mae_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'test predicted thickness std dev']= test_thickness_std_dev\n",
    "        deviations.loc[deviations.index[-1], 'train predicted thickness std dev']= train_thickness_std_dev\n",
    "# bootstrapped ensembles for predicted column    \n",
    "#drop that appended line from earlier. Probably a better way to go about it\n",
    "deviations.drop(columns = {0},inplace = True)    \n",
    "deviations = deviations.dropna()\n",
    "deviations.sort_values('test mae avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc509540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg train thickness</th>\n",
       "      <th>avg test thickness</th>\n",
       "      <th>model</th>\n",
       "      <th>test mae</th>\n",
       "      <th>train mae</th>\n",
       "      <th>architecture</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>validation split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.664687</td>\n",
       "      <td>50.010085</td>\n",
       "      <td>Glam_dnn_MULTI_0.1_0.2_300_0</td>\n",
       "      <td>20.697359</td>\n",
       "      <td>9.681968</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.298939</td>\n",
       "      <td>47.213233</td>\n",
       "      <td>Glam_dnn_MULTI_0.1_0.2_300_1</td>\n",
       "      <td>20.583303</td>\n",
       "      <td>12.181650</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.713923</td>\n",
       "      <td>52.411519</td>\n",
       "      <td>Glam_dnn_MULTI_0.1_0.2_300_2</td>\n",
       "      <td>22.110643</td>\n",
       "      <td>10.836245</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.615060</td>\n",
       "      <td>44.919894</td>\n",
       "      <td>Glam_dnn_MULTI_0.1_0.2_300_3</td>\n",
       "      <td>18.854342</td>\n",
       "      <td>9.344668</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.670214</td>\n",
       "      <td>49.853925</td>\n",
       "      <td>Glam_dnn_MULTI_0.1_0.2_300_4</td>\n",
       "      <td>20.752937</td>\n",
       "      <td>9.989418</td>\n",
       "      <td>16-8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>51.541371</td>\n",
       "      <td>104.036033</td>\n",
       "      <td>glacier_dnn_MULTI_0.1_0.2_300_20</td>\n",
       "      <td>60.638832</td>\n",
       "      <td>11.088364</td>\n",
       "      <td>6-3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>47.349043</td>\n",
       "      <td>165.566329</td>\n",
       "      <td>glacier_dnn_MULTI_0.1_0.2_300_21</td>\n",
       "      <td>125.538750</td>\n",
       "      <td>13.766709</td>\n",
       "      <td>6-3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>48.476218</td>\n",
       "      <td>100.349654</td>\n",
       "      <td>glacier_dnn_MULTI_0.1_0.2_300_22</td>\n",
       "      <td>60.688206</td>\n",
       "      <td>12.714522</td>\n",
       "      <td>6-3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>42.860918</td>\n",
       "      <td>146.646717</td>\n",
       "      <td>glacier_dnn_MULTI_0.1_0.2_300_23</td>\n",
       "      <td>110.005013</td>\n",
       "      <td>14.443310</td>\n",
       "      <td>6-3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>48.438948</td>\n",
       "      <td>184.567216</td>\n",
       "      <td>glacier_dnn_MULTI_0.1_0.2_300_24</td>\n",
       "      <td>144.228729</td>\n",
       "      <td>13.744843</td>\n",
       "      <td>6-3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     avg train thickness  avg test thickness  \\\n",
       "0              47.664687           50.010085   \n",
       "1              43.298939           47.213233   \n",
       "2              49.713923           52.411519   \n",
       "3              46.615060           44.919894   \n",
       "4              48.670214           49.853925   \n",
       "..                   ...                 ...   \n",
       "754            51.541371          104.036033   \n",
       "755            47.349043          165.566329   \n",
       "756            48.476218          100.349654   \n",
       "757            42.860918          146.646717   \n",
       "758            48.438948          184.567216   \n",
       "\n",
       "                                model    test mae  train mae architecture  \\\n",
       "0        Glam_dnn_MULTI_0.1_0.2_300_0   20.697359   9.681968         16-8   \n",
       "1        Glam_dnn_MULTI_0.1_0.2_300_1   20.583303  12.181650         16-8   \n",
       "2        Glam_dnn_MULTI_0.1_0.2_300_2   22.110643  10.836245         16-8   \n",
       "3        Glam_dnn_MULTI_0.1_0.2_300_3   18.854342   9.344668         16-8   \n",
       "4        Glam_dnn_MULTI_0.1_0.2_300_4   20.752937   9.989418         16-8   \n",
       "..                                ...         ...        ...          ...   \n",
       "754  glacier_dnn_MULTI_0.1_0.2_300_20   60.638832  11.088364          6-3   \n",
       "755  glacier_dnn_MULTI_0.1_0.2_300_21  125.538750  13.766709          6-3   \n",
       "756  glacier_dnn_MULTI_0.1_0.2_300_22   60.688206  12.714522          6-3   \n",
       "757  glacier_dnn_MULTI_0.1_0.2_300_23  110.005013  14.443310          6-3   \n",
       "758  glacier_dnn_MULTI_0.1_0.2_300_24  144.228729  13.744843          6-3   \n",
       "\n",
       "    learning rate validation split  \n",
       "0             0.1              0.2  \n",
       "1             0.1              0.2  \n",
       "2             0.1              0.2  \n",
       "3             0.1              0.2  \n",
       "4             0.1              0.2  \n",
       "..            ...              ...  \n",
       "754           0.1              0.2  \n",
       "755           0.1              0.2  \n",
       "756           0.1              0.2  \n",
       "757           0.1              0.2  \n",
       "758           0.1              0.2  \n",
       "\n",
       "[759 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
