{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install chardet\n",
    "# import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import chardet\n",
    "# import glacierml as gl\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "print(tf.__version__)\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# raw_dataset = pd.read_csv(\"/data/fast0/datasets/glathida-3.1.0/data/T.csv\")\n",
    "# raw_dataset = pd.read_csv(\"~/stuff/coding/glacier/data/T.csv\")\n",
    "\n",
    "\n",
    "#examine data columns\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Organize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Load and organize the Glathida dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlaThiDa_ID</th>\n",
       "      <th>GLACIER_DB</th>\n",
       "      <th>GLACIER_ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>AREA</th>\n",
       "      <th>MEAN_SLOPE</th>\n",
       "      <th>MEAN_THICKNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>2095</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G007707E46420N</td>\n",
       "      <td>46.415400</td>\n",
       "      <td>7.708700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>2101</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G290838E77522N</td>\n",
       "      <td>77.522300</td>\n",
       "      <td>-69.161600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2109</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014140E77907N</td>\n",
       "      <td>77.907200</td>\n",
       "      <td>14.124400</td>\n",
       "      <td>2.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>2110</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014601E77917N</td>\n",
       "      <td>77.915200</td>\n",
       "      <td>14.601000</td>\n",
       "      <td>2.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>2111</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G016068E78246N</td>\n",
       "      <td>78.245480</td>\n",
       "      <td>16.064380</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>2112</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014123E77878N</td>\n",
       "      <td>77.875600</td>\n",
       "      <td>14.100400</td>\n",
       "      <td>8.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2113</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014442E77835N</td>\n",
       "      <td>77.835800</td>\n",
       "      <td>14.407700</td>\n",
       "      <td>47.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>2114</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014692E77911N</td>\n",
       "      <td>77.911100</td>\n",
       "      <td>14.687100</td>\n",
       "      <td>2.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2115</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014983E77927N</td>\n",
       "      <td>77.926600</td>\n",
       "      <td>14.968600</td>\n",
       "      <td>6.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2116</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G015001E78014N</td>\n",
       "      <td>77.998900</td>\n",
       "      <td>15.030500</td>\n",
       "      <td>5.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2117</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014953E78012N</td>\n",
       "      <td>78.013700</td>\n",
       "      <td>14.956900</td>\n",
       "      <td>2.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>2118</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014961E77957N</td>\n",
       "      <td>77.953100</td>\n",
       "      <td>14.964800</td>\n",
       "      <td>2.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2119</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G077080E43049N</td>\n",
       "      <td>43.043653</td>\n",
       "      <td>77.080587</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2120</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014055E77902N</td>\n",
       "      <td>77.897800</td>\n",
       "      <td>14.017600</td>\n",
       "      <td>6.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2121</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014257E77913N</td>\n",
       "      <td>77.911200</td>\n",
       "      <td>14.231800</td>\n",
       "      <td>17.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2122</td>\n",
       "      <td>GLIMS</td>\n",
       "      <td>G014108E77952N</td>\n",
       "      <td>77.944500</td>\n",
       "      <td>14.141200</td>\n",
       "      <td>1.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GlaThiDa_ID GLACIER_DB      GLACIER_ID        LAT        LON   AREA  \\\n",
       "604         2095      GLIMS  G007707E46420N  46.415400   7.708700    NaN   \n",
       "610         2101      GLIMS  G290838E77522N  77.522300 -69.161600    NaN   \n",
       "618         2109      GLIMS  G014140E77907N  77.907200  14.124400   2.84   \n",
       "619         2110      GLIMS  G014601E77917N  77.915200  14.601000   2.61   \n",
       "620         2111      GLIMS  G016068E78246N  78.245480  16.064380   2.10   \n",
       "621         2112      GLIMS  G014123E77878N  77.875600  14.100400   8.14   \n",
       "622         2113      GLIMS  G014442E77835N  77.835800  14.407700  47.30   \n",
       "623         2114      GLIMS  G014692E77911N  77.911100  14.687100   2.26   \n",
       "624         2115      GLIMS  G014983E77927N  77.926600  14.968600   6.24   \n",
       "625         2116      GLIMS  G015001E78014N  77.998900  15.030500   5.46   \n",
       "626         2117      GLIMS  G014953E78012N  78.013700  14.956900   2.51   \n",
       "627         2118      GLIMS  G014961E77957N  77.953100  14.964800   2.91   \n",
       "628         2119      GLIMS  G077080E43049N  43.043653  77.080587   2.61   \n",
       "629         2120      GLIMS  G014055E77902N  77.897800  14.017600   6.23   \n",
       "630         2121      GLIMS  G014257E77913N  77.911200  14.231800  17.60   \n",
       "631         2122      GLIMS  G014108E77952N  77.944500  14.141200   1.79   \n",
       "\n",
       "     MEAN_SLOPE  MEAN_THICKNESS  \n",
       "604         NaN             NaN  \n",
       "610         NaN           114.0  \n",
       "618         NaN            81.0  \n",
       "619         NaN            34.0  \n",
       "620        13.0            32.0  \n",
       "621         NaN            91.0  \n",
       "622         NaN           106.0  \n",
       "623         NaN            26.0  \n",
       "624         NaN            29.0  \n",
       "625         NaN            33.0  \n",
       "626         NaN            37.0  \n",
       "627         NaN            24.0  \n",
       "628        20.0            47.0  \n",
       "629         NaN            31.0  \n",
       "630         NaN            94.0  \n",
       "631         NaN            11.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_dataset = pd.read_csv('/data/fast0/datasets/glathida-3.1.0/data/T.csv')\n",
    "raw_dataset = pd.read_csv('~/data/glac/T_models/T.csv')\n",
    "glathida = raw_dataset.copy()\n",
    "glathida = glathida.drop([\n",
    "#                         'GlaThiDa_ID',\n",
    "#                         'GLACIER_DB',\n",
    "#                         'GLACIER_ID',\n",
    "                        'POLITICAL_UNIT',\n",
    "                        'GLACIER_NAME',\n",
    "                        'SURVEY_DATE',\n",
    "                        'MAXIMUM_THICKNESS',\n",
    "                        'MAX_THICKNESS_UNCERTAINTY',\n",
    "                        'DATA_FLAG',\n",
    "                        'ELEVATION_DATE',\n",
    "                        'SPONSORING_AGENCY',\n",
    "                        'REMARKS',\n",
    "                        'SURVEY_METHOD_DETAILS',\n",
    "                        'SURVEY_METHOD',\n",
    "                        'NUMBER_OF_SURVEY_POINTS',\n",
    "                        'NUMBER_OF_SURVEY_PROFILES',\n",
    "                        'TOTAL_LENGTH_OF_SURVEY_PROFILES',\n",
    "                        'INTERPOLATION_METHOD',\n",
    "                        'INVESTIGATOR', \n",
    "                        'REFERENCES',\n",
    "                        'MEAN_THICKNESS_UNCERTAINTY'\n",
    "                       ], axis=1)\n",
    "idx = glathida.index[glathida['GLACIER_DB'] == 'GLIMS']\n",
    "# glathida = glathida.loc[idx]\n",
    "# glathida['GLACIER_ID'] = glathida['GLACIER_ID'].str[6:]\n",
    "# glathida[\"GLACIER_DB\"].unique()\n",
    "glathida.loc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Load and Organize the RGI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# filename = '/home/sa42/data/glac/glims/glims_extra/dump2/07_rgi60_Svalbard.csv'\n",
    "# with open(filename, 'rb') as file:\n",
    "#     print(chardet.detect(file.read()))\n",
    "df = pd.read_csv('/home/sa42/data/glac/glims/glims_extra/dump2/07_rgi60_Svalbard.csv',\n",
    "                 encoding = 'ISO-8859-1')\n",
    "RGI_07 = df\n",
    "RGI_07 = RGI_07.drop([\n",
    "    'GLIMSId',\n",
    "    'BgnDate',\n",
    "    'EndDate',\n",
    "    'O1Region',\n",
    "    'O2Region',\n",
    "    'Status',\n",
    "    'Connect',\n",
    "    'Form',\n",
    "    'TermType',\n",
    "    'Surging',\n",
    "    'Linkages',\n",
    "    'Name',\n",
    "],axis=1)\n",
    "\n",
    "df = pd.read_csv('/home/sa42/data/glac/glims/glims_extra/dump2/10_rgi60_NorthAsia.csv',\n",
    "                 encoding='ISO-8859-1')\n",
    "RGI_10 = df\n",
    "RGI_10 = RGI_10.drop([\n",
    "    'GLIMSId',\n",
    "    'BgnDate',\n",
    "    'EndDate',\n",
    "    'O1Region',\n",
    "    'O2Region',\n",
    "    'Status',\n",
    "    'Connect',\n",
    "    'Form',\n",
    "    'TermType',\n",
    "    'Surging',\n",
    "    'Linkages',\n",
    "    'Name',\n",
    "],axis=1)\n",
    "\n",
    "with open('/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv','r') as temp_f:\n",
    "# get No of columns in each line\n",
    "    col_count = [ len(l.split(',')) for l in temp_f.readlines() ]\n",
    "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "column_names = [i for i in range(0, max(col_count))]\n",
    "### Read csv\n",
    "df = pd.read_csv('/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv',\n",
    "                 header=None, delimiter=',',names=column_names)\n",
    "col_names = list(df.iloc[0])\n",
    "df = pd.read_csv('/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv',\n",
    "                 header=None, delimiter=',', names=col_names)\n",
    "df.drop(0, inplace=True); df.reset_index(drop=True, inplace=True)\n",
    "RGI_13 = df\n",
    "RGI_13 = RGI_13.drop([\n",
    "    'GLIMSId',\n",
    "    'BgnDate',\n",
    "    'EndDate',\n",
    "    'O1Region',\n",
    "    'O2Region',\n",
    "    'Status',\n",
    "    'Connect',\n",
    "    'Form',\n",
    "    'TermType',\n",
    "    'Surging',\n",
    "    'Linkages',\n",
    "    'Name',\n",
    "],axis=1)\n",
    "\n",
    "df = pd.read_csv('/home/sa42/data/glac/glims/glims_extra/dump2/16_rgi60_LowLatitudes.csv',\n",
    "                 encoding='ISO-8859-1')\n",
    "RGI_16 = df\n",
    "RGI_16 = RGI_16.drop([\n",
    "    'GLIMSId',\n",
    "    'BgnDate',\n",
    "    'EndDate',\n",
    "    'O1Region',\n",
    "    'O2Region',\n",
    "    'Status',\n",
    "    'Connect',\n",
    "    'Form',\n",
    "    'TermType',\n",
    "    'Surging',\n",
    "    'Linkages',\n",
    "    'Name',\n",
    "],axis=1)\n",
    "\n",
    "RGI = pd.concat([RGI_07,RGI_10,RGI_13,RGI_16])\n",
    "RGI['RGIId'] = RGI['RGIId'].str[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Merge RGI and GlaThiDa along the RGI-ID\n",
    "The output will include the above GlaThiDa fields, as well as Zmin, Zmax, Zmed, Aspect, and Lmax from RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['MAXIMUM_THICKNESS'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_170377/1845933635.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m'Slope'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m'Lmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;34m'MAXIMUM_THICKNESS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m ]]\n\u001b[1;32m     23\u001b[0m \u001b[0mRGI_glathida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGI_glathida\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['MAXIMUM_THICKNESS'] not in index\""
     ]
    }
   ],
   "source": [
    "idx = glathida.index[glathida['GLACIER_DB']=='RGI']\n",
    "glathida = glathida.loc[idx]\n",
    "glathida.rename(columns = {'GLACIER_ID':'RGIId'},inplace=True)\n",
    "\n",
    "RGI_glathida = pd.merge(glathida,RGI, how = 'inner', on='RGIId')\n",
    "RGI_glathida = RGI_glathida[[\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'Aspect',\n",
    "    'AREA',\n",
    "    'MEAN_SLOPE',\n",
    "    'MEAN_THICKNESS',\n",
    "    'CenLon',\n",
    "    'CenLat',\n",
    "    'Area',\n",
    "    'Zmin',\n",
    "    'Zmax',\n",
    "    'Zmed',\n",
    "    'Slope',\n",
    "    'Lmax',\n",
    "    'MAXIMUM_THICKNESS',\n",
    "]]\n",
    "RGI_glathida = RGI_glathida.dropna()\n",
    "RGI_glathida=RGI_glathida.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate out features - what will be trained to predict desired attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset and reserve some to test what was trained.\n",
    "train_dataset = RGI_glathida.sample(frac=0.8, random_state=0)\n",
    "test_dataset = RGI_glathida.drop(train_dataset.index)\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "#define label - attribute training to be picked\n",
    "train_labels = train_features.pop('MAXIMUM_THICKNESS')\n",
    "test_labels = test_features.pop('MAXIMUM_THICKNESS')\n",
    "# train_features.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = {}\n",
    "variable_list = (\n",
    " 'LAT',\n",
    " 'LON',\n",
    " 'AREA',\n",
    " 'MEAN_SLOPE',\n",
    " 'MEAN_THICKNESS',\n",
    " 'CenLon',\n",
    " 'CenLat',\n",
    " 'Aspect',\n",
    " 'Area',\n",
    " 'Zmin',\n",
    " 'Zmax',\n",
    " 'Zmed',\n",
    " 'Slope',\n",
    " 'Lmax'\n",
    ")\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    normalizer[variable_name] = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
    "    normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n",
    "    \n",
    "    \n",
    "normalizer['ALL'] = preprocessing.Normalization(axis=-1)\n",
    "normalizer['ALL'].adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single variable linear regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glacierml\n",
    "\n",
    "def build_linear_model(normalizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "        loss='mean_absolute_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_single_model_variable(x, y,feature_name):\n",
    "    plt.scatter(train_features[feature_name], train_labels, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Avg Thickness (m)')\n",
    "#     plt.xlim((0,20))\n",
    "    plt.legend()\n",
    "      \n",
    "def plot_loss(history):\n",
    "#     plt.subplots(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    #   plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "linear_model = {}\n",
    "linear_history = {}\n",
    "linear_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    linear_model[variable_name] = build_linear_model(normalizer[variable_name])\n",
    "    linear_history[variable_name] = linear_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    \n",
    "    \n",
    "    linear_results[variable_name] = linear_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "linear_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(linear_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,6,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    ax = plt.subplot(4,4,i+1)\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plot_loss(linear_history[variable_name])\n",
    "    ax.set_title(variable_name)\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = build_linear_model(normalizer['ALL'])\n",
    "\n",
    "history_full = linear_model.fit(\n",
    "train_features, train_labels,        \n",
    "   epochs=1000,\n",
    "   verbose=0,\n",
    "   validation_split = 0.2)\n",
    "\n",
    "test_results['MULTI'] = linear_model.evaluate(\n",
    "    test_features,\n",
    "    test_labels, verbose=0)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_full_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model(norm):\n",
    "    model = keras.Sequential([\n",
    "              norm,\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(1) ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dnn_test_results(feature_name):\n",
    "    dnn_test_results[feature_name] = dnn_model.evaluate(\n",
    "        test_features[feature_name],\n",
    "        test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "dnn_model = {}\n",
    "dnn_history = {}\n",
    "dnn_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:\n",
    "\n",
    "    dnn_model[variable_name] = build_dnn_model(normalizer[variable_name])\n",
    "    dnn_history[variable_name] = dnn_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    dnn_results[variable_name] = dnn_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "dnn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(dnn_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(4,4,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    xmax = np.max(train_features[variable_name])\n",
    "    xmin = np.min(train_features[variable_name])\n",
    "    x = tf.linspace(xmin, xmax, 101)\n",
    "    y = dnn_model[variable_name].predict(x)\n",
    "    plot_single_model_variable(x,y,variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(4,4,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    ax = plt.subplot(4,4,i+1)\n",
    "    plot_loss(dnn_history[variable_name])\n",
    "    ax.set_title(variable_name)\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_full_model = build_dnn_model(normalizer['ALL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dnn_history_full = dnn_full_model.fit(\n",
    "    train_features, train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dnn_history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(dnn_history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_full_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dnn_full_model.predict(test_features)\n",
    "plt.plot(test_labels,y,'.')\n",
    "plt.plot((0,200),(0,200),'-')\n",
    "plt.xlabel('True Thickness (m)')\n",
    "plt.ylabel('Model Thickness (m)')\n",
    "plt.xlim((0,200))\n",
    "plt.ylim((0,200))\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_res.EPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (Cartopy)-f",
   "language": "python",
   "name": "python-cartopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
