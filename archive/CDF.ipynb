{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ec30a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'glacierml' has no attribute 'load_LOO_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7634eae5fd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglacierml\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mE_delta_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mareas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_LOO_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'glacierml' has no attribute 'load_LOO_data'"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --user install tensorflow==2.8.0. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glacierml as gl\n",
    "import matplotlib.pyplot as plt\n",
    "E_delta_a, areas, cols, train = gl.load_LOO_data(include_train = True)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415a025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7108359",
   "metadata": {},
   "source": [
    "# Plot the glacier volume cumulative distribution function (CDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52801729",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.mean(data[cols],axis = 1) * data['Area']\n",
    "data_2 = data['FMT'] * data['Area']\n",
    "#sort data\n",
    "\n",
    "\n",
    "x_1 = np.sort(data_1)\n",
    "# x = data_1\n",
    "#calculate CDF values\n",
    "y_1 = 1. * np.arange(len(data_1)) / (len(data_1) - 1)\n",
    "\n",
    "\n",
    "x_2 = np.sort(data_2)\n",
    "# x = data\n",
    "#calculate CDF values\n",
    "y_2 = 1. * np.arange(len(data_2)) / (len(data_2) - 1)\n",
    "\n",
    "#plot CDF\n",
    "fig, ax = plt.subplots(1,1,figsize=(8, 8))\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.suptitle('RGI Volume Cumulative Distribution Function', fontsize=18, y=0.93)\n",
    "fig.patch.set_facecolor('w')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.plot(\n",
    "    x_1, \n",
    "    y_1,\n",
    "    c = 'blue',\n",
    "    label = 'Farinotti Volume'\n",
    "    \n",
    ")\n",
    "plt.plot(\n",
    "    x_2, \n",
    "    y_2,\n",
    "    c = 'orange',\n",
    "    label = 'Edasi Volume'\n",
    ")\n",
    "plt.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Ice Volume')\n",
    "ax.set_ylabel('% of Glaciers Containing Ice')\n",
    "\n",
    "k = 1\n",
    "for lambd in (1,10,100):\n",
    "    plt.plot(x_1, 1 - np.exp(- (x_1/lambd)**k ),'--')\n",
    "# plt.savefig('figs/cdf/cdf.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c3610",
   "metadata": {},
   "source": [
    "# Plot probability density function (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots()\n",
    "plt.hist(np.log10(data_1), 250)\n",
    "plt.hist(np.log10(data_2), 250)\n",
    "plt.yscale('log')\n",
    "# plt.xlim(0,1e5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56babd",
   "metadata": {},
   "source": [
    "# Use the KS test to show that the differences are significantly different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22719544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359afcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = interp1d(x_1,y_1)\n",
    "f2 = interp1d(x_2,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b03daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(x_2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.logspace(-0.96, 6, 100)\n",
    "D = np.max(abs(f2(x) - f1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b04f0b",
   "metadata": {},
   "source": [
    "# Resample to test difference of sums\n",
    "\n",
    "We want to address the question, \"Is our total ice volumes significantly different from published estimates?\"  The following shows that the answer is yes.\n",
    "\n",
    "Another way to ask the question is, \"What is the probability that the published estimate is actually less than our value?\"\n",
    "\n",
    "Let's randomly re-sample 90% of the distributions 1000 times and each time, calculate the sum of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99351dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "N1 = len(data_1)\n",
    "assert len(data_1) == len(data_2)\n",
    "resample_rate = 0.9\n",
    "Nsample = int(N1 * resample_rate)\n",
    "X = []\n",
    "for i in tqdm(range(1000)):\n",
    "    sum1 = np.sum(data_1.to_numpy()[np.random.choice(N1, size=Nsample, replace=False)])\n",
    "    sum2 = np.sum(data_2.to_numpy()[np.random.choice(N1, size=Nsample, replace=False)])\n",
    "\n",
    "    X.append (sum1-sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085df7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots()\n",
    "plt.hist(np.array(X)/resample_rate/1e6,20,density=True)\n",
    "plt.title(r'Resampled difference between the $V_F$ and $V_E$')\n",
    "plt.ylabel('Frequentist Probability')\n",
    "plt.xlabel('Ice Volume (x1000 cu km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "for i in range(len(X)):\n",
    "    ax.plot(i,np.mean(X[0:i])/1e6/resample_rate,'ok')\n",
    "plt.title('Convergence of the mean')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean (x1000 cu km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9403a5",
   "metadata": {},
   "source": [
    "### The distribution looks approximately normal (ignore the skew for now). So what is the probability that $V_F == V_E$ given the resampled difference mean and standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21631d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu  = np.mean(X)/resample_rate/1e6\n",
    "sig = np.std(X)/resample_rate/1e6\n",
    "print(f'Mean of Differences = {mu:.2f}')\n",
    "print(f'Std of Differences = {sig:.2f}')\n",
    "from scipy.stats import norm\n",
    "value = 0 # this corresponds to zero difference\n",
    "probability=norm.pdf(value,loc = mu, scale=sig)\n",
    "print(f'Probability of equality of volumes = 10**{np.log10(probability):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4dfa31",
   "metadata": {},
   "source": [
    "### So the probabilty is extremely low that the two estimates are the same!\n",
    "This is similar to the Z-test with large N.  Let's calculate what the Z-test gave us to see whether the the MC resampling added any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = np.sum(data_1.to_numpy())\n",
    "mu2 = np.sum(data_2.to_numpy())\n",
    "std1 = 41\n",
    "std2 = 42.19\n",
    "N = len(data_2.to_numpy())\n",
    "Z = np.abs(mu1-mu2) / np.sqrt(std1**2/N+std2**2/N)\n",
    "print(f'log10(Z) = {np.log10(Z)}')\n",
    "\n",
    "print(f'p  = {norm.sf(abs(Z))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd930ff",
   "metadata": {},
   "source": [
    "For such a large p-value, scipy just rounds the probabilty to zero. I think that settles it.  The differences are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ac4f8",
   "metadata": {},
   "source": [
    "# Next level up:  Account for the uncertainty as quantified in the bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_F = (data['Farinotti Mean Thickness'] * data['Area']).to_numpy()\n",
    "m_E = (data['Edasi Mean Thickness'] * data['Area']).to_numpy()\n",
    "s_E = (data['Edasi Thickness Std Dev'] * data['Area']).to_numpy()\n",
    "\n",
    "N = len(data_1)\n",
    "\n",
    "resample_rate = 0.9\n",
    "Nsample = int(N1 * resample_rate)\n",
    "X = []\n",
    "vector_normal_rand = np.vectorize(norm.rvs)\n",
    "for i in tqdm(range(1000)):\n",
    "    \n",
    "    # First, draw from the Bootstrap EDF\n",
    "    edasi_sample=vector_normal_rand(m_E,s_E)\n",
    "#     edasi_sample = np.zeros_like(m_E)\n",
    "#     for i in range(len(edasi_sample)):\n",
    "#         edasi_sample[i] = norm.rvs(loc=m_E[i], scale=s_E[i], size=1)\n",
    "    \n",
    "    mc_resample_F = m_F[np.random.choice(N, size=Nsample, replace=False)]\n",
    "    mc_resample_E = m_E[np.random.choice(N, size=Nsample, replace=False)]\n",
    "    \n",
    "    sum_F = np.sum(mc_resample_F)\n",
    "    sum_E = np.sum(mc_resample_E)\n",
    "\n",
    "    X.append (sum_F-sum_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,'ok')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(X)/1e6 / resample_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu  = np.mean(X)/resample_rate/1e6\n",
    "sig = np.std(X)/resample_rate/1e6\n",
    "print(f'Mean of Differences = {mu:.2f}')\n",
    "print(f'Std of Differences = {sig:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c001e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
