{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1135c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]2023-09-19 10:09:12.602710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-19 10:09:14.782502: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "100%|██████████| 500/500 [34:09<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Single model seed finder\n",
    "\n",
    "from tqdm import tqdm\n",
    "sds = {}\n",
    "for SEED in tqdm(range(500)):\n",
    "    #     print(SEED)\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    \n",
    "\n",
    "    import tensorflow as tf\n",
    "    tf.compat.v1.logging.set_verbosity(\n",
    "        0\n",
    "    )\n",
    "\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    def set_seeds(seed=SEED):\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def set_global_determinism(seed=SEED):\n",
    "        set_seeds(seed=seed)\n",
    "\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "    # Call the above function with seed value\n",
    "    set_global_determinism(seed=SEED)\n",
    "\n",
    "    import glacierml as gl\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import imageio\n",
    "    import math\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental import preprocessing\n",
    "    from scipy.stats import gaussian_kde\n",
    "    import os\n",
    "    from scipy.stats import norm\n",
    "    import statsmodels.api as sm\n",
    "    from scipy.stats import kstest\n",
    "    from scipy.stats import shapiro \n",
    "    import absl.logging\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Set up data\n",
    "#     n = 0\n",
    "    df = gl.coregister_data('4')\n",
    "    df = df.drop('RGIId', axis = 1)\n",
    "#     df = df.drop(['RGIId','CenLat','CenLon'], axis = 1)\n",
    "    df_sampler = df.copy()\n",
    "    df_trainer = df.copy()\n",
    "#     rs1 = 421\n",
    "    rs2 = 323\n",
    "    n = 0\n",
    "    \n",
    "    \n",
    "    # define model parameters\n",
    "    pr = 0.075\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0.001,\n",
    "        patience = 10,\n",
    "        verbose = 0,\n",
    "        mode = 'auto',\n",
    "        baseline = None,\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    \n",
    "    trfeat = {}\n",
    "    trlabs = {}\n",
    "    tefeat = {}\n",
    "    telabs = {}\n",
    "#     normalizer = {}\n",
    "    \n",
    "    # split data\n",
    "#     for n,df in enumerate(df_list):\n",
    "    (\n",
    "        trfeat[n], tefeat[n], trlabs[n], telabs[n]\n",
    "    ) = gl.split_data(df,rs2)\n",
    "    \n",
    "    # build 3 models with subset data\n",
    "    model = {}\n",
    "    model_history = {}\n",
    "\n",
    "    l1 = {}\n",
    "    l2 = {}\n",
    "#     for n,df in enumerate(df_list):\n",
    "\n",
    "\n",
    "    total_inputs = (len(df.columns)) * len(df)\n",
    "    dp = pr * total_inputs\n",
    "    tp = dp - (len(df.columns) + (len(df.columns)-1) )\n",
    "    g = (len(df.columns) + (len(df.columns) - 1))\n",
    "    l2[n] = 4\n",
    "    l1[n] = int((dp - 1 - g - 2*l2[n]) / (10 + l2[n]))\n",
    "    normalizer = preprocessing.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(trfeat[n]))\n",
    "\n",
    "    model[n] = gl.build_dnn_model(\n",
    "        normalizer, learning_rate = 0.01, \n",
    "        layer_1 = l1[n], layer_2 = l2[n],loss = 'mae'\n",
    "    )\n",
    "\n",
    "    model_history[n] = model[n].fit(\n",
    "        trfeat[n],\n",
    "        trlabs[n],\n",
    "        validation_split=0.2,\n",
    "        callbacks = [callback],\n",
    "        verbose=0, \n",
    "        epochs=500\n",
    "    )\n",
    "        \n",
    "        \n",
    "    y = {}\n",
    "    y[n] = (model[n].predict(tefeat[n],verbose = 0).flatten())\n",
    "\n",
    "    perc_res = (y[n] - telabs[n]) / telabs[n]\n",
    "    mean_perc_res = np.mean(perc_res)\n",
    "    std_perc_res = np.std(perc_res)\n",
    "    res = (y[n] - telabs[n])\n",
    "    mean_res = np.mean(res)\n",
    "    std_res = np.std(res)\n",
    "    sds[str(SEED)] = [mean_perc_res,std_perc_res,mean_res,std_res]\n",
    "df = pd.DataFrame.from_dict(sds,orient = 'index')\n",
    "import pickle\n",
    "with open('seeds_4.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xval seed finder\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# sds = {}\n",
    "# for SEED in tqdm(range(500)):\n",
    "#     #     print(SEED)\n",
    "#     import os\n",
    "#     os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    \n",
    "\n",
    "#     import tensorflow as tf\n",
    "#     tf.compat.v1.logging.set_verbosity(\n",
    "#         0\n",
    "#     )\n",
    "\n",
    "#     import numpy as np\n",
    "#     import random\n",
    "\n",
    "#     def set_seeds(seed=SEED):\n",
    "#         os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#         random.seed(seed)\n",
    "#         tf.random.set_seed(seed)\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#     def set_global_determinism(seed=SEED):\n",
    "#         set_seeds(seed=seed)\n",
    "\n",
    "#         os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#         os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "#         tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#         tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "#     # Call the above function with seed value\n",
    "#     set_global_determinism(seed=SEED)\n",
    "\n",
    "#     import glacierml as gl\n",
    "#     import tensorflow as tf\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import imageio\n",
    "#     import math\n",
    "#     from tensorflow import keras\n",
    "#     from tensorflow.keras import layers\n",
    "#     from tensorflow.keras.layers.experimental import preprocessing\n",
    "#     from scipy.stats import gaussian_kde\n",
    "#     import os\n",
    "#     from scipy.stats import norm\n",
    "#     import statsmodels.api as sm\n",
    "#     from scipy.stats import kstest\n",
    "#     from scipy.stats import shapiro \n",
    "#     import absl.logging\n",
    "#     from tqdm import tqdm\n",
    "    \n",
    "#     # Set up data\n",
    "# #     n = 0\n",
    "#     df = gl.coregister_data('4')\n",
    "# #     df = df.drop('RGIId', axis = 1)\n",
    "#     df = df.drop(['RGIId','CenLat','CenLon'],axis = 1)\n",
    "#     df_sampler = df.copy()\n",
    "#     df_trainer = df.copy()\n",
    "#     rs1 = 421\n",
    "#     rs2 = 67\n",
    "\n",
    "#     df1 = df_sampler.sample(frac = 0.333333333, random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df1.index)\n",
    "\n",
    "#     df2 = df_sampler.sample(frac = 0.5,random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df2.index)\n",
    "\n",
    "#     df3 = df_sampler\n",
    "\n",
    "#     df1test = df_trainer.drop(df1.index)\n",
    "#     df2test = df_trainer.drop(df2.index)\n",
    "#     df3test = df_trainer.drop(df3.index)\n",
    "#     df_list = [df1,df2,df3]\n",
    "\n",
    "#     df_test_list = [df1test,df2test,df3test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # define model parameters\n",
    "#     pr = 0.075\n",
    "\n",
    "#     callback = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor = 'val_loss',\n",
    "#         min_delta = 0.001,\n",
    "#         patience = 10,\n",
    "#         verbose = 0,\n",
    "#         mode = 'auto',\n",
    "#         baseline = None,\n",
    "#         restore_best_weights = True\n",
    "#     )\n",
    "    \n",
    "#     trfeat = {}\n",
    "#     trlabs = {}\n",
    "#     tefeat = {}\n",
    "#     telabs = {}\n",
    "# #     normalizer = {}\n",
    "    \n",
    "#     # split data\n",
    "#     for n,df in enumerate(df_list):\n",
    "#         (\n",
    "#             trfeat[n], tefeat[n], trlabs[n], telabs[n]\n",
    "#         ) = gl.split_data(df,rs2)\n",
    "    \n",
    "#     # build 3 models with subset data\n",
    "#     model = {}\n",
    "#     model_history = {}\n",
    "\n",
    "#     l1 = {}\n",
    "#     l2 = {}\n",
    "#     for n,df in enumerate(df_list):\n",
    "\n",
    "\n",
    "#         total_inputs = (len(df.columns)) * len(df)\n",
    "#         dp = pr * total_inputs\n",
    "#         tp = dp - (len(df.columns) + (len(df.columns)-1) )\n",
    "#         g = (len(df.columns) + (len(df.columns) - 1))\n",
    "#         l2[n] = 4\n",
    "#         l1[n] = int((dp - 1 - g - 2*l2[n]) / (10 + l2[n]))\n",
    "#         normalizer = preprocessing.Normalization(axis=-1)\n",
    "#         normalizer.adapt(np.array(trfeat[n]))\n",
    "\n",
    "#         model[n] = gl.build_dnn_model(\n",
    "#             normalizer, learning_rate = 0.01, \n",
    "#             layer_1 = l1[n], layer_2 = l2[n],loss = 'mae'\n",
    "#         )\n",
    "\n",
    "#         model_history[n] = model[n].fit(\n",
    "#             trfeat[n],\n",
    "#             trlabs[n],\n",
    "#             validation_split=0.2,\n",
    "#             callbacks = [callback],\n",
    "#             verbose=0, \n",
    "#             epochs=500\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#     y = {}\n",
    "#     for n,dftest in enumerate(df_test_list):\n",
    "#         y[n] = (model[n].predict(dftest.drop('Thickness',axis = 1),verbose = 0).flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "# #     w = {}\n",
    "#     perc_res = np.array([])\n",
    "#     res = np.array([])\n",
    "#     for n,df,dftest in (zip(range(4),df_list,df_test_list)):\n",
    "#         percent_res = (y[n] - dftest['Thickness']) / dftest['Thickness']\n",
    "#         residuals = (y[n] - dftest['Thickness'])\n",
    "#         mean_perc_res = np.mean(perc_res)\n",
    "# #         mean_est = np.mean(y[n])\n",
    "\n",
    "# #         bias = mean_perc_res * mean_est\n",
    "\n",
    "# #         q75 = (np.percentile(percent_res, 75))\n",
    "# #         q25 = (np.percentile(percent_res, 25))\n",
    "# #         iqr = q75 - q25\n",
    "# #         sig = (iqr / 1.34896) * mean_est\n",
    "# #         w[n] = abs(bias) + sig**2\n",
    "#         perc_res = np.append(percent_res,perc_res)\n",
    "#         res = np.append(res,residuals)\n",
    "#     mean_res = np.mean(res)\n",
    "#     std_res = np.std(res)\n",
    "#     mean_perc_res = np.mean(perc_res)\n",
    "#     std_perc_res = np.std(perc_res)\n",
    "    \n",
    "\n",
    "# # #     print(dp)\n",
    "#     sds[str(SEED)] = [mean_perc_res,std_perc_res,mean_res,std_res]\n",
    "#     print(sds[str(SEED)])\n",
    "# import pickle\n",
    "\n",
    "# df = pd.DataFrame.from_dict(sds,orient = 'index')\n",
    "# with open('seeds_iterated_xval_locdropped.pkl', 'wb') as file:\n",
    "#     pickle.dump(df, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# #     print(sds[str(SEED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kfold seed finder\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# sds = {}\n",
    "# for SEED in tqdm(range(500)):\n",
    "#     #     print(SEED)\n",
    "#     import os\n",
    "#     os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    \n",
    "\n",
    "#     import tensorflow as tf\n",
    "#     tf.compat.v1.logging.set_verbosity(\n",
    "#         0\n",
    "#     )\n",
    "\n",
    "#     import numpy as np\n",
    "#     import random\n",
    "\n",
    "#     def set_seeds(seed=SEED):\n",
    "#         os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#         random.seed(seed)\n",
    "#         tf.random.set_seed(seed)\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#     def set_global_determinism(seed=SEED):\n",
    "#         set_seeds(seed=seed)\n",
    "\n",
    "#         os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#         os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "#         tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#         tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "#     # Call the above function with seed value\n",
    "#     set_global_determinism(seed=SEED)\n",
    "\n",
    "#     import glacierml as gl\n",
    "#     import tensorflow as tf\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import imageio\n",
    "#     import math\n",
    "#     from tensorflow import keras\n",
    "#     from tensorflow.keras import layers\n",
    "#     from tensorflow.keras.layers.experimental import preprocessing\n",
    "#     from scipy.stats import gaussian_kde\n",
    "#     import os\n",
    "#     from scipy.stats import norm\n",
    "#     import statsmodels.api as sm\n",
    "#     from scipy.stats import kstest\n",
    "#     from scipy.stats import shapiro \n",
    "#     import absl.logging\n",
    "#     from tqdm import tqdm\n",
    "    \n",
    "#     # Set up data\n",
    "# #     n = 0\n",
    "#     df = gl.coregister_data('4')\n",
    "# #     df = df.drop('RGIId', axis = 1)\n",
    "#     df = df.drop(['RGIId','CenLat','CenLon'],axis = 1)\n",
    "#     df_sampler = df.copy()\n",
    "#     df_trainer = df.copy()\n",
    "#     rs1 = 471\n",
    "#     rs2 = 67\n",
    "\n",
    "#     df1test = df_sampler.sample(frac = 0.333333333, random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df1test.index)\n",
    "\n",
    "#     df2test = df_sampler.sample(frac = 0.5,random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df2test.index)\n",
    "\n",
    "#     df3test = df_sampler\n",
    "\n",
    "#     df1 = df_trainer.drop(df1test.index)\n",
    "#     df2 = df_trainer.drop(df2test.index)\n",
    "#     df3 = df_trainer.drop(df3test.index)\n",
    "#     df_list = [df1,df2,df3]\n",
    "\n",
    "#     df_test_list = [df1test,df2test,df3test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # define model parameters\n",
    "#     pr = 0.075\n",
    "\n",
    "#     callback = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor = 'val_loss',\n",
    "#         min_delta = 0.001,\n",
    "#         patience = 10,\n",
    "#         verbose = 0,\n",
    "#         mode = 'auto',\n",
    "#         baseline = None,\n",
    "#         restore_best_weights = True\n",
    "#     )\n",
    "    \n",
    "#     trfeat = {}\n",
    "#     trlabs = {}\n",
    "#     tefeat = {}\n",
    "#     telabs = {}\n",
    "# #     normalizer = {}\n",
    "    \n",
    "#     # split data\n",
    "#     for n,df in enumerate(df_list):\n",
    "#         (\n",
    "#             trfeat[n], tefeat[n], trlabs[n], telabs[n]\n",
    "#         ) = gl.split_data(df,rs2)\n",
    "    \n",
    "#     # build 3 models with subset data\n",
    "#     model = {}\n",
    "#     model_history = {}\n",
    "\n",
    "#     l1 = {}\n",
    "#     l2 = {}\n",
    "#     for n,df in enumerate(df_list):\n",
    "\n",
    "\n",
    "#         total_inputs = (len(df.columns)) * len(df)\n",
    "#         dp = pr * total_inputs\n",
    "#         tp = dp - (len(df.columns) + (len(df.columns)-1) )\n",
    "#         g = (len(df.columns) + (len(df.columns) - 1))\n",
    "#         l2[n] = 4\n",
    "#         l1[n] = int((dp - 1 - g - 2*l2[n]) / (10 + l2[n]))\n",
    "#         normalizer = preprocessing.Normalization(axis=-1)\n",
    "#         normalizer.adapt(np.array(trfeat[n]))\n",
    "\n",
    "#         model[n] = gl.build_dnn_model(\n",
    "#             normalizer, learning_rate = 0.01, \n",
    "#             layer_1 = l1[n], layer_2 = l2[n],loss = 'mae'\n",
    "#         )\n",
    "\n",
    "#         model_history[n] = model[n].fit(\n",
    "#             trfeat[n],\n",
    "#             trlabs[n],\n",
    "#             validation_split=0.2,\n",
    "#             callbacks = [callback],\n",
    "#             verbose=0, \n",
    "#             epochs=500\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#     y = {}\n",
    "#     for n,dftest in enumerate(df_test_list):\n",
    "#         y[n] = (model[n].predict(dftest.drop('Thickness',axis = 1),verbose = 0).flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "#     w = {}\n",
    "#     perc_res = np.array([])\n",
    "#     res = np.array([])\n",
    "#     for n,df,dftest in (zip(range(4),df_list,df_test_list)):\n",
    "#         percent_res = (y[n] - dftest['Thickness']) / dftest['Thickness']\n",
    "#         residuals = (y[n] - dftest['Thickness'])\n",
    "#         mean_perc_res = np.mean(perc_res)\n",
    "# #         mean_est = np.mean(y[n])\n",
    "\n",
    "# #         bias = mean_perc_res * mean_est\n",
    "\n",
    "# #         q75 = (np.percentile(percent_res, 75))\n",
    "# #         q25 = (np.percentile(percent_res, 25))\n",
    "# #         iqr = q75 - q25\n",
    "# #         sig = (iqr / 1.34896) * mean_est\n",
    "# #         w[n] = abs(bias) + sig**2\n",
    "#         perc_res = np.append(percent_res,perc_res)\n",
    "#         res = np.append(res,residuals)\n",
    "#     mean_res = np.mean(res)\n",
    "#     std_res = np.std(res)\n",
    "#     mean_perc_res = np.mean(perc_res)\n",
    "#     std_perc_res = np.std(perc_res)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     sds[str(SEED)] = [mean_perc_res,std_perc_res,mean_res,std_res]\n",
    "# import pickle\n",
    "\n",
    "# df = pd.DataFrame.from_dict(sds,orient = 'index')\n",
    "# with open('seeds_iterated_kfold_locdropped.pkl', 'wb') as file:\n",
    "#     pickle.dump(df, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# #     print(sds[str(SEED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('seeds_iterated_xval.pkl')\n",
    "\n",
    "df\n",
    "\n",
    "ilist =[]\n",
    "for i in range(0,500,75):\n",
    "    print(i)\n",
    "    ilist.append(i)\n",
    "ilist.append(500)\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [(0, 0, 0), (1, 0, 0)] # first color is black, last is red\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", colors, N=20)\n",
    "\n",
    "plt.scatter(df.index,1-df[0],c = 1-df[0],cmap = cm)\n",
    "plt.xticks(ilist,ilist)\n",
    "\n",
    "\n",
    "plt.title('Tensorflow Random Seeds')\n",
    "plt.ylabel('Probability of Equality of Volume')\n",
    "plt.xlabel('Random Seed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# sds = {}\n",
    "# for SEED in tqdm(range(500)):\n",
    "#     #     print(SEED)\n",
    "#     import os\n",
    "#     os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    \n",
    "\n",
    "#     import tensorflow as tf\n",
    "#     tf.compat.v1.logging.set_verbosity(\n",
    "#         0\n",
    "#     )\n",
    "\n",
    "#     import numpy as np\n",
    "#     import random\n",
    "\n",
    "#     def set_seeds(seed=SEED):\n",
    "#         os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#         random.seed(seed)\n",
    "#         tf.random.set_seed(seed)\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#     def set_global_determinism(seed=SEED):\n",
    "#         set_seeds(seed=seed)\n",
    "\n",
    "#         os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#         os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "#         tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#         tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "#     # Call the above function with seed value\n",
    "#     set_global_determinism(seed=SEED)\n",
    "\n",
    "#     import glacierml as gl\n",
    "#     import tensorflow as tf\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import imageio\n",
    "#     import math\n",
    "#     from tensorflow import keras\n",
    "#     from tensorflow.keras import layers\n",
    "#     from tensorflow.keras.layers.experimental import preprocessing\n",
    "#     from scipy.stats import gaussian_kde\n",
    "#     import os\n",
    "#     from scipy.stats import norm\n",
    "#     import statsmodels.api as sm\n",
    "#     from scipy.stats import kstest\n",
    "#     from scipy.stats import shapiro \n",
    "#     import absl.logging\n",
    "#     from tqdm import tqdm\n",
    "    \n",
    "#     # Set up data\n",
    "# #     n = 0\n",
    "#     df = gl.coregister_data('4')\n",
    "#     df = df.drop('RGIId', axis = 1)\n",
    "#     df_sampler = df.copy()\n",
    "#     df_trainer = df.copy()\n",
    "#     rs1 = 421\n",
    "#     rs2 = 67\n",
    "\n",
    "#     df1 = df_sampler.sample(frac = 0.333333333, random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df1.index)\n",
    "\n",
    "#     df2 = df_sampler.sample(frac = 0.5,random_state = rs1)\n",
    "#     df_sampler = df_sampler.drop(df2.index)\n",
    "\n",
    "#     df3 = df_sampler\n",
    "\n",
    "#     df1test = df_trainer.drop(df1test.index)\n",
    "#     df2test = df_trainer.drop(df2test.index)\n",
    "#     df3test = df_trainer.drop(df3test.index)\n",
    "#     df_list = [df1,df2,df3]\n",
    "\n",
    "#     df_test_list = [df1test,df2test,df3test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # define model parameters\n",
    "#     pr = 0.075\n",
    "\n",
    "#     callback = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor = 'val_loss',\n",
    "#         min_delta = 0.001,\n",
    "#         patience = 10,\n",
    "#         verbose = 0,\n",
    "#         mode = 'auto',\n",
    "#         baseline = None,\n",
    "#         restore_best_weights = True\n",
    "#     )\n",
    "    \n",
    "#     trfeat = {}\n",
    "#     trlabs = {}\n",
    "#     tefeat = {}\n",
    "#     telabs = {}\n",
    "# #     normalizer = {}\n",
    "    \n",
    "#     # split data\n",
    "#     for n,df in enumerate(df_list):\n",
    "#         (\n",
    "#             trfeat[n], tefeat[n], trlabs[n], telabs[n]\n",
    "#         ) = gl.split_data(df,rs1)\n",
    "    \n",
    "#     # build 3 models with subset data\n",
    "#     model = {}\n",
    "#     model_history = {}\n",
    "\n",
    "#     l1 = {}\n",
    "#     l2 = {}\n",
    "#     for n,df in enumerate(df_list):\n",
    "\n",
    "\n",
    "#         total_inputs = (len(df.columns)) * len(df)\n",
    "#         dp = pr * total_inputs\n",
    "#         tp = dp - (len(df.columns) + (len(df.columns)-1) )\n",
    "#         g = (len(df.columns) + (len(df.columns) - 1))\n",
    "#         l2[n] = 4\n",
    "#         l1[n] = int((dp - 1 - g - 2*l2[n]) / (10 + l2[n]))\n",
    "#         normalizer = preprocessing.Normalization(axis=-1)\n",
    "#         normalizer.adapt(np.array(trfeat[n]))\n",
    "\n",
    "#         model[n] = gl.build_dnn_model(\n",
    "#             normalizer, learning_rate = 0.01, \n",
    "#             layer_1 = l1[n], layer_2 = l2[n],loss = 'mae'\n",
    "#         )\n",
    "\n",
    "#         model_history[n] = model[n].fit(\n",
    "#             trfeat[n],\n",
    "#             trlabs[n],\n",
    "#             validation_split=0.2,\n",
    "#             callbacks = [callback],\n",
    "#             verbose=0, \n",
    "#             epochs=500\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#     y = {}\n",
    "#     for n,dftest in enumerate(df_test_list):\n",
    "#         y[n] = (model[n].predict(dftest.drop('Thickness',axis = 1),verbose = 0).flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "#     w = {}\n",
    "#     for n,df,dftest in (zip(range(4),df_list,df_test_list)):\n",
    "#         perc_res = (y[n] - dftest['Thickness']) / dftest['Thickness']\n",
    "#         mean_perc_res = np.mean(perc_res)\n",
    "#         mean_est = np.mean(y[n])\n",
    "\n",
    "#         bias = mean_perc_res * mean_est\n",
    "\n",
    "#         q75 = (np.percentile(perc_res, 75))\n",
    "#         q25 = (np.percentile(perc_res, 25))\n",
    "#         iqr = q75 - q25\n",
    "#         sig = (iqr / 1.34896) * mean_est\n",
    "#         w[n] = abs(bias) + sig**2\n",
    "\n",
    "    \n",
    "#     RGI = gl.load_RGI()\n",
    "#     df = gl.coregister_data('4')\n",
    "#     rfp = RGI[list(df.drop('Thickness', axis = 1))]\n",
    "#     est = pd.DataFrame()\n",
    "#     for n in range(0,3,1):\n",
    "#         n = str(i)\n",
    "#     estimates = pd.Series(\n",
    "#         model[n].predict(rfp.drop('RGIId',axis = 1),verbose = 0).flatten(),name = str(n)\n",
    "#     )\n",
    "#     est = pd.concat([est,estimates], axis  = 1)\n",
    "#     rfp = rfp.join(est)\n",
    "    \n",
    "    \n",
    "#     rfp['w_thickness'] = (rfp['0']/w[0] + rfp['1']/w[1] + rfp['2']/w[2]\n",
    "#     ) / (1/w[0] + 1/w[1] + 1/w[2])\n",
    "    \n",
    "    \n",
    "#     data = gl.load_notebook_data(coregistration = '4')[['RGIId','FMT']]\n",
    "#     data = pd.merge(rfp,data,how = 'inner', on = 'RGIId')\n",
    "#     data = data.dropna(subset = 'FMT')\n",
    "\n",
    "    \n",
    "#     data_1 = (data['FMT'] / 1e3) * data['Area']\n",
    "#     data_2 = (data['w_thickness'] / 1e3) * data['Area']\n",
    "#     x_1 = np.sort(data_1)\n",
    "#     y_1 = 1. * np.arange(len(data_1)) / (len(data_1) - 1)\n",
    "#     x_2 = np.sort(data_2)\n",
    "#     y_2 = 1. * np.arange(len(data_2)) / (len(data_2) - 1)\n",
    "    \n",
    "#     N1 = len(data_1)\n",
    "#     assert len(data_1) == len(data_2)\n",
    "#     resample_rate = 0.9\n",
    "#     Nsample = int(N1 * resample_rate)\n",
    "#     X = []\n",
    "#     for i in (range(1000)):\n",
    "#         sum1 = np.sum(data_1.to_numpy()[np.random.choice(N1, size=Nsample, replace=False)])\n",
    "#         sum2 = np.sum(data_2.to_numpy()[np.random.choice(N1, size=Nsample, replace=False)])\n",
    "\n",
    "#         X.append (sum1-sum2)\n",
    "        \n",
    "#     t = np.array(X)/resample_rate/1e3\n",
    "#     mu, std = norm.fit(t) \n",
    "#     sig = np.std(X)/resample_rate/1e3\n",
    "#     value = 0\n",
    "#     prob1=  norm.cdf(value,loc = mu, scale=sig)\n",
    "#     prob2= norm.cdf(mu,loc = mu, scale=sig)\n",
    "#     if prob1 > prob2:\n",
    "#         p2 = prob2\n",
    "#         p1 =1 - prob1\n",
    "#         dp = 1 - (1 - prob1) - prob2\n",
    "#     elif prob1 < prob2:\n",
    "#         dp = 1 - (1 - prob2) - prob1\n",
    "#         p2 = prob1\n",
    "#         p1 = prob2\n",
    "# #     print(dp)\n",
    "#     sds[str(SEED)] = [dp,mu]\n",
    "# #     print(sds[str(SEED)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(3.8.10)",
   "language": "python",
   "name": "new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
