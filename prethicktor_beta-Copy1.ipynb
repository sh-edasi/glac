{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install \n",
    "# import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import glacml as gl\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import re\n",
    "print(tf.__version__)\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# raw_dataset = pd.read_csv(\"/data/fast0/datasets/glathida-3.1.0/data/T.csv\")\n",
    "# raw_dataset = pd.read_csv(\"~/stuff/coding/glacier/data/T.csv\")\n",
    "\n",
    "\n",
    "#examine data columns\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(\"~/data/glac/T_models/T.csv\")\n",
    "glathida = raw_dataset.copy()\n",
    "glathida = glathida.drop([\n",
    "#                          \"GLACIER_DB\",\n",
    "                         \"POLITICAL_UNIT\",\n",
    "                         \"GLACIER_NAME\",\n",
    "                         \"SURVEY_DATE\",\n",
    "                         \"MEAN_THICKNESS_UNCERTAINTY\",\n",
    "#                          \"MAXIMUM_THICKNESS\",\n",
    "                         \"MAX_THICKNESS_UNCERTAINTY\",\n",
    "                         \"DATA_FLAG\",\n",
    "                         \"ELEVATION_DATE\",\n",
    "                         \"SPONSORING_AGENCY\",\n",
    "                         \"REMARKS\",\n",
    "                         \"SURVEY_METHOD_DETAILS\",\n",
    "                         \"SURVEY_METHOD\",\n",
    "                         \"NUMBER_OF_SURVEY_POINTS\",\n",
    "                         \"NUMBER_OF_SURVEY_PROFILES\",\n",
    "                         \"TOTAL_LENGTH_OF_SURVEY_PROFILES\",\n",
    "                         \"INTERPOLATION_METHOD\",\n",
    "                         \"INVESTIGATOR\",\n",
    "                         \"REFERENCES\",\n",
    "#                          \"GLACIER_ID\",\n",
    "                         \"GlaThiDa_ID\",\n",
    "                        ],axis=1)\n",
    "\n",
    "glathida.rename(columns = {\n",
    "                      \"LAT\":\"LAT_G\",\n",
    "                      \"LON\":\"LON_G\",\n",
    "                      \"AREA\":\"AREA_G\",\n",
    "                      \"MEAN_SLOPE\":\"MEAN_SLOPE_G\"\n",
    "                     },\n",
    "           inplace = True)\n",
    "glathida = glathida.dropna()\n",
    "\n",
    "with open(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",'r') as temp_f:\n",
    "# get No of columns in each line\n",
    "    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "### Read csv\n",
    "df = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",\n",
    "                 header=None, delimiter=\",\",names=column_names)\n",
    "col_names = list(df.iloc[0])\n",
    "df = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",\n",
    "                 header=None, delimiter=\",\", names=col_names)\n",
    "df.drop(0, inplace=True); df.reset_index(drop=True, inplace=True)\n",
    "RGI = df\n",
    "\n",
    "idx = glathida.index[glathida[\"GLACIER_DB\"]==\"RGI\"]\n",
    "glathida = glathida.loc[idx]\n",
    "glathida.rename(columns = {\"GLACIER_ID\":\"RGIId\"},inplace=True)\n",
    "\n",
    "RGI_glathida = pd.merge(glathida,RGI, how = \"inner\", on=\"RGIId\")\n",
    "RGI_glathida = RGI_glathida.drop([\n",
    "                                  \"GLACIER_DB\",\n",
    "                                  \"RGIId\",\n",
    "                                  \"GLIMSId\",\n",
    "                                  \"BgnDate\",\n",
    "                                  \"Status\",\n",
    "                                  \"Connect\",\n",
    "                                  \"TermType\",\n",
    "                                  \"Surging\",\n",
    "                                  \"Linkages\",\n",
    "                                  \"Name\",\n",
    "                                  \"O1Region\",\n",
    "                                  \"O2Region\",\n",
    "                                  \"EndDate\",\n",
    "                                  \"Name\"\n",
    "                                 ],axis=1)\n",
    "#split the dataset and reserve some to test what was trained.\n",
    "# train_dataset = RGI_glathida.sample(frac=0.8, random_state=0)\n",
    "# test_dataset = RGI_glathida.drop(train_dataset.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlaThiDa_ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>67.91500</td>\n",
       "      <td>18.56800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67.91000</td>\n",
       "      <td>18.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67.90000</td>\n",
       "      <td>18.57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>48.35698</td>\n",
       "      <td>-121.05735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>52.17540</td>\n",
       "      <td>-117.28400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>6627</td>\n",
       "      <td>46.20240</td>\n",
       "      <td>-121.49090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>6628</td>\n",
       "      <td>46.20240</td>\n",
       "      <td>-121.49090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>6629</td>\n",
       "      <td>46.20398</td>\n",
       "      <td>-121.49376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>6630</td>\n",
       "      <td>47.08670</td>\n",
       "      <td>12.38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>6631</td>\n",
       "      <td>47.08670</td>\n",
       "      <td>12.38000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5141 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GlaThiDa_ID       LAT        LON\n",
       "0               1  67.91500   18.56800\n",
       "1               2  67.91000   18.49600\n",
       "2               3  67.90000   18.57000\n",
       "3               4  48.35698 -121.05735\n",
       "4               5  52.17540 -117.28400\n",
       "...           ...       ...        ...\n",
       "5136         6627  46.20240 -121.49090\n",
       "5137         6628  46.20240 -121.49090\n",
       "5138         6629  46.20398 -121.49376\n",
       "5139         6630  47.08670   12.38000\n",
       "5140         6631  47.08670   12.38000\n",
       "\n",
       "[5141 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv(\"~/data/glac/T_models/T.csv\")\n",
    "T_dataset = raw_dataset\n",
    "T_dataset = T_dataset.drop(\"GLACIER_DB\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"POLITICAL_UNIT\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"GLACIER_ID\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"GLACIER_NAME\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"NUMBER_OF_SURVEY_POINTS\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"NUMBER_OF_SURVEY_PROFILES\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"TOTAL_LENGTH_OF_SURVEY_PROFILES\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"INTERPOLATION_METHOD\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"INVESTIGATOR\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"SPONSORING_AGENCY\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"ELEVATION_DATE\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"SURVEY_METHOD_DETAILS\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"SURVEY_METHOD\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"SURVEY_DATE\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"MEAN_THICKNESS\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"AREA\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"MEAN_THICKNESS_UNCERTAINTY\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"MAXIMUM_THICKNESS\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"MAX_THICKNESS_UNCERTAINTY\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"DATA_FLAG\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"REMARKS\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"REFERENCES\", axis=1)\n",
    "T_dataset = T_dataset.drop(\"MEAN_SLOPE\", axis=1)\n",
    "T_dataset = T_dataset.dropna()\n",
    "T_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT_G</th>\n",
       "      <th>LON_G</th>\n",
       "      <th>AREA_G</th>\n",
       "      <th>MEAN_SLOPE_G</th>\n",
       "      <th>MEAN_THICKNESS</th>\n",
       "      <th>MAXIMUM_THICKNESS</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.877250</td>\n",
       "      <td>88.130500</td>\n",
       "      <td>2.353870</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>115.750000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.614125</td>\n",
       "      <td>8.562494</td>\n",
       "      <td>2.519038</td>\n",
       "      <td>6.849574</td>\n",
       "      <td>20.041623</td>\n",
       "      <td>55.319526</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>38.214000</td>\n",
       "      <td>79.894000</td>\n",
       "      <td>0.536800</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.888500</td>\n",
       "      <td>83.266750</td>\n",
       "      <td>1.069082</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>42.755500</td>\n",
       "      <td>86.373500</td>\n",
       "      <td>1.400340</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.744250</td>\n",
       "      <td>91.237250</td>\n",
       "      <td>2.685128</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>155.250000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43.784000</td>\n",
       "      <td>99.881000</td>\n",
       "      <td>6.078000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LAT_G      LON_G    AREA_G  MEAN_SLOPE_G  MEAN_THICKNESS  \\\n",
       "count   4.000000   4.000000  4.000000      4.000000        4.000000   \n",
       "mean   41.877250  88.130500  2.353870     19.750000       29.500000   \n",
       "std     2.614125   8.562494  2.519038      6.849574       20.041623   \n",
       "min    38.214000  79.894000  0.536800     10.000000       15.000000   \n",
       "25%    40.888500  83.266750  1.069082     17.500000       15.750000   \n",
       "50%    42.755500  86.373500  1.400340     22.000000       22.500000   \n",
       "75%    43.744250  91.237250  2.685128     24.250000       36.250000   \n",
       "max    43.784000  99.881000  6.078000     25.000000       58.000000   \n",
       "\n",
       "       MAXIMUM_THICKNESS  NaN  \n",
       "count           4.000000  0.0  \n",
       "mean          115.750000  NaN  \n",
       "std            55.319526  NaN  \n",
       "min            66.000000  NaN  \n",
       "25%            70.500000  NaN  \n",
       "50%           110.000000  NaN  \n",
       "75%           155.250000  NaN  \n",
       "max           177.000000  NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out features - what will be trained to predict desired attribute\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "#define label - attribute training to be picked\n",
    "train_labels = train_features.pop(\"MEAN_THICKNESS\")\n",
    "test_labels = test_features.pop(\"MEAN_THICKNESS\")\n",
    "\n",
    "# train_features.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = {}\n",
    "variable_list = list(train_features)\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    normalizer[variable_name] = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
    "    normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n",
    "    \n",
    "    \n",
    "normalizer['ALL'] = preprocessing.Normalization(axis=-1)\n",
    "normalizer['ALL'].adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = {}\n",
    "linear_history = {}\n",
    "linear_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    linear_model[variable_name] = gl.build_linear_model(normalizer[variable_name])\n",
    "    linear_history[variable_name] = linear_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    \n",
    "    \n",
    "    linear_results[variable_name] = linear_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "linear_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(linear_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glacml as gl\n",
    "# gl.plot_loss(linear_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = gl.build_linear_model(normalizer['ALL'])\n",
    "\n",
    "history_full = linear_model.fit(\n",
    "train_features, train_labels,        \n",
    "   epochs=1000,\n",
    "   verbose=0,\n",
    "   validation_split = 0.2)\n",
    "\n",
    "test_results[\"MULTI\"] = linear_model.evaluate(\n",
    "    test_features,\n",
    "    test_labels, verbose=0)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_full_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model(norm):\n",
    "    model = keras.Sequential([\n",
    "              norm,\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(1) ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dnn_test_results(feature_name):\n",
    "    dnn_test_results[feature_name] = dnn_model.evaluate(\n",
    "        test_features[feature_name],\n",
    "        test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = {}\n",
    "dnn_history = {}\n",
    "dnn_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:\n",
    "\n",
    "    dnn_model[variable_name] = build_dnn_model(normalizer[variable_name])\n",
    "    dnn_history[variable_name] = dnn_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    dnn_results[variable_name_dnn] = dnn_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "dnn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(dnn_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    xmax = np.max(train_features[variable_name])\n",
    "    xmin = np.min(train_features[variable_name])\n",
    "    x = tf.linspace(xmin, xmax, 101)\n",
    "    y = dnn_model[variable_name].predict(x)\n",
    "    plot_single_model_variable(x,y,variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    plot_loss(dnn_history[variable_name])\n",
    "    ax.set_title(variable_name)\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_full_model = build_dnn_model(normalizer['ALL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dnn_history_full = dnn_full_model.fit(\n",
    "    train_features, train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dnn_history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(dnn_history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_full_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dnn_full_model.predict(test_features)\n",
    "fig,ax=plt.subplots()\n",
    "fig.patch.set_facecolor('w')\n",
    "plt.plot(test_labels,y,\"o\")\n",
    "plt.plot((0,120),(0,120),'-')\n",
    "plt.xlabel('True Thickness (m)')\n",
    "plt.ylabel('Model Thickness (m)')\n",
    "plt.xlim((0,120))\n",
    "plt.ylim((0,120))\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_res.EPS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (Cartopy)-f",
   "language": "python",
   "name": "python-cartopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
