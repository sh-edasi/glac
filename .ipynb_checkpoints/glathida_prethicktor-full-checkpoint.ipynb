{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install \n",
    "# import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import glacml as gl\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import re\n",
    "print(tf.__version__)\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# raw_dataset = pd.read_csv(\"/data/fast0/datasets/glathida-3.1.0/data/T.csv\")\n",
    "# raw_dataset = pd.read_csv(\"~/stuff/coding/glacier/data/T.csv\")\n",
    "\n",
    "\n",
    "#examine data columns\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GLACIER_DB           RGIId   LAT_G   LON_G   AREA_G  MEAN_SLOPE_G  \\\n",
      "0        RGI  RGI60-13.31537  38.214  99.881  0.53680          24.0   \n",
      "1        RGI  RGI60-13.47247  43.731  84.391  1.24651          20.0   \n",
      "2        RGI  RGI60-13.48211  43.784  88.356  6.07800          25.0   \n",
      "3        RGI  RGI60-13.43165  41.780  79.894  1.55417          10.0   \n",
      "4        RGI  RGI60-13.32330  39.237  97.755  2.96629          21.0   \n",
      "\n",
      "   MEAN_THICKNESS  MAXIMUM_THICKNESS         GLIMSId   BgnDate  ... Aspect  \\\n",
      "0            16.0               66.0  G099881E38214N  20070813  ...      3   \n",
      "1            15.0               72.0  G084391E43731N  20090808  ...     28   \n",
      "2            58.0              177.0  G088356E43784N  20100813  ...    172   \n",
      "3            29.0              148.0  G079894E41780N  20070824  ...    194   \n",
      "4            41.0              121.0  G097755E39237N  20060918  ...      2   \n",
      "\n",
      "     Lmax Status Connect Form TermType Surging Linkages  \\\n",
      "0    1051      0       0    0        0       9        9   \n",
      "1    1492      0       0    0        0       9        9   \n",
      "2    7215      0       0    0        0       9        9   \n",
      "3    5814      0       0    1        0       9        9   \n",
      "4    3007      0       0    0        0       9        1   \n",
      "\n",
      "                               Name NaN  \n",
      "0                      CN5Y424B0004 NaN  \n",
      "1                      CN5Y741C0051 NaN  \n",
      "2                      CN5Y813B0008 NaN  \n",
      "3  CN5Y673P0072 Qingbingtan Glacier NaN  \n",
      "4         CN5Y437C0018 Qiyi Glacier NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "#drop everything but lat, long, elevation, thickness\n",
    "raw_dataset = pd.read_csv(\"~/data/glac/T_models/T.csv\")\n",
    "glathida = raw_dataset.copy()\n",
    "glathida = glathida.drop([\n",
    "#                          \"GLACIER_DB\",\n",
    "                         \"POLITICAL_UNIT\",\n",
    "                         \"GLACIER_NAME\",\n",
    "                         \"SURVEY_DATE\",\n",
    "                         \"MEAN_THICKNESS_UNCERTAINTY\",\n",
    "#                          \"MAXIMUM_THICKNESS\",\n",
    "                         \"MAX_THICKNESS_UNCERTAINTY\",\n",
    "                         \"DATA_FLAG\",\n",
    "                         \"ELEVATION_DATE\",\n",
    "                         \"SPONSORING_AGENCY\",\n",
    "                         \"REMARKS\",\n",
    "                         \"SURVEY_METHOD_DETAILS\",\n",
    "                         \"SURVEY_METHOD\",\n",
    "                         \"NUMBER_OF_SURVEY_POINTS\",\n",
    "                         \"NUMBER_OF_SURVEY_PROFILES\",\n",
    "                         \"TOTAL_LENGTH_OF_SURVEY_PROFILES\",\n",
    "                         \"INTERPOLATION_METHOD\",\n",
    "                         \"INVESTIGATOR\",\n",
    "                         \"REFERENCES\",\n",
    "#                          \"GLACIER_ID\",\n",
    "                         \"GlaThiDa_ID\",\n",
    "                        ],axis=1)\n",
    "\n",
    "glathida.rename(columns = {\n",
    "                      \"LAT\":\"LAT_G\",\n",
    "                      \"LON\":\"LON_G\",\n",
    "                      \"AREA\":\"AREA_G\",\n",
    "                      \"MEAN_SLOPE\":\"MEAN_SLOPE_G\"\n",
    "                     },\n",
    "           inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "glathida = glathida.dropna()\n",
    "\n",
    "\n",
    "#split the dataset and reserve some to test what was trained.\n",
    "# train_dataset = dataset.sample(frac=0.8, random_state=1)\n",
    "# test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "# RGI = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\")\n",
    "# RGI = RGI.drop([\n",
    "# #                 \"RGIId\",\n",
    "#                 \"GLIMSId\",\n",
    "#                 \"BgnDate\",\n",
    "#                 \"EndDate\",\n",
    "#                 \"O1Region\",\n",
    "#                 \"O2Region\", \n",
    "# #                 \"Zmin\",\n",
    "# #                 \"Zmed\",\n",
    "# #                 \"Zmax\",\n",
    "# #                 \"Aspect\",\n",
    "# #                 \"Lmax\",\n",
    "#                 \"Status\",\n",
    "#                 \"Connect\",\n",
    "#                 \"Form\",\n",
    "#                 \"TermType\",\n",
    "#                 \"Surging\",\n",
    "#                 \"Linkages\", \n",
    "#                 \"Name\",\n",
    "#                ],axis=1)\n",
    "# RGI.rename(columns = {\n",
    "#                       \"CenLon\":\"LON_R\",\n",
    "#                       \"CenLat\":\"LAT_R\",\n",
    "#                       \"Area\":\"AREA_R\",\n",
    "#                       \"Slope\":\"MEAN_SLOPE_R\"\n",
    "#                      },\n",
    "#            inplace = True)\n",
    "with open(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",'r') as temp_f:\n",
    "# get No of columns in each line\n",
    "    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "### Read csv\n",
    "df = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",\n",
    "                 header=None, delimiter=\",\", \n",
    "names=column_names)\n",
    "col_names = list(df.iloc[0])\n",
    "df = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/13_rgi60_CentralAsia.csv\",\n",
    "                 header=None, delimiter=\",\", names=col_names)\n",
    "df.drop(0, inplace=True); df.reset_index(drop=True, inplace=True)\n",
    "RGI = df\n",
    "\n",
    "\n",
    "\n",
    "idx = glathida.index[glathida[\"GLACIER_DB\"]==\"RGI\"]\n",
    "glathida = glathida.loc[idx]\n",
    "glathida.rename(columns = {\"GLACIER_ID\":\"RGIId\"},inplace=True)\n",
    "glathida\n",
    "# RGI\n",
    "RGI_glathida = pd.merge(glathida,RGI, how = \"inner\", on=\"RGIId\")\n",
    "RGI_glathida.drop([\n",
    "                   \"GLACIER_DB\",\n",
    "                   \"RGIId\",\n",
    "                   \"GLIMSId\",\n",
    "                   \"BgnDate\",\n",
    "                   \"Status\",\n",
    "                   \"Connect\",\n",
    "                   \"Form\",\n",
    "                   \"TermType\",\n",
    "                   \"Surging\",\n",
    "                   \"Linkages\",\n",
    "                   \"Name\",\n",
    "                   \"O1Region\",\n",
    "                   \"O2Region\",\n",
    "                   \"EndDate\",\n",
    "                   \"Name\"\n",
    "                ],axis=1)\n",
    "print(RGI_glathida)\n",
    "#split the dataset and reserve some to test what was trained.\n",
    "train_dataset = RGI_glathida.sample(frac=0.8, random_state=0)\n",
    "test_dataset = RGI_glathida.drop(train_dataset.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LAT_G</th>\n",
       "      <td>4.0</td>\n",
       "      <td>41.87725</td>\n",
       "      <td>2.614125</td>\n",
       "      <td>38.2140</td>\n",
       "      <td>40.888500</td>\n",
       "      <td>42.75550</td>\n",
       "      <td>43.744250</td>\n",
       "      <td>43.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LON_G</th>\n",
       "      <td>4.0</td>\n",
       "      <td>88.13050</td>\n",
       "      <td>8.562494</td>\n",
       "      <td>79.8940</td>\n",
       "      <td>83.266750</td>\n",
       "      <td>86.37350</td>\n",
       "      <td>91.237250</td>\n",
       "      <td>99.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AREA_G</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.35387</td>\n",
       "      <td>2.519038</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>1.069082</td>\n",
       "      <td>1.40034</td>\n",
       "      <td>2.685128</td>\n",
       "      <td>6.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN_SLOPE_G</th>\n",
       "      <td>4.0</td>\n",
       "      <td>19.75000</td>\n",
       "      <td>6.849574</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>25.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN_THICKNESS</th>\n",
       "      <td>4.0</td>\n",
       "      <td>29.50000</td>\n",
       "      <td>20.041623</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>22.50000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>58.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAXIMUM_THICKNESS</th>\n",
       "      <td>4.0</td>\n",
       "      <td>115.75000</td>\n",
       "      <td>55.319526</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>110.00000</td>\n",
       "      <td>155.250000</td>\n",
       "      <td>177.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count       mean        std      min        25%        50%  \\\n",
       "LAT_G                4.0   41.87725   2.614125  38.2140  40.888500   42.75550   \n",
       "LON_G                4.0   88.13050   8.562494  79.8940  83.266750   86.37350   \n",
       "AREA_G               4.0    2.35387   2.519038   0.5368   1.069082    1.40034   \n",
       "MEAN_SLOPE_G         4.0   19.75000   6.849574  10.0000  17.500000   22.00000   \n",
       "MEAN_THICKNESS       4.0   29.50000  20.041623  15.0000  15.750000   22.50000   \n",
       "MAXIMUM_THICKNESS    4.0  115.75000  55.319526  66.0000  70.500000  110.00000   \n",
       "NaN                  0.0        NaN        NaN      NaN        NaN        NaN   \n",
       "\n",
       "                          75%      max  \n",
       "LAT_G               43.744250   43.784  \n",
       "LON_G               91.237250   99.881  \n",
       "AREA_G               2.685128    6.078  \n",
       "MEAN_SLOPE_G        24.250000   25.000  \n",
       "MEAN_THICKNESS      36.250000   58.000  \n",
       "MAXIMUM_THICKNESS  155.250000  177.000  \n",
       "NaN                       NaN      NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out features - what will be trained to predict desired attribute\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "#define label - attribute training to be picked\n",
    "train_labels = train_features.pop(\"MEAN_THICKNESS\")\n",
    "test_labels = test_features.pop(\"MEAN_THICKNESS\")\n",
    "\n",
    "# train_features.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 00:55:57.183934: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-16 00:55:57.183972: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sermeq.ess.washington.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-03-16 00:55:57.184848: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-16 00:55:57.550475: W tensorflow/core/framework/op_kernel.cc:1722] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node Cast\n (defined at /home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py:278)\n]] [Op:__inference_adapt_step_235]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Cast:\nIn[0] IteratorGetNext (defined at /home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py:116)\n\nOperation defined at: (most recent call last)\n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3173, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_2075219/2350232436.py\", line 6, in <module>\n>>>     normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\", line 244, in adapt\n>>>     self._adapt_function(iterator)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step\n>>>     self.update_state(data)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py\", line 206, in update_state\n>>>     data = self._standardize_inputs(data)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py\", line 278, in _standardize_inputs\n>>>     inputs = tf.cast(inputs, self.dtype)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2075219/2350232436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnormalizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnormalizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node Cast\n (defined at /home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py:278)\n]] [Op:__inference_adapt_step_235]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Cast:\nIn[0] IteratorGetNext (defined at /home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py:116)\n\nOperation defined at: (most recent call last)\n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n>>>     raw_cell, store_history, silent, shell_futures)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3173, in run_cell_async\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_2075219/2350232436.py\", line 6, in <module>\n>>>     normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\", line 244, in adapt\n>>>     self._adapt_function(iterator)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step\n>>>     self.update_state(data)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py\", line 206, in update_state\n>>>     data = self._standardize_inputs(data)\n>>> \n>>>   File \"/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/layers/preprocessing/normalization.py\", line 278, in _standardize_inputs\n>>>     inputs = tf.cast(inputs, self.dtype)\n>>> "
     ]
    }
   ],
   "source": [
    "normalizer = {}\n",
    "variable_list = list(train_features)\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    normalizer[variable_name] = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
    "    normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n",
    "    \n",
    "    \n",
    "normalizer['ALL'] = preprocessing.Normalization(axis=-1)\n",
    "normalizer['ALL'].adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = {}\n",
    "linear_history = {}\n",
    "linear_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    linear_model[variable_name] = gl.build_linear_model(normalizer[variable_name])\n",
    "    linear_history[variable_name] = linear_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    \n",
    "    \n",
    "    linear_results[variable_name] = linear_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "linear_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(linear_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glacml as gl\n",
    "# gl.plot_loss(linear_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = gl.build_linear_model(normalizer['ALL'])\n",
    "\n",
    "history_full = linear_model.fit(\n",
    "train_features, train_labels,        \n",
    "   epochs=1000,\n",
    "   verbose=0,\n",
    "   validation_split = 0.2)\n",
    "\n",
    "test_results[\"MULTI\"] = linear_model.evaluate(\n",
    "    test_features,\n",
    "    test_labels, verbose=0)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_full_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model(norm):\n",
    "    model = keras.Sequential([\n",
    "              norm,\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(64, activation='relu'),\n",
    "              layers.Dense(1) ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dnn_test_results(feature_name):\n",
    "    dnn_test_results[feature_name] = dnn_model.evaluate(\n",
    "        test_features[feature_name],\n",
    "        test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = {}\n",
    "dnn_history = {}\n",
    "dnn_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:\n",
    "\n",
    "    dnn_model[variable_name] = build_dnn_model(normalizer[variable_name])\n",
    "    dnn_history[variable_name] = dnn_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1000,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    dnn_results[variable_name_dnn] = dnn_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)\n",
    "\n",
    "dnn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in variable_list:    \n",
    "    df = pd.DataFrame(dnn_history[variable_name].history)\n",
    "    dfs = df.loc[[df.last_valid_index()]]\n",
    "    dfs.insert(0, 'Variable', [variable_name])\n",
    "    \n",
    "    print(dfs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    xmax = np.max(train_features[variable_name])\n",
    "    xmin = np.min(train_features[variable_name])\n",
    "    x = tf.linspace(xmin, xmax, 101)\n",
    "    y = dnn_model[variable_name].predict(x)\n",
    "    plot_single_model_variable(x,y,variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    plot_loss(dnn_history[variable_name])\n",
    "    ax.set_title(variable_name)\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_full_model = build_dnn_model(normalizer['ALL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dnn_history_full = dnn_full_model.fit(\n",
    "    train_features, train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dnn_history_full.history)\n",
    "dfs = df.loc[[df.last_valid_index()]]\n",
    "dfs.insert(0, 'Variable', 'Multi-Variable')\n",
    "    \n",
    "print(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(dnn_history_full)\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_full_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dnn_full_model.predict(test_features)\n",
    "fig,ax=plt.subplots()\n",
    "fig.patch.set_facecolor('w')\n",
    "plt.plot(test_labels,y,\"o\")\n",
    "plt.plot((0,120),(0,120),'-')\n",
    "plt.xlabel('True Thickness (m)')\n",
    "plt.ylabel('Model Thickness (m)')\n",
    "plt.xlim((0,120))\n",
    "plt.ylim((0,120))\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_res.EPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split the dataset and reserve some to test what was trained.\n",
    "# train_dataset = dataset.sample(frac=0.8, random_state=1)\n",
    "# test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "RGI = pd.read_csv(\"/home/sa42/data/glac/glims/glims_extra/dump2/01_rgi60_Alaska.csv\")\n",
    "RGI = RGI.drop(\"RGIId\", axis=1)\n",
    "RGI = RGI.drop(\"GLIMSId\", axis=1)\n",
    "RGI = RGI.drop(\"BgnDate\", axis=1)\n",
    "RGI = RGI.drop(\"EndDate\", axis=1)\n",
    "RGI = RGI.drop(\"O1Region\", axis=1)\n",
    "RGI = RGI.drop(\"O2Region\", axis=1)\n",
    "RGI = RGI.drop(\"Zmin\", axis=1)\n",
    "RGI = RGI.drop(\"Zmed\", axis=1)\n",
    "RGI = RGI.drop(\"Zmax\", axis=1)\n",
    "RGI = RGI.drop(\"Aspect\", axis=1)\n",
    "RGI = RGI.drop(\"Lmax\", axis=1)\n",
    "RGI = RGI.drop(\"Status\", axis=1)\n",
    "RGI = RGI.drop(\"Connect\", axis=1)\n",
    "RGI = RGI.drop(\"Form\", axis=1)\n",
    "RGI = RGI.drop(\"TermType\", axis=1)\n",
    "RGI = RGI.drop(\"Surging\", axis=1)\n",
    "RGI = RGI.drop(\"Linkages\", axis=1)\n",
    "RGI = RGI.drop(\"Name\", axis=1)\n",
    "RGI.rename(columns = {\"CenLon\":\"LON\",\n",
    "                      \"CenLat\":\"LAT\",\n",
    "                      \"Area\":\"AREA\",\n",
    "                      \"Slope\":\"MEAN_SLOPE\"},\n",
    "           inplace = True)\n",
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer['ALL'].adapt(np.array(RGI))\n",
    "y_alaska = dnn_full_model.predict(RGI)\n",
    "print(y_alaska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI[\"Prethicktion\"] = y_alaska\n",
    "\n",
    "RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer['ALL'].adapt(np.array(RGI))\n",
    "y_alaska = dnn_full_model.predict(RGI)\n",
    "print(y_alaska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI[\"thickness_prethicktions\"] = y_alaska\n",
    "RGI.index[RGI[\"thickness_prethicktions\"]<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGI[\"volume\"] = RGI[\"AREA\"] * (RGI[\"thickness_prethicktions\"]/1000)\n",
    "RGI[\"volume\"].astype(float).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish normalization layers\n",
    "alaska_normalizer = {}\n",
    "variable_list = (\"LON\",\n",
    "                 \"LAT\",\n",
    "                 \"AREA\",\n",
    "                 \"MEAN_SLOPE\")\n",
    "for variable_name in variable_list:\n",
    "\n",
    "    normalizer[variable_name] = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
    "    normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n",
    "    \n",
    "    \n",
    "normalizer['ALL'] = preprocessing.Normalization(axis=-1)\n",
    "y_alaska = dnn_full_model.predict(RGI)\n",
    "\n",
    "print(y_alaska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_saved_for_later\n",
    "\n",
    "\n",
    "# dataset_saved_for_later = dataset_saved_for_later.drop(\"GLACIER_ID\", axis=1)\n",
    "# dataset_saved_for_later = dataset_saved_for_later.drop(\"GlaThiDa_ID\", axis=1)\n",
    "# #drop the remaining null pieces from elevation\n",
    "# dataset_saved_for_later = dataset_saved_for_later.dropna()\n",
    "# dataset_saved_for_later.isna().sum()\n",
    "dataset_saved_for_later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_alaska_test = dnn_full_model.predict(RGI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_alaska_test,\".\")\n",
    "# plt.plot((0,1000),(0,1000),'-')\n",
    "plt.xlabel('True Thickness (m)')\n",
    "plt.ylabel('Model Thickness (m)')\n",
    "plt.xlim((-10,3000))\n",
    "plt.ylim((-200,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_alaska_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (Cartopy)-f",
   "language": "python",
   "name": "python-cartopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
