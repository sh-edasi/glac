{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install ing_theme_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# import qbstyles\n",
    "# from ing_theme_matplotlib import mpl_style\n",
    "import glacierml as gl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Organize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Load and organize the Glathida dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa42/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (2,4,5,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# raw_dataset = pd.read_csv('/data/fast0/datasets/glathida-3.1.0/data/T.csv')\n",
    "\n",
    "TTT = pd.read_csv('/home/sa42/data/glac/T_models/TTT.csv')\n",
    "glathida = TTT[[\n",
    "#     'GlaThiDa_ID',\n",
    "#     'POLITICAL_UNIT',\n",
    "#     'GLACIER_NAME',\n",
    "#     'SURVEY_DATE',\n",
    "#     'PROFILE_ID',\n",
    "#     'POINT_ID',\n",
    "    'POINT_LAT',\n",
    "    'POINT_LON',\n",
    "    'ELEVATION',\n",
    "    'THICKNESS',\n",
    "#     'THICKNESS_UNCERTAINTY',\n",
    "#     'DATA_FLAG',\n",
    "#     'REMARKS'\n",
    "]]\n",
    "glathida = glathida.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate out features - what will be trained to predict desired attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POINT_LAT</th>\n",
       "      <td>47.477816</td>\n",
       "      <td>53.238428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POINT_LON</th>\n",
       "      <td>-46.458083</td>\n",
       "      <td>45.484560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEVATION</th>\n",
       "      <td>1175.033891</td>\n",
       "      <td>937.446613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean         std\n",
       "POINT_LAT    47.477816   53.238428\n",
       "POINT_LON   -46.458083   45.484560\n",
       "ELEVATION  1175.033891  937.446613"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = glathida.sample(frac=0.8, random_state=0)\n",
    "test_dataset = glathida.drop(train_dataset.index)\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "#define label - attribute training to be picked\n",
    "train_labels = train_features.pop('THICKNESS')\n",
    "test_labels = test_features.pop('THICKNESS')\n",
    "\n",
    "train_features.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]2022-03-19 23:17:27.597905: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-19 23:17:27.597958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sermeq.ess.washington.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-03-19 23:17:27.598833: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████| 3/3 [01:05<00:00, 21.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3574167/1056442016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ALL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnormalizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ALL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m-> 3129\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3514\u001b[0m     \u001b[0mcache_key_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3515\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python-cartopy-f/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, cache_key_context, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   3293\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m       input_signature = pywrap_tfe.TFE_Py_EncodeArg(\n\u001b[0;32m-> 3295\u001b[0;31m           inputs, include_tensor_ranks_only, ENCODE_VARIABLES_BY_RESOURCE_ID)\n\u001b[0m\u001b[1;32m   3296\u001b[0m       \u001b[0mhashable_input_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_input_signature_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "normalizer = {}\n",
    "variable_list = list(train_features)\n",
    "for variable_name in tqdm(variable_list):\n",
    "\n",
    "    normalizer[variable_name] = gl.preprocessing.Normalization(input_shape=[1,], axis=None)\n",
    "    normalizer[variable_name].adapt(np.array(train_features[variable_name]))\n",
    "    \n",
    "    \n",
    "normalizer['ALL'] = gl.preprocessing.Normalization(axis=-1)\n",
    "normalizer['ALL'].adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = {}\n",
    "linear_history = {}\n",
    "linear_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for variable_name in tqdm(variable_list):\n",
    "\n",
    "    linear_model[variable_name] = gl.build_linear_model(normalizer[variable_name])\n",
    "    linear_history[variable_name] = linear_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    \n",
    "    \n",
    "    linear_results[variable_name] = linear_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(2,6,figsize=(10,10))\n",
    "# for i, variable_name in enumerate(variable_list):\n",
    "# #     mpl_style(\"dark\")\n",
    "#     ax = plt.subplot(4,4,i+1)\n",
    "#     plt.subplot(4,4,i+1)\n",
    "#     gl.plot_loss(linear_history[variable_name])\n",
    "#     ax.set_title(variable_name)\n",
    "# #     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = gl.build_linear_model(normalizer['ALL'])\n",
    "\n",
    "linear_history['MULTI'] = linear_model.fit(\n",
    "train_features, train_labels,        \n",
    "   epochs=1,\n",
    "   verbose=0,\n",
    "   validation_split = 0.2)\n",
    "\n",
    "linear_results['MULTI'] = linear_model.evaluate(\n",
    "    test_features,\n",
    "    test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gl.plot_loss(history_full)\n",
    "# # plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_full_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = {}\n",
    "dnn_model = {}\n",
    "dnn_history = {}\n",
    "dnn_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for variable_name in tqdm(variable_list):\n",
    "\n",
    "    dnn_model[variable_name] = gl.build_dnn_model(normalizer[variable_name])\n",
    "    dnn_history[variable_name] = dnn_model[variable_name].fit(\n",
    "                                        train_features[variable_name], train_labels,        \n",
    "                                        epochs=1,\n",
    "                                        verbose=0,\n",
    "                                        validation_split = 0.2)\n",
    "    dnn_results[variable_name] = dnn_model[variable_name].evaluate(\n",
    "                                        test_features[variable_name],\n",
    "                                        test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(4,4,figsize=(10,10))\n",
    "# for i, variable_name in enumerate(variable_list):\n",
    "#     plt.subplot(4,4,i+1)\n",
    "#     xmax = np.max(train_features[variable_name])\n",
    "#     xmin = np.min(train_features[variable_name])\n",
    "#     x = tf.linspace(xmin, xmax, 101)\n",
    "#     y = dnn_model[variable_name].predict(x)\n",
    "#     plot_single_model_variable(x,y,variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(4,4,figsize=(10,10))\n",
    "for i, variable_name in enumerate(variable_list):\n",
    "    ax = plt.subplot(4,4,i+1)\n",
    "    gl.plot_loss(dnn_history[variable_name])\n",
    "    ax.set_title(variable_name)\n",
    "#     plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_full_model = gl.build_dnn_model(normalizer['ALL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dnn_history['MULTI'] = dnn_full_model.fit(\n",
    "    train_features, train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=1)\n",
    "\n",
    "dnn_results['MULTI'] = dnn_full_model.evaluate(\n",
    "    test_features,\n",
    "    test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.plot_loss(dnn_history['MULTI'])\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_dnn_full_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dnn_full_model.predict(test_features)\n",
    "plt.plot(test_labels,y,'.')\n",
    "plt.plot((0,200),(0,200),'-')\n",
    "plt.xlabel('True Thickness (m)')\n",
    "plt.ylabel('Model Thickness (m)')\n",
    "plt.xlim((0,200))\n",
    "plt.ylim((0,200))\n",
    "# plt.savefig(\"/home/sa42/notebooks/glac/figs/GTP1_res.EPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for variable_name in list(dnn_history):    \n",
    "    df1 = pd.DataFrame(dnn_history[variable_name].history)\n",
    "    df2 = pd.DataFrame(linear_history[variable_name].history)\n",
    "    df1 = df1.loc[[df1.last_valid_index()]]\n",
    "    df2 = df2.loc[[df2.last_valid_index()]]\n",
    "    df1['Architecture'] = 'DNN'\n",
    "    df2['Architecture'] = 'Linear'\n",
    "    df1.insert(0, 'Variable', [variable_name])\n",
    "    df2.insert(0, 'Variable', [variable_name])\n",
    "    df = pd.concat([df1,df2])\n",
    "    dfs = dfs.append(df)\n",
    "df = dfs[[\n",
    "    'Architecture',\n",
    "    'Variable',\n",
    "    'loss',\n",
    "    'val_loss'\n",
    "]]\n",
    "df = df.sort_values(by=['Architecture','Variable'], ascending=[False,False])\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGI merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (Cartopy)-f",
   "language": "python",
   "name": "python-cartopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
